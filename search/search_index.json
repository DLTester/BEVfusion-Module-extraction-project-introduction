{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BEVFusion \u6a21\u5757\u5316\u63d0\u53d6\u6587\u6863 \u00b6 \u672c\u9879\u76ee\u5bf9 BEVFusion \u7f51\u7edc\u7684\u5173\u952e\u7ec4\u4ef6\u8fdb\u884c\u7ed3\u6784\u5316\u62c6\u5206\uff0c\u5f62\u6210 \u53ef\u590d\u7528\u7b97\u5b50\uff08Operators\uff09 \u4e0e \u53ef\u63d2\u62d4\u7f51\u7edc\u6a21\u5757\uff08Networks\uff09 \uff0c\u65b9\u4fbf\u7406\u89e3\u3001\u8c03\u8bd5\u4e0e\u4e8c\u6b21\u5f00\u53d1\u3002 \u672c\u6587\u6863\u65e8\u5728\u63d0\u4f9b\uff1a \u5404\u5b50\u6a21\u5757\u7684\u529f\u80fd\u8bf4\u660e \u5bf9\u5e94\u7b97\u5b50\u4e0e\u7f51\u7edc\u5c42\u7684 API \u6587\u6863 \u4e3b\u6846\u67b6 BEVFusionFramework \u7684\u6574\u4f53\u8c03\u7528\u5173\u7cfb \u6587\u6863\u4e3b\u8981\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u90e8\u5206\uff1a \ud83d\udcf7 Camera \u4fa7\u6a21\u5757 \u00b6 \u5305\u62ec\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u3001\u51e0\u4f55\u53d8\u6362\u4ee5\u53ca\u5c06\u7279\u5f81\u6620\u5c04\u5230 BEV \u7a7a\u95f4\u7684\u76f8\u5173\u7b97\u5b50\u4e0e\u7f51\u7edc\u7ed3\u6784\u3002 \u8be6\u89c1\uff1a \ud83d\udc49 camera_side.md \ud83d\udef0 LiDAR \u4fa7\u6a21\u5757 \u00b6 \u5305\u542b\u4f53\u7d20\u5316\u3001\u7a00\u758f\u5377\u79ef Backbone\u3001\u70b9\u4e91 BEV \u6620\u5c04\u7b49\u6838\u5fc3\u6a21\u5757\u3002 \u8be6\u89c1\uff1a \ud83d\udc49 lidar_side.md \ud83d\uddfa BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757 \u00b6 \u63cf\u8ff0\u56fe\u50cf BEV \u4e0e\u70b9\u4e91 BEV \u7684\u878d\u5408\u6d41\u7a0b\uff0c\u5305\u62ec BEV \u7f16\u7801\u5668\u4e0e\u878d\u5408\u7b97\u5b50\u3002 \u8be6\u89c1\uff1a \ud83d\udc49 bev_space_integration.md \ud83e\udde9 BEVFusion \u4e3b\u6846\u67b6 \u00b6 \u4e3a\u4ee5\u4e0a\u5404\u6a21\u5757\u63d0\u4f9b\u7edf\u4e00\u5c01\u88c5\uff0c\u5b9a\u4e49\u5b8c\u6574\u7684 BEVFusion \u63a8\u7406\u6d41\u7a0b\u3002 \u8be6\u89c1\uff1a \ud83d\udc49 BEVFusionFramework.md","title":"\u9996\u9875"},{"location":"#bevfusion","text":"\u672c\u9879\u76ee\u5bf9 BEVFusion \u7f51\u7edc\u7684\u5173\u952e\u7ec4\u4ef6\u8fdb\u884c\u7ed3\u6784\u5316\u62c6\u5206\uff0c\u5f62\u6210 \u53ef\u590d\u7528\u7b97\u5b50\uff08Operators\uff09 \u4e0e \u53ef\u63d2\u62d4\u7f51\u7edc\u6a21\u5757\uff08Networks\uff09 \uff0c\u65b9\u4fbf\u7406\u89e3\u3001\u8c03\u8bd5\u4e0e\u4e8c\u6b21\u5f00\u53d1\u3002 \u672c\u6587\u6863\u65e8\u5728\u63d0\u4f9b\uff1a \u5404\u5b50\u6a21\u5757\u7684\u529f\u80fd\u8bf4\u660e \u5bf9\u5e94\u7b97\u5b50\u4e0e\u7f51\u7edc\u5c42\u7684 API \u6587\u6863 \u4e3b\u6846\u67b6 BEVFusionFramework \u7684\u6574\u4f53\u8c03\u7528\u5173\u7cfb \u6587\u6863\u4e3b\u8981\u5206\u4e3a\u4ee5\u4e0b\u51e0\u4e2a\u90e8\u5206\uff1a","title":"BEVFusion \u6a21\u5757\u5316\u63d0\u53d6\u6587\u6863"},{"location":"#camera","text":"\u5305\u62ec\u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u3001\u51e0\u4f55\u53d8\u6362\u4ee5\u53ca\u5c06\u7279\u5f81\u6620\u5c04\u5230 BEV \u7a7a\u95f4\u7684\u76f8\u5173\u7b97\u5b50\u4e0e\u7f51\u7edc\u7ed3\u6784\u3002 \u8be6\u89c1\uff1a \ud83d\udc49 camera_side.md","title":"\ud83d\udcf7 Camera \u4fa7\u6a21\u5757"},{"location":"#lidar","text":"\u5305\u542b\u4f53\u7d20\u5316\u3001\u7a00\u758f\u5377\u79ef Backbone\u3001\u70b9\u4e91 BEV \u6620\u5c04\u7b49\u6838\u5fc3\u6a21\u5757\u3002 \u8be6\u89c1\uff1a \ud83d\udc49 lidar_side.md","title":"\ud83d\udef0 LiDAR \u4fa7\u6a21\u5757"},{"location":"#bev","text":"\u63cf\u8ff0\u56fe\u50cf BEV \u4e0e\u70b9\u4e91 BEV \u7684\u878d\u5408\u6d41\u7a0b\uff0c\u5305\u62ec BEV \u7f16\u7801\u5668\u4e0e\u878d\u5408\u7b97\u5b50\u3002 \u8be6\u89c1\uff1a \ud83d\udc49 bev_space_integration.md","title":"\ud83d\uddfa BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757"},{"location":"#bevfusion_1","text":"\u4e3a\u4ee5\u4e0a\u5404\u6a21\u5757\u63d0\u4f9b\u7edf\u4e00\u5c01\u88c5\uff0c\u5b9a\u4e49\u5b8c\u6574\u7684 BEVFusion \u63a8\u7406\u6d41\u7a0b\u3002 \u8be6\u89c1\uff1a \ud83d\udc49 BEVFusionFramework.md","title":"\ud83e\udde9 BEVFusion \u4e3b\u6846\u67b6"},{"location":"BEVFusionFramework/","text":"BEVFusion Framework\uff08BEVFusion \u603b\u4f53\u6846\u67b6\uff09 \u00b6 BEVFusionFramework \u7684\u4f5c\u7528\u662f\uff1a \u7edf\u4e00\u7ec4\u7ec7\u56fe\u50cf\u4fa7\u3001\u70b9\u4e91\u4fa7\u4e0e BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757\uff0c \u5b9e\u73b0 BEVFusion \u7684\u5b8c\u6574\u63a8\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u56fe\u50cf\u5904\u7406\u3001\u70b9\u4e91\u5904\u7406\u3001BEV \u751f\u6210\u4e0e\u6700\u7ec8\u878d\u5408\u3002 \u672c\u6846\u67b6\u5305\u542b\u56db\u7c7b\u6838\u5fc3\u529f\u80fd\uff1a \u56fe\u50cf\u4fa7\u5904\u7406\u6d41\u7a0b \u8c03\u7528 Camera Encoder Interface\uff0c\u5bf9\u591a\u89c6\u89d2\u56fe\u50cf\u63d0\u53d6\u8bed\u4e49\u4e0e\u6df1\u5ea6\u7279\u5f81\uff0c\u5e76\u751f\u6210 Camera BEV \u7279\u5f81\u3002 \u70b9\u4e91\u4fa7\u5904\u7406\u6d41\u7a0b \u8c03\u7528 Lidar Preprocess Interface + \u70b9\u4e91\u4e3b\u5e72\u7f51\u7edc\uff0c\u751f\u6210 LiDAR BEV \u7279\u5f81\u3002 BEV \u7a7a\u95f4\u878d\u5408\u6d41\u7a0b \u5c06\u56fe\u50cf BEV \u4e0e\u70b9\u4e91 BEV \u7279\u5f81\u8fdb\u884c\u62fc\u63a5\u6216\u5176\u4ed6\u878d\u5408\u65b9\u5f0f\uff0c\u5f62\u6210\u7edf\u4e00 BEV \u8868\u793a\u3002 \u63a8\u7406\u7ec4\u7ec7\u903b\u8f91 \u5c01\u88c5\u8f93\u5165 \u2192 \u63d0\u53d6\u7279\u5f81 \u2192 BEV\u751f\u6210 \u2192 \u878d\u5408 \u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u4f5c\u4e3a\u6a21\u578b\u6574\u4f53\u5165\u53e3\u3002 API \u6587\u6863 \u00b6 \u4ee5\u4e0b\u5217\u51fa BEVFusionFramework \u7684\u4e3b\u6846\u67b6\u7c7b\u4e0e\u8c03\u7528\u7ed3\u6784\u3002 'BEVFusionFramework' \u00b6 \u603b\u6846\u67b6\uff0c\u8d1f\u8d23\u4e32\u8054 Camera Side\u3001Lidar Side \u4e0e BEV Space Integration \u7684\u6240\u6709\u6a21\u5757\uff0c \u5b9e\u73b0 BEVFusion \u7684\u5b8c\u6574\u63a8\u7406 pipeline\u3002 Bases: Module BEVFusionFramework\uff08BEVfusion \u7b97\u5b50\u7ea7\u878d\u5408\u6846\u67b6\uff09\u3002 \u8be5\u7c7b\u628a\u7b97\u5b50\u5c42 + \u7f51\u7edc\u5c42\u4e32\u8054\u8d77\u6765\uff0c\u5b9e\u73b0\uff1a \u591a\u89c6\u89d2\u56fe\u50cf + LiDAR \u70b9\u4e91 \u2192 Fused BEV Features \u7684\u5b8c\u6574\u524d\u5411\u6d41\u7a0b\u3002 Source code in BEVFusionFramework.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class BEVFusionFramework ( nn . Module ): \"\"\" BEVFusionFramework\uff08BEVfusion \u7b97\u5b50\u7ea7\u878d\u5408\u6846\u67b6\uff09\u3002 \u8be5\u7c7b\u628a\u7b97\u5b50\u5c42 + \u7f51\u7edc\u5c42\u4e32\u8054\u8d77\u6765\uff0c\u5b9e\u73b0\uff1a \u591a\u89c6\u89d2\u56fe\u50cf + LiDAR \u70b9\u4e91 \u2192 Fused BEV Features \u7684\u5b8c\u6574\u524d\u5411\u6d41\u7a0b\u3002 \"\"\" def __init__ ( self , camera_encoder : CameraEncoderInterface , lidar_preprocess : LidarPreprocessInterface , camera_fpn_cfg : dict , depth_head_cfg : dict , voxel_cfg : dict , vfe_cfg : dict , backbone3d_cfg : dict , bev_encoder_cfg : dict , ): super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 # \u63a5\u53e3\u6a21\u5757 self . camera_encoder = camera_encoder # \u76f8\u673a\u7f16\u7801\u63a5\u53e3 self . lidar_preprocess = lidar_preprocess # \u70b9\u4e91\u9884\u5904\u7406\u63a5\u53e3 # Camera side \u7f51\u7edc\u5c42\uff08\u7b97\u5b50 1 & 2\uff09 self . camera_fpn = CameraFPNFusion ( ** camera_fpn_cfg ) # \u56fe\u50cf\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408 self . depth_head = DepthDistributionHead ( ** depth_head_cfg ) # \u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b # LiDAR side \u7f51\u7edc\u5c42\uff08\u7b97\u5b50 7 / 8 / 9\uff09 self . lidar_voxelization = LidarVoxelization ( ** voxel_cfg ) # \u70b9\u4e91\u4f53\u7d20\u5316 self . vfe = VoxelFeatureEncoder ( ** vfe_cfg ) # \u4f53\u7d20\u7279\u5f81\u7f16\u7801 self . backbone3d = Sparse3DBackbone ( ** backbone3d_cfg ) # \u7a00\u758f 3D \u4e3b\u5e72 # BEV \u878d\u5408\u7f16\u7801\uff08\u7b97\u5b50 12\uff09 self . bev_encoder = BEVFusionEncoder ( ** bev_encoder_cfg ) # BEV \u878d\u5408\u7f16\u7801\u5668 def forward ( self , images : torch . Tensor , raw_points , pixels_2d : torch . Tensor , depths : torch . Tensor , K : torch . Tensor , R : torch . Tensor , t : torch . Tensor , bev_indices : torch . Tensor , batch_size : int , ) -> torch . Tensor : \"\"\" Args: images (torch.Tensor): \u591a\u89c6\u89d2\u56fe\u50cf (B, N_cam, 3, H, W)\u3002 # \u56fe\u50cf\u8f93\u5165 raw_points: \u539f\u59cb\u70b9\u4e91\u6570\u636e\uff0c\u7531 LidarPreprocessInterface \u8d1f\u8d23\u89e3\u6790\u3002 # \u70b9\u4e91\u8f93\u5165 pixels_2d (torch.Tensor): \u50cf\u7d20\u7f51\u683c (H_f, W_f, 2)\uff0c\u7528\u4e8e\u51e0\u4f55\u5347\u7ef4\u3002 # \u50cf\u7d20\u5750\u6807 depths (torch.Tensor): \u6df1\u5ea6\u79bb\u6563 bins (D,)\u3002 # \u6df1\u5ea6\u53d6\u6837 K (torch.Tensor): \u76f8\u673a\u5185\u53c2 (3, 3)\u3002 # \u5185\u53c2 R (torch.Tensor): \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u65cb\u8f6c\u77e9\u9635 (3, 3)\u3002 # \u65cb\u8f6c t (torch.Tensor): \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u5e73\u79fb\u5411\u91cf (3,)\u3002 # \u5e73\u79fb bev_indices (torch.Tensor): BEV \u7d22\u5f15 (B, D, H_f, W_f, 2)\u3002 # BEV \u7f51\u683c\u7d22\u5f15 batch_size (int): \u5f53\u524d batch \u5927\u5c0f\u3002 # B Returns: torch.Tensor: Fused BEV \u7279\u5f81 (B, C_bev, H_bev, W_bev)\u3002 # \u8f93\u51fa BEV \"\"\" # ===================== Camera side ===================== # \u63a5\u53e3\uff1a\u591a\u89c6\u89d2\u56fe\u50cf\u9884\u5904\u7406 + backbone \u2192 \u591a\u5c3a\u5ea6\u7279\u5f81\u5217\u8868 multi_scale_feats = self . camera_encoder ( images ) # [ (B,C_i,H_i,W_i), ... ] # \u7b97\u5b50 1\uff1aFPN \u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408 cam_fpn_feat = self . camera_fpn ( multi_scale_feats ) # (B, C_fpn, H_f, W_f) # \u7b97\u5b50 2\uff1a\u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b depth_prob = self . depth_head ( cam_fpn_feat ) # (B, D, H_f, W_f) # \u7b97\u5b50 5\uff1a\u6df1\u5ea6\u52a0\u6743\u7279\u5f81\u5c55\u5f00\uff08\u8c03\u7528\u81ea\u5b9a\u4e49\u7b97\u5b50\uff09 feat_3d = torch . ops . bevfusion_ops . depth_weighted_feature_expand ( cam_fpn_feat , depth_prob , ) # (B, D, C_fpn, H_f, W_f) # \u7b97\u5b50 3\uff1a\u50cf\u7d20 2D\u21923D \u5347\u7ef4\uff08\u53ef\u7528\u4e8e\u51e0\u4f55\u53ef\u89c6\u5316 / \u524d\u7f6e\u8ba1\u7b97\uff09 points_cam = torch . ops . bevfusion_ops . camera_lift_2d_to_3d ( pixels_2d , depths , K , R , t , ) # (D, H_f, W_f, 3) # \u7b97\u5b50 4\uff1a\u5750\u6807\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb\uff08\u5982 LiDAR / Ego\uff09 points_world = torch . ops . bevfusion_ops . camera_geometry_transform ( points_3d = points_cam , R = R , t = t , ) # (D, H_f, W_f, 3) _ = points_world # \u8fd9\u91cc\u793a\u610f\u4f7f\u7528\uff0c\u53ef\u5728\u9700\u8981\u65f6\u8fdb\u4e00\u6b65\u5229\u7528 # \u7b97\u5b50 6\uff1aCamera BEV \u805a\u5408 B , D , C_fpn , H_f , W_f = feat_3d . shape # \u8bfb\u53d6\u5c3a\u5bf8 bev_h = bev_indices . shape [ 2 ] # BEV \u9ad8\u5ea6 bev_w = bev_indices . shape [ 3 ] # BEV \u5bbd\u5ea6 cam_bev = torch . ops . bevfusion_ops . camera_bev_pooling ( feat_3d , bev_indices , bev_h , bev_w , ) # (B, C_fpn, bev_h, bev_w) # ===================== LiDAR side ====================== # \u63a5\u53e3\uff1a\u70b9\u4e91\u9884\u5904\u7406 points = self . lidar_preprocess ( raw_points ) # (B, N, C_pts) # \u7b97\u5b50 7\uff1a\u4f53\u7d20\u5316 voxels , coords , num_points = self . lidar_voxelization ( points ) # (M,T,Cv),(M,4),(M,) # \u7b97\u5b50 8\uff1a\u4f53\u7d20\u7279\u5f81\u7f16\u7801 voxel_feats = self . vfe ( voxels , num_points ) # (M, C_vfe) # \u7b97\u5b50 9\uff1a3D \u4e3b\u5e72\u7f51\u7edc feat_3d_lidar = self . backbone3d ( voxel_feats , coords , batch_size , ) # (B, C_3d, Z, Y, X) # \u7b97\u5b50 10\uff1a\u9ad8\u5ea6\u7ef4\u538b\u7f29 \u2192 LiDAR BEV lidar_bev = torch . ops . bevfusion_ops . lidar_flatten_z_to_bev ( feat_3d_lidar , ) # (B, C_lidar, bev_h, bev_w) # ===================== BEV \u878d\u5408 ======================== # \u7b97\u5b50 11\uff1aCamera BEV + LiDAR BEV \u901a\u9053\u62fc\u63a5 bev_concat = torch . ops . bevfusion_ops . bev_concat_fusion ( cam_bev , lidar_bev , ) # (B, C_in, bev_h, bev_w) # \u7b97\u5b50 12\uff1aBEV \u878d\u5408\u7f16\u7801 bev_out = self . bev_encoder ( bev_concat ) # (B, C_bev, bev_h, bev_w) return bev_out # \u8fd4\u56de\u6700\u7ec8 Fused BEV Features forward ( images , raw_points , pixels_2d , depths , K , R , t , bev_indices , batch_size ) \u00b6 Parameters: images ( Tensor ) \u2013 \u591a\u89c6\u89d2\u56fe\u50cf (B, N_cam, 3, H, W)\u3002 # \u56fe\u50cf\u8f93\u5165 raw_points \u2013 \u539f\u59cb\u70b9\u4e91\u6570\u636e\uff0c\u7531 LidarPreprocessInterface \u8d1f\u8d23\u89e3\u6790\u3002 # \u70b9\u4e91\u8f93\u5165 pixels_2d ( Tensor ) \u2013 \u50cf\u7d20\u7f51\u683c (H_f, W_f, 2)\uff0c\u7528\u4e8e\u51e0\u4f55\u5347\u7ef4\u3002 # \u50cf\u7d20\u5750\u6807 depths ( Tensor ) \u2013 \u6df1\u5ea6\u79bb\u6563 bins (D,)\u3002 # \u6df1\u5ea6\u53d6\u6837 K ( Tensor ) \u2013 \u76f8\u673a\u5185\u53c2 (3, 3)\u3002 # \u5185\u53c2 R ( Tensor ) \u2013 \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u65cb\u8f6c\u77e9\u9635 (3, 3)\u3002 # \u65cb\u8f6c t ( Tensor ) \u2013 \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u5e73\u79fb\u5411\u91cf (3,)\u3002 # \u5e73\u79fb bev_indices ( Tensor ) \u2013 BEV \u7d22\u5f15 (B, D, H_f, W_f, 2)\u3002 # BEV \u7f51\u683c\u7d22\u5f15 batch_size ( int ) \u2013 \u5f53\u524d batch \u5927\u5c0f\u3002 # B Returns: Tensor \u2013 torch.Tensor: Fused BEV \u7279\u5f81 (B, C_bev, H_bev, W_bev)\u3002 # \u8f93\u51fa BEV Source code in BEVFusionFramework.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def forward ( self , images : torch . Tensor , raw_points , pixels_2d : torch . Tensor , depths : torch . Tensor , K : torch . Tensor , R : torch . Tensor , t : torch . Tensor , bev_indices : torch . Tensor , batch_size : int , ) -> torch . Tensor : \"\"\" Args: images (torch.Tensor): \u591a\u89c6\u89d2\u56fe\u50cf (B, N_cam, 3, H, W)\u3002 # \u56fe\u50cf\u8f93\u5165 raw_points: \u539f\u59cb\u70b9\u4e91\u6570\u636e\uff0c\u7531 LidarPreprocessInterface \u8d1f\u8d23\u89e3\u6790\u3002 # \u70b9\u4e91\u8f93\u5165 pixels_2d (torch.Tensor): \u50cf\u7d20\u7f51\u683c (H_f, W_f, 2)\uff0c\u7528\u4e8e\u51e0\u4f55\u5347\u7ef4\u3002 # \u50cf\u7d20\u5750\u6807 depths (torch.Tensor): \u6df1\u5ea6\u79bb\u6563 bins (D,)\u3002 # \u6df1\u5ea6\u53d6\u6837 K (torch.Tensor): \u76f8\u673a\u5185\u53c2 (3, 3)\u3002 # \u5185\u53c2 R (torch.Tensor): \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u65cb\u8f6c\u77e9\u9635 (3, 3)\u3002 # \u65cb\u8f6c t (torch.Tensor): \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u5e73\u79fb\u5411\u91cf (3,)\u3002 # \u5e73\u79fb bev_indices (torch.Tensor): BEV \u7d22\u5f15 (B, D, H_f, W_f, 2)\u3002 # BEV \u7f51\u683c\u7d22\u5f15 batch_size (int): \u5f53\u524d batch \u5927\u5c0f\u3002 # B Returns: torch.Tensor: Fused BEV \u7279\u5f81 (B, C_bev, H_bev, W_bev)\u3002 # \u8f93\u51fa BEV \"\"\" # ===================== Camera side ===================== # \u63a5\u53e3\uff1a\u591a\u89c6\u89d2\u56fe\u50cf\u9884\u5904\u7406 + backbone \u2192 \u591a\u5c3a\u5ea6\u7279\u5f81\u5217\u8868 multi_scale_feats = self . camera_encoder ( images ) # [ (B,C_i,H_i,W_i), ... ] # \u7b97\u5b50 1\uff1aFPN \u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408 cam_fpn_feat = self . camera_fpn ( multi_scale_feats ) # (B, C_fpn, H_f, W_f) # \u7b97\u5b50 2\uff1a\u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b depth_prob = self . depth_head ( cam_fpn_feat ) # (B, D, H_f, W_f) # \u7b97\u5b50 5\uff1a\u6df1\u5ea6\u52a0\u6743\u7279\u5f81\u5c55\u5f00\uff08\u8c03\u7528\u81ea\u5b9a\u4e49\u7b97\u5b50\uff09 feat_3d = torch . ops . bevfusion_ops . depth_weighted_feature_expand ( cam_fpn_feat , depth_prob , ) # (B, D, C_fpn, H_f, W_f) # \u7b97\u5b50 3\uff1a\u50cf\u7d20 2D\u21923D \u5347\u7ef4\uff08\u53ef\u7528\u4e8e\u51e0\u4f55\u53ef\u89c6\u5316 / \u524d\u7f6e\u8ba1\u7b97\uff09 points_cam = torch . ops . bevfusion_ops . camera_lift_2d_to_3d ( pixels_2d , depths , K , R , t , ) # (D, H_f, W_f, 3) # \u7b97\u5b50 4\uff1a\u5750\u6807\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb\uff08\u5982 LiDAR / Ego\uff09 points_world = torch . ops . bevfusion_ops . camera_geometry_transform ( points_3d = points_cam , R = R , t = t , ) # (D, H_f, W_f, 3) _ = points_world # \u8fd9\u91cc\u793a\u610f\u4f7f\u7528\uff0c\u53ef\u5728\u9700\u8981\u65f6\u8fdb\u4e00\u6b65\u5229\u7528 # \u7b97\u5b50 6\uff1aCamera BEV \u805a\u5408 B , D , C_fpn , H_f , W_f = feat_3d . shape # \u8bfb\u53d6\u5c3a\u5bf8 bev_h = bev_indices . shape [ 2 ] # BEV \u9ad8\u5ea6 bev_w = bev_indices . shape [ 3 ] # BEV \u5bbd\u5ea6 cam_bev = torch . ops . bevfusion_ops . camera_bev_pooling ( feat_3d , bev_indices , bev_h , bev_w , ) # (B, C_fpn, bev_h, bev_w) # ===================== LiDAR side ====================== # \u63a5\u53e3\uff1a\u70b9\u4e91\u9884\u5904\u7406 points = self . lidar_preprocess ( raw_points ) # (B, N, C_pts) # \u7b97\u5b50 7\uff1a\u4f53\u7d20\u5316 voxels , coords , num_points = self . lidar_voxelization ( points ) # (M,T,Cv),(M,4),(M,) # \u7b97\u5b50 8\uff1a\u4f53\u7d20\u7279\u5f81\u7f16\u7801 voxel_feats = self . vfe ( voxels , num_points ) # (M, C_vfe) # \u7b97\u5b50 9\uff1a3D \u4e3b\u5e72\u7f51\u7edc feat_3d_lidar = self . backbone3d ( voxel_feats , coords , batch_size , ) # (B, C_3d, Z, Y, X) # \u7b97\u5b50 10\uff1a\u9ad8\u5ea6\u7ef4\u538b\u7f29 \u2192 LiDAR BEV lidar_bev = torch . ops . bevfusion_ops . lidar_flatten_z_to_bev ( feat_3d_lidar , ) # (B, C_lidar, bev_h, bev_w) # ===================== BEV \u878d\u5408 ======================== # \u7b97\u5b50 11\uff1aCamera BEV + LiDAR BEV \u901a\u9053\u62fc\u63a5 bev_concat = torch . ops . bevfusion_ops . bev_concat_fusion ( cam_bev , lidar_bev , ) # (B, C_in, bev_h, bev_w) # \u7b97\u5b50 12\uff1aBEV \u878d\u5408\u7f16\u7801 bev_out = self . bev_encoder ( bev_concat ) # (B, C_bev, bev_h, bev_w) return bev_out # \u8fd4\u56de\u6700\u7ec8 Fused BEV Features","title":"BEVFusion \u6846\u67b6\u603b\u89c8"},{"location":"BEVFusionFramework/#bevfusion-frameworkbevfusion","text":"BEVFusionFramework \u7684\u4f5c\u7528\u662f\uff1a \u7edf\u4e00\u7ec4\u7ec7\u56fe\u50cf\u4fa7\u3001\u70b9\u4e91\u4fa7\u4e0e BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757\uff0c \u5b9e\u73b0 BEVFusion \u7684\u5b8c\u6574\u63a8\u7406\u6d41\u7a0b\uff0c\u5305\u62ec\u56fe\u50cf\u5904\u7406\u3001\u70b9\u4e91\u5904\u7406\u3001BEV \u751f\u6210\u4e0e\u6700\u7ec8\u878d\u5408\u3002 \u672c\u6846\u67b6\u5305\u542b\u56db\u7c7b\u6838\u5fc3\u529f\u80fd\uff1a \u56fe\u50cf\u4fa7\u5904\u7406\u6d41\u7a0b \u8c03\u7528 Camera Encoder Interface\uff0c\u5bf9\u591a\u89c6\u89d2\u56fe\u50cf\u63d0\u53d6\u8bed\u4e49\u4e0e\u6df1\u5ea6\u7279\u5f81\uff0c\u5e76\u751f\u6210 Camera BEV \u7279\u5f81\u3002 \u70b9\u4e91\u4fa7\u5904\u7406\u6d41\u7a0b \u8c03\u7528 Lidar Preprocess Interface + \u70b9\u4e91\u4e3b\u5e72\u7f51\u7edc\uff0c\u751f\u6210 LiDAR BEV \u7279\u5f81\u3002 BEV \u7a7a\u95f4\u878d\u5408\u6d41\u7a0b \u5c06\u56fe\u50cf BEV \u4e0e\u70b9\u4e91 BEV \u7279\u5f81\u8fdb\u884c\u62fc\u63a5\u6216\u5176\u4ed6\u878d\u5408\u65b9\u5f0f\uff0c\u5f62\u6210\u7edf\u4e00 BEV \u8868\u793a\u3002 \u63a8\u7406\u7ec4\u7ec7\u903b\u8f91 \u5c01\u88c5\u8f93\u5165 \u2192 \u63d0\u53d6\u7279\u5f81 \u2192 BEV\u751f\u6210 \u2192 \u878d\u5408 \u7684\u5b8c\u6574\u6d41\u7a0b\uff0c\u4f5c\u4e3a\u6a21\u578b\u6574\u4f53\u5165\u53e3\u3002","title":"BEVFusion Framework\uff08BEVFusion \u603b\u4f53\u6846\u67b6\uff09"},{"location":"BEVFusionFramework/#api","text":"\u4ee5\u4e0b\u5217\u51fa BEVFusionFramework \u7684\u4e3b\u6846\u67b6\u7c7b\u4e0e\u8c03\u7528\u7ed3\u6784\u3002","title":"API \u6587\u6863"},{"location":"BEVFusionFramework/#bevfusionframework","text":"\u603b\u6846\u67b6\uff0c\u8d1f\u8d23\u4e32\u8054 Camera Side\u3001Lidar Side \u4e0e BEV Space Integration \u7684\u6240\u6709\u6a21\u5757\uff0c \u5b9e\u73b0 BEVFusion \u7684\u5b8c\u6574\u63a8\u7406 pipeline\u3002 Bases: Module BEVFusionFramework\uff08BEVfusion \u7b97\u5b50\u7ea7\u878d\u5408\u6846\u67b6\uff09\u3002 \u8be5\u7c7b\u628a\u7b97\u5b50\u5c42 + \u7f51\u7edc\u5c42\u4e32\u8054\u8d77\u6765\uff0c\u5b9e\u73b0\uff1a \u591a\u89c6\u89d2\u56fe\u50cf + LiDAR \u70b9\u4e91 \u2192 Fused BEV Features \u7684\u5b8c\u6574\u524d\u5411\u6d41\u7a0b\u3002 Source code in BEVFusionFramework.py 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 class BEVFusionFramework ( nn . Module ): \"\"\" BEVFusionFramework\uff08BEVfusion \u7b97\u5b50\u7ea7\u878d\u5408\u6846\u67b6\uff09\u3002 \u8be5\u7c7b\u628a\u7b97\u5b50\u5c42 + \u7f51\u7edc\u5c42\u4e32\u8054\u8d77\u6765\uff0c\u5b9e\u73b0\uff1a \u591a\u89c6\u89d2\u56fe\u50cf + LiDAR \u70b9\u4e91 \u2192 Fused BEV Features \u7684\u5b8c\u6574\u524d\u5411\u6d41\u7a0b\u3002 \"\"\" def __init__ ( self , camera_encoder : CameraEncoderInterface , lidar_preprocess : LidarPreprocessInterface , camera_fpn_cfg : dict , depth_head_cfg : dict , voxel_cfg : dict , vfe_cfg : dict , backbone3d_cfg : dict , bev_encoder_cfg : dict , ): super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 # \u63a5\u53e3\u6a21\u5757 self . camera_encoder = camera_encoder # \u76f8\u673a\u7f16\u7801\u63a5\u53e3 self . lidar_preprocess = lidar_preprocess # \u70b9\u4e91\u9884\u5904\u7406\u63a5\u53e3 # Camera side \u7f51\u7edc\u5c42\uff08\u7b97\u5b50 1 & 2\uff09 self . camera_fpn = CameraFPNFusion ( ** camera_fpn_cfg ) # \u56fe\u50cf\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408 self . depth_head = DepthDistributionHead ( ** depth_head_cfg ) # \u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b # LiDAR side \u7f51\u7edc\u5c42\uff08\u7b97\u5b50 7 / 8 / 9\uff09 self . lidar_voxelization = LidarVoxelization ( ** voxel_cfg ) # \u70b9\u4e91\u4f53\u7d20\u5316 self . vfe = VoxelFeatureEncoder ( ** vfe_cfg ) # \u4f53\u7d20\u7279\u5f81\u7f16\u7801 self . backbone3d = Sparse3DBackbone ( ** backbone3d_cfg ) # \u7a00\u758f 3D \u4e3b\u5e72 # BEV \u878d\u5408\u7f16\u7801\uff08\u7b97\u5b50 12\uff09 self . bev_encoder = BEVFusionEncoder ( ** bev_encoder_cfg ) # BEV \u878d\u5408\u7f16\u7801\u5668 def forward ( self , images : torch . Tensor , raw_points , pixels_2d : torch . Tensor , depths : torch . Tensor , K : torch . Tensor , R : torch . Tensor , t : torch . Tensor , bev_indices : torch . Tensor , batch_size : int , ) -> torch . Tensor : \"\"\" Args: images (torch.Tensor): \u591a\u89c6\u89d2\u56fe\u50cf (B, N_cam, 3, H, W)\u3002 # \u56fe\u50cf\u8f93\u5165 raw_points: \u539f\u59cb\u70b9\u4e91\u6570\u636e\uff0c\u7531 LidarPreprocessInterface \u8d1f\u8d23\u89e3\u6790\u3002 # \u70b9\u4e91\u8f93\u5165 pixels_2d (torch.Tensor): \u50cf\u7d20\u7f51\u683c (H_f, W_f, 2)\uff0c\u7528\u4e8e\u51e0\u4f55\u5347\u7ef4\u3002 # \u50cf\u7d20\u5750\u6807 depths (torch.Tensor): \u6df1\u5ea6\u79bb\u6563 bins (D,)\u3002 # \u6df1\u5ea6\u53d6\u6837 K (torch.Tensor): \u76f8\u673a\u5185\u53c2 (3, 3)\u3002 # \u5185\u53c2 R (torch.Tensor): \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u65cb\u8f6c\u77e9\u9635 (3, 3)\u3002 # \u65cb\u8f6c t (torch.Tensor): \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u5e73\u79fb\u5411\u91cf (3,)\u3002 # \u5e73\u79fb bev_indices (torch.Tensor): BEV \u7d22\u5f15 (B, D, H_f, W_f, 2)\u3002 # BEV \u7f51\u683c\u7d22\u5f15 batch_size (int): \u5f53\u524d batch \u5927\u5c0f\u3002 # B Returns: torch.Tensor: Fused BEV \u7279\u5f81 (B, C_bev, H_bev, W_bev)\u3002 # \u8f93\u51fa BEV \"\"\" # ===================== Camera side ===================== # \u63a5\u53e3\uff1a\u591a\u89c6\u89d2\u56fe\u50cf\u9884\u5904\u7406 + backbone \u2192 \u591a\u5c3a\u5ea6\u7279\u5f81\u5217\u8868 multi_scale_feats = self . camera_encoder ( images ) # [ (B,C_i,H_i,W_i), ... ] # \u7b97\u5b50 1\uff1aFPN \u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408 cam_fpn_feat = self . camera_fpn ( multi_scale_feats ) # (B, C_fpn, H_f, W_f) # \u7b97\u5b50 2\uff1a\u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b depth_prob = self . depth_head ( cam_fpn_feat ) # (B, D, H_f, W_f) # \u7b97\u5b50 5\uff1a\u6df1\u5ea6\u52a0\u6743\u7279\u5f81\u5c55\u5f00\uff08\u8c03\u7528\u81ea\u5b9a\u4e49\u7b97\u5b50\uff09 feat_3d = torch . ops . bevfusion_ops . depth_weighted_feature_expand ( cam_fpn_feat , depth_prob , ) # (B, D, C_fpn, H_f, W_f) # \u7b97\u5b50 3\uff1a\u50cf\u7d20 2D\u21923D \u5347\u7ef4\uff08\u53ef\u7528\u4e8e\u51e0\u4f55\u53ef\u89c6\u5316 / \u524d\u7f6e\u8ba1\u7b97\uff09 points_cam = torch . ops . bevfusion_ops . camera_lift_2d_to_3d ( pixels_2d , depths , K , R , t , ) # (D, H_f, W_f, 3) # \u7b97\u5b50 4\uff1a\u5750\u6807\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb\uff08\u5982 LiDAR / Ego\uff09 points_world = torch . ops . bevfusion_ops . camera_geometry_transform ( points_3d = points_cam , R = R , t = t , ) # (D, H_f, W_f, 3) _ = points_world # \u8fd9\u91cc\u793a\u610f\u4f7f\u7528\uff0c\u53ef\u5728\u9700\u8981\u65f6\u8fdb\u4e00\u6b65\u5229\u7528 # \u7b97\u5b50 6\uff1aCamera BEV \u805a\u5408 B , D , C_fpn , H_f , W_f = feat_3d . shape # \u8bfb\u53d6\u5c3a\u5bf8 bev_h = bev_indices . shape [ 2 ] # BEV \u9ad8\u5ea6 bev_w = bev_indices . shape [ 3 ] # BEV \u5bbd\u5ea6 cam_bev = torch . ops . bevfusion_ops . camera_bev_pooling ( feat_3d , bev_indices , bev_h , bev_w , ) # (B, C_fpn, bev_h, bev_w) # ===================== LiDAR side ====================== # \u63a5\u53e3\uff1a\u70b9\u4e91\u9884\u5904\u7406 points = self . lidar_preprocess ( raw_points ) # (B, N, C_pts) # \u7b97\u5b50 7\uff1a\u4f53\u7d20\u5316 voxels , coords , num_points = self . lidar_voxelization ( points ) # (M,T,Cv),(M,4),(M,) # \u7b97\u5b50 8\uff1a\u4f53\u7d20\u7279\u5f81\u7f16\u7801 voxel_feats = self . vfe ( voxels , num_points ) # (M, C_vfe) # \u7b97\u5b50 9\uff1a3D \u4e3b\u5e72\u7f51\u7edc feat_3d_lidar = self . backbone3d ( voxel_feats , coords , batch_size , ) # (B, C_3d, Z, Y, X) # \u7b97\u5b50 10\uff1a\u9ad8\u5ea6\u7ef4\u538b\u7f29 \u2192 LiDAR BEV lidar_bev = torch . ops . bevfusion_ops . lidar_flatten_z_to_bev ( feat_3d_lidar , ) # (B, C_lidar, bev_h, bev_w) # ===================== BEV \u878d\u5408 ======================== # \u7b97\u5b50 11\uff1aCamera BEV + LiDAR BEV \u901a\u9053\u62fc\u63a5 bev_concat = torch . ops . bevfusion_ops . bev_concat_fusion ( cam_bev , lidar_bev , ) # (B, C_in, bev_h, bev_w) # \u7b97\u5b50 12\uff1aBEV \u878d\u5408\u7f16\u7801 bev_out = self . bev_encoder ( bev_concat ) # (B, C_bev, bev_h, bev_w) return bev_out # \u8fd4\u56de\u6700\u7ec8 Fused BEV Features","title":"'BEVFusionFramework'"},{"location":"BEVFusionFramework/#BEVFusionFramework.BEVFusionFramework.forward","text":"Parameters: images ( Tensor ) \u2013 \u591a\u89c6\u89d2\u56fe\u50cf (B, N_cam, 3, H, W)\u3002 # \u56fe\u50cf\u8f93\u5165 raw_points \u2013 \u539f\u59cb\u70b9\u4e91\u6570\u636e\uff0c\u7531 LidarPreprocessInterface \u8d1f\u8d23\u89e3\u6790\u3002 # \u70b9\u4e91\u8f93\u5165 pixels_2d ( Tensor ) \u2013 \u50cf\u7d20\u7f51\u683c (H_f, W_f, 2)\uff0c\u7528\u4e8e\u51e0\u4f55\u5347\u7ef4\u3002 # \u50cf\u7d20\u5750\u6807 depths ( Tensor ) \u2013 \u6df1\u5ea6\u79bb\u6563 bins (D,)\u3002 # \u6df1\u5ea6\u53d6\u6837 K ( Tensor ) \u2013 \u76f8\u673a\u5185\u53c2 (3, 3)\u3002 # \u5185\u53c2 R ( Tensor ) \u2013 \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u65cb\u8f6c\u77e9\u9635 (3, 3)\u3002 # \u65cb\u8f6c t ( Tensor ) \u2013 \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u5e73\u79fb\u5411\u91cf (3,)\u3002 # \u5e73\u79fb bev_indices ( Tensor ) \u2013 BEV \u7d22\u5f15 (B, D, H_f, W_f, 2)\u3002 # BEV \u7f51\u683c\u7d22\u5f15 batch_size ( int ) \u2013 \u5f53\u524d batch \u5927\u5c0f\u3002 # B Returns: Tensor \u2013 torch.Tensor: Fused BEV \u7279\u5f81 (B, C_bev, H_bev, W_bev)\u3002 # \u8f93\u51fa BEV Source code in BEVFusionFramework.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 def forward ( self , images : torch . Tensor , raw_points , pixels_2d : torch . Tensor , depths : torch . Tensor , K : torch . Tensor , R : torch . Tensor , t : torch . Tensor , bev_indices : torch . Tensor , batch_size : int , ) -> torch . Tensor : \"\"\" Args: images (torch.Tensor): \u591a\u89c6\u89d2\u56fe\u50cf (B, N_cam, 3, H, W)\u3002 # \u56fe\u50cf\u8f93\u5165 raw_points: \u539f\u59cb\u70b9\u4e91\u6570\u636e\uff0c\u7531 LidarPreprocessInterface \u8d1f\u8d23\u89e3\u6790\u3002 # \u70b9\u4e91\u8f93\u5165 pixels_2d (torch.Tensor): \u50cf\u7d20\u7f51\u683c (H_f, W_f, 2)\uff0c\u7528\u4e8e\u51e0\u4f55\u5347\u7ef4\u3002 # \u50cf\u7d20\u5750\u6807 depths (torch.Tensor): \u6df1\u5ea6\u79bb\u6563 bins (D,)\u3002 # \u6df1\u5ea6\u53d6\u6837 K (torch.Tensor): \u76f8\u673a\u5185\u53c2 (3, 3)\u3002 # \u5185\u53c2 R (torch.Tensor): \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u65cb\u8f6c\u77e9\u9635 (3, 3)\u3002 # \u65cb\u8f6c t (torch.Tensor): \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u5e73\u79fb\u5411\u91cf (3,)\u3002 # \u5e73\u79fb bev_indices (torch.Tensor): BEV \u7d22\u5f15 (B, D, H_f, W_f, 2)\u3002 # BEV \u7f51\u683c\u7d22\u5f15 batch_size (int): \u5f53\u524d batch \u5927\u5c0f\u3002 # B Returns: torch.Tensor: Fused BEV \u7279\u5f81 (B, C_bev, H_bev, W_bev)\u3002 # \u8f93\u51fa BEV \"\"\" # ===================== Camera side ===================== # \u63a5\u53e3\uff1a\u591a\u89c6\u89d2\u56fe\u50cf\u9884\u5904\u7406 + backbone \u2192 \u591a\u5c3a\u5ea6\u7279\u5f81\u5217\u8868 multi_scale_feats = self . camera_encoder ( images ) # [ (B,C_i,H_i,W_i), ... ] # \u7b97\u5b50 1\uff1aFPN \u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408 cam_fpn_feat = self . camera_fpn ( multi_scale_feats ) # (B, C_fpn, H_f, W_f) # \u7b97\u5b50 2\uff1a\u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b depth_prob = self . depth_head ( cam_fpn_feat ) # (B, D, H_f, W_f) # \u7b97\u5b50 5\uff1a\u6df1\u5ea6\u52a0\u6743\u7279\u5f81\u5c55\u5f00\uff08\u8c03\u7528\u81ea\u5b9a\u4e49\u7b97\u5b50\uff09 feat_3d = torch . ops . bevfusion_ops . depth_weighted_feature_expand ( cam_fpn_feat , depth_prob , ) # (B, D, C_fpn, H_f, W_f) # \u7b97\u5b50 3\uff1a\u50cf\u7d20 2D\u21923D \u5347\u7ef4\uff08\u53ef\u7528\u4e8e\u51e0\u4f55\u53ef\u89c6\u5316 / \u524d\u7f6e\u8ba1\u7b97\uff09 points_cam = torch . ops . bevfusion_ops . camera_lift_2d_to_3d ( pixels_2d , depths , K , R , t , ) # (D, H_f, W_f, 3) # \u7b97\u5b50 4\uff1a\u5750\u6807\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb\uff08\u5982 LiDAR / Ego\uff09 points_world = torch . ops . bevfusion_ops . camera_geometry_transform ( points_3d = points_cam , R = R , t = t , ) # (D, H_f, W_f, 3) _ = points_world # \u8fd9\u91cc\u793a\u610f\u4f7f\u7528\uff0c\u53ef\u5728\u9700\u8981\u65f6\u8fdb\u4e00\u6b65\u5229\u7528 # \u7b97\u5b50 6\uff1aCamera BEV \u805a\u5408 B , D , C_fpn , H_f , W_f = feat_3d . shape # \u8bfb\u53d6\u5c3a\u5bf8 bev_h = bev_indices . shape [ 2 ] # BEV \u9ad8\u5ea6 bev_w = bev_indices . shape [ 3 ] # BEV \u5bbd\u5ea6 cam_bev = torch . ops . bevfusion_ops . camera_bev_pooling ( feat_3d , bev_indices , bev_h , bev_w , ) # (B, C_fpn, bev_h, bev_w) # ===================== LiDAR side ====================== # \u63a5\u53e3\uff1a\u70b9\u4e91\u9884\u5904\u7406 points = self . lidar_preprocess ( raw_points ) # (B, N, C_pts) # \u7b97\u5b50 7\uff1a\u4f53\u7d20\u5316 voxels , coords , num_points = self . lidar_voxelization ( points ) # (M,T,Cv),(M,4),(M,) # \u7b97\u5b50 8\uff1a\u4f53\u7d20\u7279\u5f81\u7f16\u7801 voxel_feats = self . vfe ( voxels , num_points ) # (M, C_vfe) # \u7b97\u5b50 9\uff1a3D \u4e3b\u5e72\u7f51\u7edc feat_3d_lidar = self . backbone3d ( voxel_feats , coords , batch_size , ) # (B, C_3d, Z, Y, X) # \u7b97\u5b50 10\uff1a\u9ad8\u5ea6\u7ef4\u538b\u7f29 \u2192 LiDAR BEV lidar_bev = torch . ops . bevfusion_ops . lidar_flatten_z_to_bev ( feat_3d_lidar , ) # (B, C_lidar, bev_h, bev_w) # ===================== BEV \u878d\u5408 ======================== # \u7b97\u5b50 11\uff1aCamera BEV + LiDAR BEV \u901a\u9053\u62fc\u63a5 bev_concat = torch . ops . bevfusion_ops . bev_concat_fusion ( cam_bev , lidar_bev , ) # (B, C_in, bev_h, bev_w) # \u7b97\u5b50 12\uff1aBEV \u878d\u5408\u7f16\u7801 bev_out = self . bev_encoder ( bev_concat ) # (B, C_bev, bev_h, bev_w) return bev_out # \u8fd4\u56de\u6700\u7ec8 Fused BEV Features","title":"forward"},{"location":"bev_space_integration/","text":"BEV Space Integration\uff08BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757\uff09 \u00b6 BEV Space Integration \u6a21\u5757\u7684\u4f5c\u7528\u662f\uff1a \u5c06\u6765\u81ea\u56fe\u50cf\u4fa7\u4e0e\u70b9\u4e91\u4fa7\u7684 BEV \u7279\u5f81\u8fdb\u884c\u7edf\u4e00\u7a7a\u95f4\u8868\u793a\u4e0e\u878d\u5408\uff0c \u5e76\u901a\u8fc7 BEV \u7f16\u7801\u5668\u63d0\u53d6\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\uff0c\u4e3a\u6700\u7ec8\u4efb\u52a1\u68c0\u6d4b\u5934\u63d0\u4f9b\u8f93\u5165\u3002 \u672c\u6a21\u5757\u5305\u542b\u56db\u7c7b\u6838\u5fc3\u529f\u80fd\uff1a \u56fe\u50cf BEV \u7279\u5f81\u9884\u5904\u7406 \u63a5\u6536 Camera Side \u7684 BEV \u7279\u5f81\u5e76\u8fdb\u884c\u7ef4\u5ea6\u53d8\u6362\u6216\u5bf9\u9f50\u3002 \u70b9\u4e91 BEV \u7279\u5f81\u9884\u5904\u7406 \u63a5\u6536 Lidar Side \u7684 BEV \u7279\u5f81\u5e76\u8fdb\u884c\u901a\u9053\u3001\u5c3a\u5ea6\u5bf9\u9f50\u3002 \u591a\u6a21\u6001 BEV \u878d\u5408\u7b97\u5b50 \u5c06\u56fe\u50cf BEV \u4e0e\u70b9\u4e91 BEV \u5728\u540c\u4e00\u7a7a\u95f4\u4e2d\u878d\u5408\uff08\u5982 concat / add / weighted fusion\uff09\u3002 BEV \u7f16\u7801\u5668\uff08BEV Encoder\uff09 \u57fa\u4e8e 2D CNN \u63d0\u53d6\u6700\u7ec8\u9ad8\u5c42 BEV \u8bed\u4e49\u7279\u5f81\u3002 API \u6587\u6863 \u00b6 \u4ee5\u4e0b\u5217\u51fa BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757\u4e2d\u7684\u7b97\u5b50\u4e0e\u7f51\u7edc\u6a21\u5757\uff0c\u5e76\u7ed9\u51fa\u5bf9\u5e94\u6587\u4ef6\u8def\u5f84\u3002 'bev_concat_fusion' \u00b6 \u5c06 Camera BEV \u4e0e LiDAR BEV \u8fdb\u884c\u901a\u9053\u7ef4\u62fc\u63a5\u878d\u5408\u3002 bev_concat_fusion ( bev_cam , bev_lidar , dim = 1 ) \u00b6 \u5c06\u76f8\u673a\u5206\u652f\u4e0e LiDAR \u5206\u652f\u5728 BEV \u5e73\u9762\u4e0a\u7684\u7279\u5f81\u56fe\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u62fc\u63a5\uff0c \u5f97\u5230\u878d\u5408\u540e\u7684 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6\u4e3a (B, C_cam + C_lidar, H, W)\u3002 Parameters: bev_cam ( Tensor ) \u2013 (B, C_cam, H, W) \u76f8\u673a\u5206\u652f\u7684 BEV \u7279\u5f81\u56fe\uff1a - B: batch size - C_cam: \u76f8\u673a BEV \u7279\u5f81\u901a\u9053\u6570 - H, W: BEV \u7f51\u683c\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002 bev_lidar ( Tensor ) \u2013 (B, C_lidar, H, W) LiDAR \u5206\u652f\u7684 BEV \u7279\u5f81\u56fe\uff1a - C_lidar: LiDAR BEV \u7279\u5f81\u901a\u9053\u6570 - H, W: \u9700\u4e0e bev_cam \u5728\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u5bf9\u9f50\u3002 dim ( int , default: 1 ) \u2013 int \u62fc\u63a5\u7ef4\u5ea6\uff0c\u9ed8\u8ba4\u4e3a 1\uff08\u901a\u9053\u7ef4\u5ea6\uff09\uff0c\u901a\u5e38\u4e0d\u9700\u8981\u4fee\u6539\u3002 Returns: bev_fused ( Tensor ) \u2013 (B, C_cam + C_lidar, H, W) \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u62fc\u63a5\u540e\u7684\u878d\u5408 BEV \u7279\u5f81\u56fe\uff0c\u53ef\u4f5c\u4e3a\u540e\u7eed BEV \u878d\u5408\u7f16\u7801\u7b97\u5b50\uff08\u5982 ConvFuser\uff09\u7684\u8f93\u5165\u3002 Source code in BEV_space_integration\\operator\\bev_concat_fusion.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @torch . library . custom_op ( \"bevfusion_ops::bev_concat_fusion\" , mutates_args = [] ) def bev_concat_fusion ( bev_cam : torch . Tensor , bev_lidar : torch . Tensor , dim : int = 1 , ) -> torch . Tensor : \"\"\" \u5c06\u76f8\u673a\u5206\u652f\u4e0e LiDAR \u5206\u652f\u5728 BEV \u5e73\u9762\u4e0a\u7684\u7279\u5f81\u56fe\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u62fc\u63a5\uff0c \u5f97\u5230\u878d\u5408\u540e\u7684 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6\u4e3a (B, C_cam + C_lidar, H, W)\u3002 Args: bev_cam: (B, C_cam, H, W) \u76f8\u673a\u5206\u652f\u7684 BEV \u7279\u5f81\u56fe\uff1a - B: batch size - C_cam: \u76f8\u673a BEV \u7279\u5f81\u901a\u9053\u6570 - H, W: BEV \u7f51\u683c\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002 bev_lidar: (B, C_lidar, H, W) LiDAR \u5206\u652f\u7684 BEV \u7279\u5f81\u56fe\uff1a - C_lidar: LiDAR BEV \u7279\u5f81\u901a\u9053\u6570 - H, W: \u9700\u4e0e bev_cam \u5728\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u5bf9\u9f50\u3002 dim: int \u62fc\u63a5\u7ef4\u5ea6\uff0c\u9ed8\u8ba4\u4e3a 1\uff08\u901a\u9053\u7ef4\u5ea6\uff09\uff0c\u901a\u5e38\u4e0d\u9700\u8981\u4fee\u6539\u3002 Returns: bev_fused: (B, C_cam + C_lidar, H, W) \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u62fc\u63a5\u540e\u7684\u878d\u5408 BEV \u7279\u5f81\u56fe\uff0c\u53ef\u4f5c\u4e3a\u540e\u7eed BEV \u878d\u5408\u7f16\u7801\u7b97\u5b50\uff08\u5982 ConvFuser\uff09\u7684\u8f93\u5165\u3002 \"\"\" assert bev_cam . dim () == 4 , \"bev_cam \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u68c0\u67e5\u76f8\u673a BEV \u5f62\u72b6 assert bev_lidar . dim () == 4 , \"bev_lidar \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u68c0\u67e5 LiDAR BEV \u5f62\u72b6 assert bev_cam . shape [ 0 ] == bev_lidar . shape [ 0 ], \"batch \u7ef4\u5ea6\u4e0d\u5339\u914d\" # \u68c0\u67e5 batch \u7ef4 assert bev_cam . shape [ 2 :] == bev_lidar . shape [ 2 :], \"\u7a7a\u95f4\u5c3a\u5bf8 (H,W) \u4e0d\u5339\u914d\" # \u68c0\u67e5\u7a7a\u95f4\u5c3a\u5bf8 bev_fused = torch . cat ([ bev_cam , bev_lidar ], dim = dim ) # \u5728\u901a\u9053\u7ef4\u5ea6\u62fc\u63a5\u7279\u5f81 return bev_fused # \u8fd4\u56de\u878d\u5408 BEV \u7279\u5f81 'BEVFusionEncoder' \u00b6 2D BEV \u7f16\u7801\u5668\uff0c\u5bf9\u878d\u5408\u540e\u7684 BEV \u7279\u5f81\u8fdb\u884c\u5377\u79ef\u5904\u7406\u3002 BEVFusionEncoder \u00b6 Bases: Module BEVFusionEncoder \u6a21\u5757\uff08BEV \u878d\u5408\u7f16\u7801\u7b97\u5b50 / ConvFuser\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u5728 BEV \u5e73\u9762\u4e0a\u62fc\u63a5\u540e\u7684\u591a\u6a21\u6001\u7279\u5f81\uff08Camera BEV + LiDAR BEV\uff09\uff0c \u901a\u8fc7\u82e5\u5e72\u5c42 2D \u5377\u79ef\u5bf9\u901a\u9053\u7ef4\u4e0e\u7a7a\u95f4\u7ef4\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff0c\u8f93\u51fa\u6700\u7ec8\u7684 Fused BEV Features\uff0c\u4f5c\u4e3a\u540e\u7eed\u68c0\u6d4b\u3001\u5206\u5272\u7b49\u4efb\u52a1\u5934\u7684\u7edf\u4e00\u8f93\u5165\u3002 \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 \u00b6 Conv2d (C_in \u2192 C_mid, 3\u00d73) + BN + ReLU \u878d\u5408\u4e0d\u540c\u6a21\u6001\u901a\u9053\u4fe1\u606f Conv2d (C_mid \u2192 C_out, 3\u00d73) + BN + ReLU \u8fdb\u4e00\u6b65\u63d0\u53d6 BEV \u7a7a\u95f4\u7ed3\u6784\u7279\u5f81 \uff08\u53ef\u9009\uff09\u6b8b\u5dee\u8fde\u63a5\u589e\u5f3a\u7a33\u5b9a\u6027 \u8f93\u5165 \u00b6 Args: x: (B, C_in, H, W) \u62fc\u63a5\u540e\u7684 BEV \u7279\u5f81\u56fe\uff1a - B: batch size - C_in: Camera BEV \u901a\u9053\u6570 + LiDAR BEV \u901a\u9053\u6570 - H, W: BEV \u7f51\u683c\u7a7a\u95f4\u5c3a\u5bf8 \u8f93\u51fa \u00b6 Returns: fused_bev: (B, C_out, H, W) \u878d\u5408\u7f16\u7801\u540e\u7684\u6700\u7ec8 BEV \u7279\u5f81\u56fe\u3002 \u6a21\u5757\u7528\u9014 \u00b6 \u4f5c\u4e3a Camera BEV \u4e0e LiDAR BEV \u4e4b\u540e\u7684\u6700\u7ec8\u878d\u5408\u7f16\u7801\u6a21\u5757 \u8f93\u51fa\u7684 Fused BEV Features \u76f4\u63a5\u7528\u4e8e\u4e0b\u6e38 3D \u68c0\u6d4b / \u5730\u56fe\u5206\u5272\u7b49\u4efb\u52a1 Source code in BEV_space_integration\\network\\BEVFusionEncoder.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class BEVFusionEncoder ( nn . Module ): \"\"\" BEVFusionEncoder \u6a21\u5757\uff08BEV \u878d\u5408\u7f16\u7801\u7b97\u5b50 / ConvFuser\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u5728 BEV \u5e73\u9762\u4e0a\u62fc\u63a5\u540e\u7684\u591a\u6a21\u6001\u7279\u5f81\uff08Camera BEV + LiDAR BEV\uff09\uff0c \u901a\u8fc7\u82e5\u5e72\u5c42 2D \u5377\u79ef\u5bf9\u901a\u9053\u7ef4\u4e0e\u7a7a\u95f4\u7ef4\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff0c\u8f93\u51fa\u6700\u7ec8\u7684 Fused BEV Features\uff0c\u4f5c\u4e3a\u540e\u7eed\u68c0\u6d4b\u3001\u5206\u5272\u7b49\u4efb\u52a1\u5934\u7684\u7edf\u4e00\u8f93\u5165\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. **Conv2d (C_in \u2192 C_mid, 3\u00d73) + BN + ReLU** - \u878d\u5408\u4e0d\u540c\u6a21\u6001\u901a\u9053\u4fe1\u606f 2. **Conv2d (C_mid \u2192 C_out, 3\u00d73) + BN + ReLU** - \u8fdb\u4e00\u6b65\u63d0\u53d6 BEV \u7a7a\u95f4\u7ed3\u6784\u7279\u5f81 3. \uff08\u53ef\u9009\uff09\u6b8b\u5dee\u8fde\u63a5\u589e\u5f3a\u7a33\u5b9a\u6027 --- ### \u8f93\u5165 Args: x: (B, C_in, H, W) \u62fc\u63a5\u540e\u7684 BEV \u7279\u5f81\u56fe\uff1a - B: batch size - C_in: Camera BEV \u901a\u9053\u6570 + LiDAR BEV \u901a\u9053\u6570 - H, W: BEV \u7f51\u683c\u7a7a\u95f4\u5c3a\u5bf8 --- ### \u8f93\u51fa Returns: fused_bev: (B, C_out, H, W) \u878d\u5408\u7f16\u7801\u540e\u7684\u6700\u7ec8 BEV \u7279\u5f81\u56fe\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u4f5c\u4e3a Camera BEV \u4e0e LiDAR BEV \u4e4b\u540e\u7684\u6700\u7ec8\u878d\u5408\u7f16\u7801\u6a21\u5757 - \u8f93\u51fa\u7684 Fused BEV Features \u76f4\u63a5\u7528\u4e8e\u4e0b\u6e38 3D \u68c0\u6d4b / \u5730\u56fe\u5206\u5272\u7b49\u4efb\u52a1 \"\"\" def __init__ ( self , in_channels : int , mid_channels : int , out_channels : int , use_residual : bool = False , ): \"\"\" Args: in_channels (int): \u8f93\u5165 BEV \u7279\u5f81\u901a\u9053\u6570 C_in\u3002 # \u8f93\u5165\u901a\u9053 mid_channels (int): \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels (int): \u8f93\u51fa BEV \u7279\u5f81\u901a\u9053\u6570 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_residual (bool): \u662f\u5426\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u3002 # \u662f\u5426\u542f\u7528\u6b8b\u5dee \"\"\" super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . use_residual = use_residual # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528\u6b8b\u5dee self . conv1 = nn . Conv2d ( in_channels , mid_channels , kernel_size = 3 , padding = 1 , bias = False ) # \u7b2c\u4e00\u5c42 3\u00d73 Conv self . bn1 = nn . BatchNorm2d ( mid_channels ) # BN self . conv2 = nn . Conv2d ( mid_channels , out_channels , kernel_size = 3 , padding = 1 , bias = False ) # \u7b2c\u4e8c\u5c42 3\u00d73 Conv self . bn2 = nn . BatchNorm2d ( out_channels ) # BN if use_residual and in_channels != out_channels : # \u82e5\u901a\u9053\u4e0d\u4e00\u81f4\u4e14\u542f\u7528\u6b8b\u5dee self . res_conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 , bias = False ) # \u901a\u9053\u5bf9\u9f50\u5377\u79ef else : self . res_conv = None # \u4e0d\u4f7f\u7528\u6b8b\u5dee\u5bf9\u9f50 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x (torch.Tensor): \u8f93\u5165 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: torch.Tensor: \u878d\u5408\u7f16\u7801\u540e\u7684 BEV \u7279\u5f81\u56fe (B, C_out, H, W)\u3002 # \u8f93\u51fa\u7279\u5f81 \"\"\" assert x . dim () == 4 , \"x \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 identity = x # \u4fdd\u5b58\u6b8b\u5dee\u5206\u652f x = self . conv1 ( x ) # \u7b2c\u4e00\u5c42\u5377\u79ef x = self . bn1 ( x ) # BN x = F . relu ( x , inplace = True ) # ReLU x = self . conv2 ( x ) # \u7b2c\u4e8c\u5c42\u5377\u79ef x = self . bn2 ( x ) # BN if self . use_residual : # \u82e5\u542f\u7528\u6b8b\u5dee if self . res_conv is not None : # \u82e5\u9700\u901a\u9053\u5bf9\u9f50 identity = self . res_conv ( identity ) # \u6b8b\u5dee\u901a\u9053\u5bf9\u9f50 x = x + identity # \u6b8b\u5dee\u76f8\u52a0 x = F . relu ( x , inplace = True ) # \u6700\u540e\u4e00\u6b21\u6fc0\u6d3b return x # \u8fd4\u56de Fused BEV Features __init__ ( in_channels , mid_channels , out_channels , use_residual = False ) \u00b6 Parameters: in_channels ( int ) \u2013 \u8f93\u5165 BEV \u7279\u5f81\u901a\u9053\u6570 C_in\u3002 # \u8f93\u5165\u901a\u9053 mid_channels ( int ) \u2013 \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels ( int ) \u2013 \u8f93\u51fa BEV \u7279\u5f81\u901a\u9053\u6570 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_residual ( bool , default: False ) \u2013 \u662f\u5426\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u3002 # \u662f\u5426\u542f\u7528\u6b8b\u5dee Source code in BEV_space_integration\\network\\BEVFusionEncoder.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , in_channels : int , mid_channels : int , out_channels : int , use_residual : bool = False , ): \"\"\" Args: in_channels (int): \u8f93\u5165 BEV \u7279\u5f81\u901a\u9053\u6570 C_in\u3002 # \u8f93\u5165\u901a\u9053 mid_channels (int): \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels (int): \u8f93\u51fa BEV \u7279\u5f81\u901a\u9053\u6570 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_residual (bool): \u662f\u5426\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u3002 # \u662f\u5426\u542f\u7528\u6b8b\u5dee \"\"\" super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . use_residual = use_residual # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528\u6b8b\u5dee self . conv1 = nn . Conv2d ( in_channels , mid_channels , kernel_size = 3 , padding = 1 , bias = False ) # \u7b2c\u4e00\u5c42 3\u00d73 Conv self . bn1 = nn . BatchNorm2d ( mid_channels ) # BN self . conv2 = nn . Conv2d ( mid_channels , out_channels , kernel_size = 3 , padding = 1 , bias = False ) # \u7b2c\u4e8c\u5c42 3\u00d73 Conv self . bn2 = nn . BatchNorm2d ( out_channels ) # BN if use_residual and in_channels != out_channels : # \u82e5\u901a\u9053\u4e0d\u4e00\u81f4\u4e14\u542f\u7528\u6b8b\u5dee self . res_conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 , bias = False ) # \u901a\u9053\u5bf9\u9f50\u5377\u79ef else : self . res_conv = None # \u4e0d\u4f7f\u7528\u6b8b\u5dee\u5bf9\u9f50 forward ( x ) \u00b6 Parameters: x ( Tensor ) \u2013 \u8f93\u5165 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: Tensor \u2013 torch.Tensor: \u878d\u5408\u7f16\u7801\u540e\u7684 BEV \u7279\u5f81\u56fe (B, C_out, H, W)\u3002 # \u8f93\u51fa\u7279\u5f81 Source code in BEV_space_integration\\network\\BEVFusionEncoder.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x (torch.Tensor): \u8f93\u5165 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: torch.Tensor: \u878d\u5408\u7f16\u7801\u540e\u7684 BEV \u7279\u5f81\u56fe (B, C_out, H, W)\u3002 # \u8f93\u51fa\u7279\u5f81 \"\"\" assert x . dim () == 4 , \"x \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 identity = x # \u4fdd\u5b58\u6b8b\u5dee\u5206\u652f x = self . conv1 ( x ) # \u7b2c\u4e00\u5c42\u5377\u79ef x = self . bn1 ( x ) # BN x = F . relu ( x , inplace = True ) # ReLU x = self . conv2 ( x ) # \u7b2c\u4e8c\u5c42\u5377\u79ef x = self . bn2 ( x ) # BN if self . use_residual : # \u82e5\u542f\u7528\u6b8b\u5dee if self . res_conv is not None : # \u82e5\u9700\u901a\u9053\u5bf9\u9f50 identity = self . res_conv ( identity ) # \u6b8b\u5dee\u901a\u9053\u5bf9\u9f50 x = x + identity # \u6b8b\u5dee\u76f8\u52a0 x = F . relu ( x , inplace = True ) # \u6700\u540e\u4e00\u6b21\u6fc0\u6d3b return x # \u8fd4\u56de Fused BEV Features","title":"BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757"},{"location":"bev_space_integration/#bev-space-integrationbev","text":"BEV Space Integration \u6a21\u5757\u7684\u4f5c\u7528\u662f\uff1a \u5c06\u6765\u81ea\u56fe\u50cf\u4fa7\u4e0e\u70b9\u4e91\u4fa7\u7684 BEV \u7279\u5f81\u8fdb\u884c\u7edf\u4e00\u7a7a\u95f4\u8868\u793a\u4e0e\u878d\u5408\uff0c \u5e76\u901a\u8fc7 BEV \u7f16\u7801\u5668\u63d0\u53d6\u9ad8\u5c42\u8bed\u4e49\u7279\u5f81\uff0c\u4e3a\u6700\u7ec8\u4efb\u52a1\u68c0\u6d4b\u5934\u63d0\u4f9b\u8f93\u5165\u3002 \u672c\u6a21\u5757\u5305\u542b\u56db\u7c7b\u6838\u5fc3\u529f\u80fd\uff1a \u56fe\u50cf BEV \u7279\u5f81\u9884\u5904\u7406 \u63a5\u6536 Camera Side \u7684 BEV \u7279\u5f81\u5e76\u8fdb\u884c\u7ef4\u5ea6\u53d8\u6362\u6216\u5bf9\u9f50\u3002 \u70b9\u4e91 BEV \u7279\u5f81\u9884\u5904\u7406 \u63a5\u6536 Lidar Side \u7684 BEV \u7279\u5f81\u5e76\u8fdb\u884c\u901a\u9053\u3001\u5c3a\u5ea6\u5bf9\u9f50\u3002 \u591a\u6a21\u6001 BEV \u878d\u5408\u7b97\u5b50 \u5c06\u56fe\u50cf BEV \u4e0e\u70b9\u4e91 BEV \u5728\u540c\u4e00\u7a7a\u95f4\u4e2d\u878d\u5408\uff08\u5982 concat / add / weighted fusion\uff09\u3002 BEV \u7f16\u7801\u5668\uff08BEV Encoder\uff09 \u57fa\u4e8e 2D CNN \u63d0\u53d6\u6700\u7ec8\u9ad8\u5c42 BEV \u8bed\u4e49\u7279\u5f81\u3002","title":"BEV Space Integration\uff08BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757\uff09"},{"location":"bev_space_integration/#api","text":"\u4ee5\u4e0b\u5217\u51fa BEV \u7a7a\u95f4\u878d\u5408\u6a21\u5757\u4e2d\u7684\u7b97\u5b50\u4e0e\u7f51\u7edc\u6a21\u5757\uff0c\u5e76\u7ed9\u51fa\u5bf9\u5e94\u6587\u4ef6\u8def\u5f84\u3002","title":"API \u6587\u6863"},{"location":"bev_space_integration/#bev_concat_fusion","text":"\u5c06 Camera BEV \u4e0e LiDAR BEV \u8fdb\u884c\u901a\u9053\u7ef4\u62fc\u63a5\u878d\u5408\u3002","title":"'bev_concat_fusion'"},{"location":"bev_space_integration/#BEV_space_integration.operator.bev_concat_fusion.bev_concat_fusion","text":"\u5c06\u76f8\u673a\u5206\u652f\u4e0e LiDAR \u5206\u652f\u5728 BEV \u5e73\u9762\u4e0a\u7684\u7279\u5f81\u56fe\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u62fc\u63a5\uff0c \u5f97\u5230\u878d\u5408\u540e\u7684 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6\u4e3a (B, C_cam + C_lidar, H, W)\u3002 Parameters: bev_cam ( Tensor ) \u2013 (B, C_cam, H, W) \u76f8\u673a\u5206\u652f\u7684 BEV \u7279\u5f81\u56fe\uff1a - B: batch size - C_cam: \u76f8\u673a BEV \u7279\u5f81\u901a\u9053\u6570 - H, W: BEV \u7f51\u683c\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002 bev_lidar ( Tensor ) \u2013 (B, C_lidar, H, W) LiDAR \u5206\u652f\u7684 BEV \u7279\u5f81\u56fe\uff1a - C_lidar: LiDAR BEV \u7279\u5f81\u901a\u9053\u6570 - H, W: \u9700\u4e0e bev_cam \u5728\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u5bf9\u9f50\u3002 dim ( int , default: 1 ) \u2013 int \u62fc\u63a5\u7ef4\u5ea6\uff0c\u9ed8\u8ba4\u4e3a 1\uff08\u901a\u9053\u7ef4\u5ea6\uff09\uff0c\u901a\u5e38\u4e0d\u9700\u8981\u4fee\u6539\u3002 Returns: bev_fused ( Tensor ) \u2013 (B, C_cam + C_lidar, H, W) \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u62fc\u63a5\u540e\u7684\u878d\u5408 BEV \u7279\u5f81\u56fe\uff0c\u53ef\u4f5c\u4e3a\u540e\u7eed BEV \u878d\u5408\u7f16\u7801\u7b97\u5b50\uff08\u5982 ConvFuser\uff09\u7684\u8f93\u5165\u3002 Source code in BEV_space_integration\\operator\\bev_concat_fusion.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 @torch . library . custom_op ( \"bevfusion_ops::bev_concat_fusion\" , mutates_args = [] ) def bev_concat_fusion ( bev_cam : torch . Tensor , bev_lidar : torch . Tensor , dim : int = 1 , ) -> torch . Tensor : \"\"\" \u5c06\u76f8\u673a\u5206\u652f\u4e0e LiDAR \u5206\u652f\u5728 BEV \u5e73\u9762\u4e0a\u7684\u7279\u5f81\u56fe\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u62fc\u63a5\uff0c \u5f97\u5230\u878d\u5408\u540e\u7684 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6\u4e3a (B, C_cam + C_lidar, H, W)\u3002 Args: bev_cam: (B, C_cam, H, W) \u76f8\u673a\u5206\u652f\u7684 BEV \u7279\u5f81\u56fe\uff1a - B: batch size - C_cam: \u76f8\u673a BEV \u7279\u5f81\u901a\u9053\u6570 - H, W: BEV \u7f51\u683c\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002 bev_lidar: (B, C_lidar, H, W) LiDAR \u5206\u652f\u7684 BEV \u7279\u5f81\u56fe\uff1a - C_lidar: LiDAR BEV \u7279\u5f81\u901a\u9053\u6570 - H, W: \u9700\u4e0e bev_cam \u5728\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u5bf9\u9f50\u3002 dim: int \u62fc\u63a5\u7ef4\u5ea6\uff0c\u9ed8\u8ba4\u4e3a 1\uff08\u901a\u9053\u7ef4\u5ea6\uff09\uff0c\u901a\u5e38\u4e0d\u9700\u8981\u4fee\u6539\u3002 Returns: bev_fused: (B, C_cam + C_lidar, H, W) \u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u62fc\u63a5\u540e\u7684\u878d\u5408 BEV \u7279\u5f81\u56fe\uff0c\u53ef\u4f5c\u4e3a\u540e\u7eed BEV \u878d\u5408\u7f16\u7801\u7b97\u5b50\uff08\u5982 ConvFuser\uff09\u7684\u8f93\u5165\u3002 \"\"\" assert bev_cam . dim () == 4 , \"bev_cam \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u68c0\u67e5\u76f8\u673a BEV \u5f62\u72b6 assert bev_lidar . dim () == 4 , \"bev_lidar \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u68c0\u67e5 LiDAR BEV \u5f62\u72b6 assert bev_cam . shape [ 0 ] == bev_lidar . shape [ 0 ], \"batch \u7ef4\u5ea6\u4e0d\u5339\u914d\" # \u68c0\u67e5 batch \u7ef4 assert bev_cam . shape [ 2 :] == bev_lidar . shape [ 2 :], \"\u7a7a\u95f4\u5c3a\u5bf8 (H,W) \u4e0d\u5339\u914d\" # \u68c0\u67e5\u7a7a\u95f4\u5c3a\u5bf8 bev_fused = torch . cat ([ bev_cam , bev_lidar ], dim = dim ) # \u5728\u901a\u9053\u7ef4\u5ea6\u62fc\u63a5\u7279\u5f81 return bev_fused # \u8fd4\u56de\u878d\u5408 BEV \u7279\u5f81","title":"bev_concat_fusion"},{"location":"bev_space_integration/#bevfusionencoder","text":"2D BEV \u7f16\u7801\u5668\uff0c\u5bf9\u878d\u5408\u540e\u7684 BEV \u7279\u5f81\u8fdb\u884c\u5377\u79ef\u5904\u7406\u3002","title":"'BEVFusionEncoder'"},{"location":"bev_space_integration/#BEV_space_integration.network.BEVFusionEncoder.BEVFusionEncoder","text":"Bases: Module BEVFusionEncoder \u6a21\u5757\uff08BEV \u878d\u5408\u7f16\u7801\u7b97\u5b50 / ConvFuser\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u5728 BEV \u5e73\u9762\u4e0a\u62fc\u63a5\u540e\u7684\u591a\u6a21\u6001\u7279\u5f81\uff08Camera BEV + LiDAR BEV\uff09\uff0c \u901a\u8fc7\u82e5\u5e72\u5c42 2D \u5377\u79ef\u5bf9\u901a\u9053\u7ef4\u4e0e\u7a7a\u95f4\u7ef4\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff0c\u8f93\u51fa\u6700\u7ec8\u7684 Fused BEV Features\uff0c\u4f5c\u4e3a\u540e\u7eed\u68c0\u6d4b\u3001\u5206\u5272\u7b49\u4efb\u52a1\u5934\u7684\u7edf\u4e00\u8f93\u5165\u3002","title":"BEVFusionEncoder"},{"location":"bev_space_integration/#BEV_space_integration.network.BEVFusionEncoder.BEVFusionEncoder--_1","text":"Conv2d (C_in \u2192 C_mid, 3\u00d73) + BN + ReLU \u878d\u5408\u4e0d\u540c\u6a21\u6001\u901a\u9053\u4fe1\u606f Conv2d (C_mid \u2192 C_out, 3\u00d73) + BN + ReLU \u8fdb\u4e00\u6b65\u63d0\u53d6 BEV \u7a7a\u95f4\u7ed3\u6784\u7279\u5f81 \uff08\u53ef\u9009\uff09\u6b8b\u5dee\u8fde\u63a5\u589e\u5f3a\u7a33\u5b9a\u6027","title":"\u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09"},{"location":"bev_space_integration/#BEV_space_integration.network.BEVFusionEncoder.BEVFusionEncoder--_2","text":"Args: x: (B, C_in, H, W) \u62fc\u63a5\u540e\u7684 BEV \u7279\u5f81\u56fe\uff1a - B: batch size - C_in: Camera BEV \u901a\u9053\u6570 + LiDAR BEV \u901a\u9053\u6570 - H, W: BEV \u7f51\u683c\u7a7a\u95f4\u5c3a\u5bf8","title":"\u8f93\u5165"},{"location":"bev_space_integration/#BEV_space_integration.network.BEVFusionEncoder.BEVFusionEncoder--_3","text":"Returns: fused_bev: (B, C_out, H, W) \u878d\u5408\u7f16\u7801\u540e\u7684\u6700\u7ec8 BEV \u7279\u5f81\u56fe\u3002","title":"\u8f93\u51fa"},{"location":"bev_space_integration/#BEV_space_integration.network.BEVFusionEncoder.BEVFusionEncoder--_4","text":"\u4f5c\u4e3a Camera BEV \u4e0e LiDAR BEV \u4e4b\u540e\u7684\u6700\u7ec8\u878d\u5408\u7f16\u7801\u6a21\u5757 \u8f93\u51fa\u7684 Fused BEV Features \u76f4\u63a5\u7528\u4e8e\u4e0b\u6e38 3D \u68c0\u6d4b / \u5730\u56fe\u5206\u5272\u7b49\u4efb\u52a1 Source code in BEV_space_integration\\network\\BEVFusionEncoder.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 class BEVFusionEncoder ( nn . Module ): \"\"\" BEVFusionEncoder \u6a21\u5757\uff08BEV \u878d\u5408\u7f16\u7801\u7b97\u5b50 / ConvFuser\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u5728 BEV \u5e73\u9762\u4e0a\u62fc\u63a5\u540e\u7684\u591a\u6a21\u6001\u7279\u5f81\uff08Camera BEV + LiDAR BEV\uff09\uff0c \u901a\u8fc7\u82e5\u5e72\u5c42 2D \u5377\u79ef\u5bf9\u901a\u9053\u7ef4\u4e0e\u7a7a\u95f4\u7ef4\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff0c\u8f93\u51fa\u6700\u7ec8\u7684 Fused BEV Features\uff0c\u4f5c\u4e3a\u540e\u7eed\u68c0\u6d4b\u3001\u5206\u5272\u7b49\u4efb\u52a1\u5934\u7684\u7edf\u4e00\u8f93\u5165\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. **Conv2d (C_in \u2192 C_mid, 3\u00d73) + BN + ReLU** - \u878d\u5408\u4e0d\u540c\u6a21\u6001\u901a\u9053\u4fe1\u606f 2. **Conv2d (C_mid \u2192 C_out, 3\u00d73) + BN + ReLU** - \u8fdb\u4e00\u6b65\u63d0\u53d6 BEV \u7a7a\u95f4\u7ed3\u6784\u7279\u5f81 3. \uff08\u53ef\u9009\uff09\u6b8b\u5dee\u8fde\u63a5\u589e\u5f3a\u7a33\u5b9a\u6027 --- ### \u8f93\u5165 Args: x: (B, C_in, H, W) \u62fc\u63a5\u540e\u7684 BEV \u7279\u5f81\u56fe\uff1a - B: batch size - C_in: Camera BEV \u901a\u9053\u6570 + LiDAR BEV \u901a\u9053\u6570 - H, W: BEV \u7f51\u683c\u7a7a\u95f4\u5c3a\u5bf8 --- ### \u8f93\u51fa Returns: fused_bev: (B, C_out, H, W) \u878d\u5408\u7f16\u7801\u540e\u7684\u6700\u7ec8 BEV \u7279\u5f81\u56fe\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u4f5c\u4e3a Camera BEV \u4e0e LiDAR BEV \u4e4b\u540e\u7684\u6700\u7ec8\u878d\u5408\u7f16\u7801\u6a21\u5757 - \u8f93\u51fa\u7684 Fused BEV Features \u76f4\u63a5\u7528\u4e8e\u4e0b\u6e38 3D \u68c0\u6d4b / \u5730\u56fe\u5206\u5272\u7b49\u4efb\u52a1 \"\"\" def __init__ ( self , in_channels : int , mid_channels : int , out_channels : int , use_residual : bool = False , ): \"\"\" Args: in_channels (int): \u8f93\u5165 BEV \u7279\u5f81\u901a\u9053\u6570 C_in\u3002 # \u8f93\u5165\u901a\u9053 mid_channels (int): \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels (int): \u8f93\u51fa BEV \u7279\u5f81\u901a\u9053\u6570 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_residual (bool): \u662f\u5426\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u3002 # \u662f\u5426\u542f\u7528\u6b8b\u5dee \"\"\" super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . use_residual = use_residual # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528\u6b8b\u5dee self . conv1 = nn . Conv2d ( in_channels , mid_channels , kernel_size = 3 , padding = 1 , bias = False ) # \u7b2c\u4e00\u5c42 3\u00d73 Conv self . bn1 = nn . BatchNorm2d ( mid_channels ) # BN self . conv2 = nn . Conv2d ( mid_channels , out_channels , kernel_size = 3 , padding = 1 , bias = False ) # \u7b2c\u4e8c\u5c42 3\u00d73 Conv self . bn2 = nn . BatchNorm2d ( out_channels ) # BN if use_residual and in_channels != out_channels : # \u82e5\u901a\u9053\u4e0d\u4e00\u81f4\u4e14\u542f\u7528\u6b8b\u5dee self . res_conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 , bias = False ) # \u901a\u9053\u5bf9\u9f50\u5377\u79ef else : self . res_conv = None # \u4e0d\u4f7f\u7528\u6b8b\u5dee\u5bf9\u9f50 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x (torch.Tensor): \u8f93\u5165 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: torch.Tensor: \u878d\u5408\u7f16\u7801\u540e\u7684 BEV \u7279\u5f81\u56fe (B, C_out, H, W)\u3002 # \u8f93\u51fa\u7279\u5f81 \"\"\" assert x . dim () == 4 , \"x \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 identity = x # \u4fdd\u5b58\u6b8b\u5dee\u5206\u652f x = self . conv1 ( x ) # \u7b2c\u4e00\u5c42\u5377\u79ef x = self . bn1 ( x ) # BN x = F . relu ( x , inplace = True ) # ReLU x = self . conv2 ( x ) # \u7b2c\u4e8c\u5c42\u5377\u79ef x = self . bn2 ( x ) # BN if self . use_residual : # \u82e5\u542f\u7528\u6b8b\u5dee if self . res_conv is not None : # \u82e5\u9700\u901a\u9053\u5bf9\u9f50 identity = self . res_conv ( identity ) # \u6b8b\u5dee\u901a\u9053\u5bf9\u9f50 x = x + identity # \u6b8b\u5dee\u76f8\u52a0 x = F . relu ( x , inplace = True ) # \u6700\u540e\u4e00\u6b21\u6fc0\u6d3b return x # \u8fd4\u56de Fused BEV Features","title":"\u6a21\u5757\u7528\u9014"},{"location":"bev_space_integration/#BEV_space_integration.network.BEVFusionEncoder.BEVFusionEncoder.__init__","text":"Parameters: in_channels ( int ) \u2013 \u8f93\u5165 BEV \u7279\u5f81\u901a\u9053\u6570 C_in\u3002 # \u8f93\u5165\u901a\u9053 mid_channels ( int ) \u2013 \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels ( int ) \u2013 \u8f93\u51fa BEV \u7279\u5f81\u901a\u9053\u6570 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_residual ( bool , default: False ) \u2013 \u662f\u5426\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u3002 # \u662f\u5426\u542f\u7528\u6b8b\u5dee Source code in BEV_space_integration\\network\\BEVFusionEncoder.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def __init__ ( self , in_channels : int , mid_channels : int , out_channels : int , use_residual : bool = False , ): \"\"\" Args: in_channels (int): \u8f93\u5165 BEV \u7279\u5f81\u901a\u9053\u6570 C_in\u3002 # \u8f93\u5165\u901a\u9053 mid_channels (int): \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels (int): \u8f93\u51fa BEV \u7279\u5f81\u901a\u9053\u6570 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_residual (bool): \u662f\u5426\u4f7f\u7528\u6b8b\u5dee\u8fde\u63a5\u3002 # \u662f\u5426\u542f\u7528\u6b8b\u5dee \"\"\" super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . use_residual = use_residual # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528\u6b8b\u5dee self . conv1 = nn . Conv2d ( in_channels , mid_channels , kernel_size = 3 , padding = 1 , bias = False ) # \u7b2c\u4e00\u5c42 3\u00d73 Conv self . bn1 = nn . BatchNorm2d ( mid_channels ) # BN self . conv2 = nn . Conv2d ( mid_channels , out_channels , kernel_size = 3 , padding = 1 , bias = False ) # \u7b2c\u4e8c\u5c42 3\u00d73 Conv self . bn2 = nn . BatchNorm2d ( out_channels ) # BN if use_residual and in_channels != out_channels : # \u82e5\u901a\u9053\u4e0d\u4e00\u81f4\u4e14\u542f\u7528\u6b8b\u5dee self . res_conv = nn . Conv2d ( in_channels , out_channels , kernel_size = 1 , bias = False ) # \u901a\u9053\u5bf9\u9f50\u5377\u79ef else : self . res_conv = None # \u4e0d\u4f7f\u7528\u6b8b\u5dee\u5bf9\u9f50","title":"__init__"},{"location":"bev_space_integration/#BEV_space_integration.network.BEVFusionEncoder.BEVFusionEncoder.forward","text":"Parameters: x ( Tensor ) \u2013 \u8f93\u5165 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: Tensor \u2013 torch.Tensor: \u878d\u5408\u7f16\u7801\u540e\u7684 BEV \u7279\u5f81\u56fe (B, C_out, H, W)\u3002 # \u8f93\u51fa\u7279\u5f81 Source code in BEV_space_integration\\network\\BEVFusionEncoder.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x (torch.Tensor): \u8f93\u5165 BEV \u7279\u5f81\u56fe\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: torch.Tensor: \u878d\u5408\u7f16\u7801\u540e\u7684 BEV \u7279\u5f81\u56fe (B, C_out, H, W)\u3002 # \u8f93\u51fa\u7279\u5f81 \"\"\" assert x . dim () == 4 , \"x \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 identity = x # \u4fdd\u5b58\u6b8b\u5dee\u5206\u652f x = self . conv1 ( x ) # \u7b2c\u4e00\u5c42\u5377\u79ef x = self . bn1 ( x ) # BN x = F . relu ( x , inplace = True ) # ReLU x = self . conv2 ( x ) # \u7b2c\u4e8c\u5c42\u5377\u79ef x = self . bn2 ( x ) # BN if self . use_residual : # \u82e5\u542f\u7528\u6b8b\u5dee if self . res_conv is not None : # \u82e5\u9700\u901a\u9053\u5bf9\u9f50 identity = self . res_conv ( identity ) # \u6b8b\u5dee\u901a\u9053\u5bf9\u9f50 x = x + identity # \u6b8b\u5dee\u76f8\u52a0 x = F . relu ( x , inplace = True ) # \u6700\u540e\u4e00\u6b21\u6fc0\u6d3b return x # \u8fd4\u56de Fused BEV Features","title":"forward"},{"location":"camera_side/","text":"Camera Side\uff08\u56fe\u50cf\u4fa7\u6a21\u5757\uff09 \u00b6 Camera Side \u6a21\u5757\u7684\u4f5c\u7528\u662f\uff1a \u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u3001\u6df1\u5ea6\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u5c06 2D \u7279\u5f81\u8f6c\u6362\u5230 3D / BEV \u7a7a\u95f4\uff0c\u4e3a\u540e\u7eed BEV \u878d\u5408\u63d0\u4f9b\u56fe\u50cf\u4fa7 BEV \u7279\u5f81\u3002 \u672c\u6a21\u5757\u5305\u542b\u56db\u7c7b\u6838\u5fc3\u529f\u80fd\uff1a \u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff08FPN / Backbone\uff09 \u63d0\u53d6\u591a\u5c3a\u5ea6\u56fe\u50cf\u8bed\u4e49\u7279\u5f81\u3002 \u6df1\u5ea6\u9884\u6d4b\u6a21\u5757 \u8f93\u51fa\u50cf\u7d20\u7ea7\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u7528\u4e8e Lift \u7279\u5f81\u5230 3D\u3002 \u51e0\u4f55\u6295\u5f71\u7b97\u5b50\uff08Lift / Transform / Pooling\uff09 \u5c06\u56fe\u50cf\u7279\u5f81\u6620\u5c04\u81f3 3D \u7a7a\u95f4\uff0c\u518d\u6295\u5f71\u5230 BEV \u7f51\u683c\u3002 \u7279\u5f81\u589e\u5f3a\u7b97\u5b50 \u5bf9\u56fe\u50cf BEV \u7279\u5f81\u8fdb\u884c\u52a0\u6743\u6216\u6269\u5c55\u3002 API \u6587\u6863 \u00b6 \u4ee5\u4e0b\u5217\u51fa Camera Side \u4e2d\u5168\u90e8\u7b97\u5b50\u4e0e\u7f51\u7edc\u6a21\u5757\uff0c\u5e76\u7ed9\u51fa\u5176\u5bf9\u5e94\u6587\u4ef6\u8def\u5f84\u3002 CameraFPNFusion \u00b6 \u591a\u5c3a\u5ea6 FPN \u7279\u5f81\u878d\u5408\u6a21\u5757\u3002 CameraFPNFusion \u00b6 Bases: Module CameraFPNFusion \u6a21\u5757\uff08\u56fe\u50cf\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u901a\u8fc7\u5bf9\u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5206\u522b\u505a 1\u00d71 Conv \u901a\u9053\u6620\u5c04\u3001\u4e0a\u91c7\u6837\u5230\u7edf\u4e00\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c \u7136\u540e\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u9010\u5143\u7d20\u76f8\u52a0\uff0c\u5f97\u5230\u4e00\u5f20\u878d\u5408\u540e\u7684 2D \u7279\u5f81\u56fe\u3002\u8be5\u7279\u5f81\u56fe\u4f5c\u4e3a\u540e\u7eed \u6df1\u5ea6\u4f30\u8ba1\u4e0e Camera-to-BEV \u89c6\u89d2\u53d8\u6362\u7684\u7edf\u4e00\u8f93\u5165\u63a5\u53e3\u3002 \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 \u00b6 \u5bf9\u6bcf\u4e2a\u5c3a\u5ea6\u7279\u5f81\u4f7f\u7528 Conv2d C_in_i \u2192 C_out, kernel_size=1, stride=1, padding=0 \u505a\u901a\u9053\u7ebf\u6027\u53d8\u6362\uff0c\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u6570 C_out (\u53ef\u9009) BatchNorm2d \u5bf9\u6bcf\u4e2a\u5c3a\u5ea6\u6620\u5c04\u540e\u7684\u7279\u5f81\u505a\u5f52\u4e00\u5316 (\u53ef\u9009) ReLU \u6fc0\u6d3b \u5728\u878d\u5408\u524d\u589e\u5f3a\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b \u7279\u5f81\u4e0a\u91c7\u6837\u5230\u6700\u9ad8\u5206\u8fa8\u7387\u5e76\u9010\u5143\u7d20\u76f8\u52a0 \u5f97\u5230\u5355\u5c3a\u5ea6\u3001\u591a\u901a\u9053\u7684\u878d\u5408\u7279\u5f81\u56fe \u8f93\u5165 \u00b6 Args: feats (List[torch.Tensor]): \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 \u7b2c i \u4e2a\u7279\u5f81\u5f62\u72b6\u4e3a (B, C_in_i, H_i, W_i)\uff1a - B: batch size - C_in_i: \u7b2c i \u4e2a\u5c3a\u5ea6\u7684\u901a\u9053\u6570 - H_i, W_i: \u7b2c i \u4e2a\u5c3a\u5ea6\u7684\u7a7a\u95f4\u5c3a\u5bf8 \u8f93\u51fa \u00b6 Returns: torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe\uff0c\u5f62\u72b6\u4e3a (B, C_out, H_out, W_out)\uff0c \u5176\u4e2d H_out, W_out \u7b49\u4e8e\u5217\u8868\u4e2d\u6700\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002 \u6a21\u5757\u7528\u9014 \u00b6 \u5c06\u6765\u81ea\u76f8\u673a backbone \u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5bf9\u9f50\u5230\u7edf\u4e00\u7a7a\u95f4\u5c3a\u5ea6\u4e0e\u901a\u9053\u6570 \u4f5c\u4e3a\u540e\u7eed depth head / LSS \u89c6\u89d2\u53d8\u6362\u7b97\u5b50\u7684\u7edf\u4e00 2D \u7279\u5f81\u8f93\u5165 \u4e5f\u53ef\u4f5c\u4e3a\u901a\u7528\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7b97\u5b50\u590d\u7528\u4e8e\u5176\u5b83\u76f8\u673a\u5206\u652f\u7f51\u7edc Source code in camera_side\\network\\CameraFPNFusion.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class CameraFPNFusion ( nn . Module ): \"\"\" CameraFPNFusion \u6a21\u5757\uff08\u56fe\u50cf\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u901a\u8fc7\u5bf9\u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5206\u522b\u505a 1\u00d71 Conv \u901a\u9053\u6620\u5c04\u3001\u4e0a\u91c7\u6837\u5230\u7edf\u4e00\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c \u7136\u540e\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u9010\u5143\u7d20\u76f8\u52a0\uff0c\u5f97\u5230\u4e00\u5f20\u878d\u5408\u540e\u7684 2D \u7279\u5f81\u56fe\u3002\u8be5\u7279\u5f81\u56fe\u4f5c\u4e3a\u540e\u7eed \u6df1\u5ea6\u4f30\u8ba1\u4e0e Camera-to-BEV \u89c6\u89d2\u53d8\u6362\u7684\u7edf\u4e00\u8f93\u5165\u63a5\u53e3\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. \u5bf9\u6bcf\u4e2a\u5c3a\u5ea6\u7279\u5f81\u4f7f\u7528 **Conv2d C_in_i \u2192 C_out, kernel_size=1, stride=1, padding=0** - \u505a\u901a\u9053\u7ebf\u6027\u53d8\u6362\uff0c\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u6570 C_out 2. **(\u53ef\u9009) BatchNorm2d** - \u5bf9\u6bcf\u4e2a\u5c3a\u5ea6\u6620\u5c04\u540e\u7684\u7279\u5f81\u505a\u5f52\u4e00\u5316 3. **(\u53ef\u9009) ReLU \u6fc0\u6d3b** - \u5728\u878d\u5408\u524d\u589e\u5f3a\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b 4. \u7279\u5f81\u4e0a\u91c7\u6837\u5230\u6700\u9ad8\u5206\u8fa8\u7387\u5e76\u9010\u5143\u7d20\u76f8\u52a0 - \u5f97\u5230\u5355\u5c3a\u5ea6\u3001\u591a\u901a\u9053\u7684\u878d\u5408\u7279\u5f81\u56fe --- ### \u8f93\u5165 Args: feats (List[torch.Tensor]): \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 \u7b2c i \u4e2a\u7279\u5f81\u5f62\u72b6\u4e3a (B, C_in_i, H_i, W_i)\uff1a - B: batch size - C_in_i: \u7b2c i \u4e2a\u5c3a\u5ea6\u7684\u901a\u9053\u6570 - H_i, W_i: \u7b2c i \u4e2a\u5c3a\u5ea6\u7684\u7a7a\u95f4\u5c3a\u5bf8 --- ### \u8f93\u51fa Returns: torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe\uff0c\u5f62\u72b6\u4e3a (B, C_out, H_out, W_out)\uff0c \u5176\u4e2d H_out, W_out \u7b49\u4e8e\u5217\u8868\u4e2d\u6700\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u5c06\u6765\u81ea\u76f8\u673a backbone \u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5bf9\u9f50\u5230\u7edf\u4e00\u7a7a\u95f4\u5c3a\u5ea6\u4e0e\u901a\u9053\u6570 - \u4f5c\u4e3a\u540e\u7eed depth head / LSS \u89c6\u89d2\u53d8\u6362\u7b97\u5b50\u7684\u7edf\u4e00 2D \u7279\u5f81\u8f93\u5165 - \u4e5f\u53ef\u4f5c\u4e3a\u901a\u7528\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7b97\u5b50\u590d\u7528\u4e8e\u5176\u5b83\u76f8\u673a\u5206\u652f\u7f51\u7edc \"\"\" def __init__ ( self , in_channels_list , out_channels : int , use_bn : bool = True , use_relu : bool = True , ): \"\"\" Args: in_channels_list (List[int]): \u591a\u5c3a\u5ea6\u8f93\u5165\u7279\u5f81\u7684\u901a\u9053\u6570\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 # \u7b80\u77ed\u89e3\u91ca out_channels (int): \u878d\u5408\u540e\u7279\u5f81\u7684\u76ee\u6807\u901a\u9053\u6570\u3002 # \u76ee\u6807\u901a\u9053 use_bn (bool): \u662f\u5426\u5728 1\u00d71 Conv \u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN use_relu (bool): \u662f\u5426\u5728 BN \u540e\u4f7f\u7528 ReLU \u6fc0\u6d3b\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 ReLU \"\"\" super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . out_channels = out_channels # \u8bb0\u5f55\u8f93\u51fa\u901a\u9053\u6570 self . use_bn = use_bn # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528 BN self . use_relu = use_relu # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528 ReLU # \u4e3a\u6bcf\u4e2a\u5c3a\u5ea6\u6784\u5efa\u4e00\u5957 1\u00d71 Conv (+BN+ReLU) \u8fdb\u884c\u901a\u9053\u6620\u5c04 self . proj_layers = nn . ModuleList () # \u4fdd\u5b58\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u6620\u5c04\u5c42 for c_in in in_channels_list : # \u904d\u5386\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u8f93\u5165\u901a\u9053 layers = [ nn . Conv2d ( c_in , out_channels , 1 , bias = not use_bn )] # 1\u00d71 Conv if use_bn : layers . append ( nn . BatchNorm2d ( out_channels )) # \u53ef\u9009 BN if use_relu : layers . append ( nn . ReLU ( inplace = True )) # \u53ef\u9009 ReLU self . proj_layers . append ( nn . Sequential ( * layers )) # \u5c01\u88c5\u4e3a\u987a\u5e8f\u6a21\u5757 def forward ( self , feats ): \"\"\" \u524d\u5411\u8ba1\u7b97\uff1a\u5c06\u591a\u5c3a\u5ea6\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u5e76\u4e0a\u91c7\u6837\u540e\u76f8\u52a0\u3002 Args: feats (List[torch.Tensor]): \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u5f62\u72b6\u53c2\u89c1\u7c7b\u6ce8\u91ca\u3002 # \u8f93\u5165\u7279\u5f81\u5217\u8868 Returns: torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe (B, C_out, H_out, W_out)\u3002 # \u8fd4\u56de\u878d\u5408\u7279\u5f81 \"\"\" assert len ( feats ) == len ( self . proj_layers ), \"feats \u6570\u91cf\u4e0e proj_layers \u4e0d\u4e00\u81f4\" # \u7b80\u5355\u68c0\u67e5 # \u4ee5\u7b2c\u4e00\u4e2a\u7279\u5f81\u4e3a\u6700\u9ad8\u5206\u8fa8\u7387\u53c2\u8003 ref_feat = feats [ 0 ] # \u53c2\u8003\u5c3a\u5ea6\u7279\u5f81 _ , _ , H_ref , W_ref = ref_feat . shape # \u53c2\u8003\u7a7a\u95f4\u5c3a\u5bf8 fused = 0.0 # \u521d\u59cb\u5316\u878d\u5408\u7ed3\u679c for feat , proj in zip ( feats , self . proj_layers ): # \u904d\u5386\u6bcf\u4e2a\u5c3a\u5ea6 x = proj ( feat ) # \u901a\u9053\u6620\u5c04\u5230 C_out if x . shape [ - 2 :] != ( H_ref , W_ref ): # \u82e5\u7a7a\u95f4\u5c3a\u5bf8\u4e0d\u540c x = F . interpolate ( x , size = ( H_ref , W_ref ), mode = \"bilinear\" , align_corners = False ) # \u4e0a\u91c7\u6837\u5230\u53c2\u8003\u5c3a\u5ea6 fused = fused + x # \u9010\u5c3a\u5ea6\u7d2f\u52a0\u7279\u5f81 return fused # \u8fd4\u56de\u878d\u5408\u7ed3\u679c __init__ ( in_channels_list , out_channels , use_bn = True , use_relu = True ) \u00b6 Parameters: in_channels_list ( List [ int ] ) \u2013 \u591a\u5c3a\u5ea6\u8f93\u5165\u7279\u5f81\u7684\u901a\u9053\u6570\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 # \u7b80\u77ed\u89e3\u91ca out_channels ( int ) \u2013 \u878d\u5408\u540e\u7279\u5f81\u7684\u76ee\u6807\u901a\u9053\u6570\u3002 # \u76ee\u6807\u901a\u9053 use_bn ( bool , default: True ) \u2013 \u662f\u5426\u5728 1\u00d71 Conv \u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN use_relu ( bool , default: True ) \u2013 \u662f\u5426\u5728 BN \u540e\u4f7f\u7528 ReLU \u6fc0\u6d3b\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 ReLU Source code in camera_side\\network\\CameraFPNFusion.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_channels_list , out_channels : int , use_bn : bool = True , use_relu : bool = True , ): \"\"\" Args: in_channels_list (List[int]): \u591a\u5c3a\u5ea6\u8f93\u5165\u7279\u5f81\u7684\u901a\u9053\u6570\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 # \u7b80\u77ed\u89e3\u91ca out_channels (int): \u878d\u5408\u540e\u7279\u5f81\u7684\u76ee\u6807\u901a\u9053\u6570\u3002 # \u76ee\u6807\u901a\u9053 use_bn (bool): \u662f\u5426\u5728 1\u00d71 Conv \u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN use_relu (bool): \u662f\u5426\u5728 BN \u540e\u4f7f\u7528 ReLU \u6fc0\u6d3b\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 ReLU \"\"\" super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . out_channels = out_channels # \u8bb0\u5f55\u8f93\u51fa\u901a\u9053\u6570 self . use_bn = use_bn # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528 BN self . use_relu = use_relu # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528 ReLU # \u4e3a\u6bcf\u4e2a\u5c3a\u5ea6\u6784\u5efa\u4e00\u5957 1\u00d71 Conv (+BN+ReLU) \u8fdb\u884c\u901a\u9053\u6620\u5c04 self . proj_layers = nn . ModuleList () # \u4fdd\u5b58\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u6620\u5c04\u5c42 for c_in in in_channels_list : # \u904d\u5386\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u8f93\u5165\u901a\u9053 layers = [ nn . Conv2d ( c_in , out_channels , 1 , bias = not use_bn )] # 1\u00d71 Conv if use_bn : layers . append ( nn . BatchNorm2d ( out_channels )) # \u53ef\u9009 BN if use_relu : layers . append ( nn . ReLU ( inplace = True )) # \u53ef\u9009 ReLU self . proj_layers . append ( nn . Sequential ( * layers )) # \u5c01\u88c5\u4e3a\u987a\u5e8f\u6a21\u5757 forward ( feats ) \u00b6 \u524d\u5411\u8ba1\u7b97\uff1a\u5c06\u591a\u5c3a\u5ea6\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u5e76\u4e0a\u91c7\u6837\u540e\u76f8\u52a0\u3002 Parameters: feats ( List [ Tensor ] ) \u2013 \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u5f62\u72b6\u53c2\u89c1\u7c7b\u6ce8\u91ca\u3002 # \u8f93\u5165\u7279\u5f81\u5217\u8868 Returns: \u2013 torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe (B, C_out, H_out, W_out)\u3002 # \u8fd4\u56de\u878d\u5408\u7279\u5f81 Source code in camera_side\\network\\CameraFPNFusion.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def forward ( self , feats ): \"\"\" \u524d\u5411\u8ba1\u7b97\uff1a\u5c06\u591a\u5c3a\u5ea6\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u5e76\u4e0a\u91c7\u6837\u540e\u76f8\u52a0\u3002 Args: feats (List[torch.Tensor]): \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u5f62\u72b6\u53c2\u89c1\u7c7b\u6ce8\u91ca\u3002 # \u8f93\u5165\u7279\u5f81\u5217\u8868 Returns: torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe (B, C_out, H_out, W_out)\u3002 # \u8fd4\u56de\u878d\u5408\u7279\u5f81 \"\"\" assert len ( feats ) == len ( self . proj_layers ), \"feats \u6570\u91cf\u4e0e proj_layers \u4e0d\u4e00\u81f4\" # \u7b80\u5355\u68c0\u67e5 # \u4ee5\u7b2c\u4e00\u4e2a\u7279\u5f81\u4e3a\u6700\u9ad8\u5206\u8fa8\u7387\u53c2\u8003 ref_feat = feats [ 0 ] # \u53c2\u8003\u5c3a\u5ea6\u7279\u5f81 _ , _ , H_ref , W_ref = ref_feat . shape # \u53c2\u8003\u7a7a\u95f4\u5c3a\u5bf8 fused = 0.0 # \u521d\u59cb\u5316\u878d\u5408\u7ed3\u679c for feat , proj in zip ( feats , self . proj_layers ): # \u904d\u5386\u6bcf\u4e2a\u5c3a\u5ea6 x = proj ( feat ) # \u901a\u9053\u6620\u5c04\u5230 C_out if x . shape [ - 2 :] != ( H_ref , W_ref ): # \u82e5\u7a7a\u95f4\u5c3a\u5bf8\u4e0d\u540c x = F . interpolate ( x , size = ( H_ref , W_ref ), mode = \"bilinear\" , align_corners = False ) # \u4e0a\u91c7\u6837\u5230\u53c2\u8003\u5c3a\u5ea6 fused = fused + x # \u9010\u5c3a\u5ea6\u7d2f\u52a0\u7279\u5f81 return fused # \u8fd4\u56de\u878d\u5408\u7ed3\u679c DepthDistributionHead \u00b6 \u50cf\u7d20\u7ea7\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b\u6a21\u5757\u3002 DepthDistributionHead \u00b6 Bases: Module DepthDistributionHead \u6a21\u5757\uff08\u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u878d\u5408\u540e\u7684\u76f8\u673a 2D \u7279\u5f81\u56fe\uff0c\u901a\u8fc7\u82e5\u5e72 2D \u5377\u79ef\u5c42\u9884\u6d4b\u6bcf\u4e2a\u50cf\u7d20\u5728 \u79bb\u6563\u6df1\u5ea6 bins \u4e0a\u7684\u6982\u7387\u5206\u5e03\uff0c\u7528\u4e8e\u540e\u7eed\u7684 2D\u21923D \u5347\u7ef4\u4e0e Camera-to-BEV \u89c6\u89d2\u53d8\u6362\u3002\u8f93\u51fa\u4e3a (B, D, H, W)\uff0c\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u505a softmax \u5f52\u4e00\u5316\u3002 \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 \u00b6 Conv2d C_in \u2192 C_mid, kernel_size=3, padding=1 \u5bf9\u8f93\u5165\u7279\u5f81\u505a\u5c40\u90e8\u611f\u53d7\u91ce\u589e\u5f3a (\u53ef\u9009) BatchNorm2d + ReLU \u7a33\u5b9a\u8bad\u7ec3\u5e76\u5f15\u5165\u975e\u7ebf\u6027 Conv2d C_mid \u2192 D, kernel_size=1 \u6620\u5c04\u5230\u6df1\u5ea6\u901a\u9053\u7ef4\u5ea6 D Softmax (\u6cbf depth \u7ef4\u5ea6) \u5f97\u5230\u6bcf\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03 \u8f93\u5165 \u00b6 Args: x (torch.Tensor): \u76f8\u673a\u878d\u5408\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\uff1a - B: batch size - C_in: \u8f93\u5165\u901a\u9053\u6570 - H, W: \u7279\u5f81\u56fe\u7a7a\u95f4\u5c3a\u5bf8 \u8f93\u51fa \u00b6 Returns: torch.Tensor: \u50cf\u7d20\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u5f62\u72b6 (B, D, H, W)\uff0c\u5728\u7ef4\u5ea6 1 \u4e0a\u6c42\u548c\u4e3a 1\u3002 \u6a21\u5757\u7528\u9014 \u00b6 \u4e3a LSS / BEVFusion \u63d0\u4f9b\u6bcf\u4e2a\u50cf\u7d20\u5728\u79bb\u6563\u6df1\u5ea6\u4e0a\u7684\u6982\u7387\u4f30\u8ba1 \u540e\u7eed\u7b97\u5b50 camera_lift_2d_to_3d \u3001 depth_weighted_feature_expand \u5c06\u590d\u7528\u8be5\u6df1\u5ea6\u5206\u5e03\u5b8c\u6210 2D\u21923D \u5347\u7ef4\u4e0e\u7279\u5f81\u52a0\u6743 Source code in camera_side\\network\\DepthDistributionHead.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class DepthDistributionHead ( nn . Module ): \"\"\" DepthDistributionHead \u6a21\u5757\uff08\u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u878d\u5408\u540e\u7684\u76f8\u673a 2D \u7279\u5f81\u56fe\uff0c\u901a\u8fc7\u82e5\u5e72 2D \u5377\u79ef\u5c42\u9884\u6d4b\u6bcf\u4e2a\u50cf\u7d20\u5728 \u79bb\u6563\u6df1\u5ea6 bins \u4e0a\u7684\u6982\u7387\u5206\u5e03\uff0c\u7528\u4e8e\u540e\u7eed\u7684 2D\u21923D \u5347\u7ef4\u4e0e Camera-to-BEV \u89c6\u89d2\u53d8\u6362\u3002\u8f93\u51fa\u4e3a (B, D, H, W)\uff0c\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u505a softmax \u5f52\u4e00\u5316\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. **Conv2d C_in \u2192 C_mid, kernel_size=3, padding=1** - \u5bf9\u8f93\u5165\u7279\u5f81\u505a\u5c40\u90e8\u611f\u53d7\u91ce\u589e\u5f3a 2. **(\u53ef\u9009) BatchNorm2d + ReLU** - \u7a33\u5b9a\u8bad\u7ec3\u5e76\u5f15\u5165\u975e\u7ebf\u6027 3. **Conv2d C_mid \u2192 D, kernel_size=1** - \u6620\u5c04\u5230\u6df1\u5ea6\u901a\u9053\u7ef4\u5ea6 D 4. **Softmax (\u6cbf depth \u7ef4\u5ea6)** - \u5f97\u5230\u6bcf\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03 --- ### \u8f93\u5165 Args: x (torch.Tensor): \u76f8\u673a\u878d\u5408\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\uff1a - B: batch size - C_in: \u8f93\u5165\u901a\u9053\u6570 - H, W: \u7279\u5f81\u56fe\u7a7a\u95f4\u5c3a\u5bf8 --- ### \u8f93\u51fa Returns: torch.Tensor: \u50cf\u7d20\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u5f62\u72b6 (B, D, H, W)\uff0c\u5728\u7ef4\u5ea6 1 \u4e0a\u6c42\u548c\u4e3a 1\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u4e3a LSS / BEVFusion \u63d0\u4f9b\u6bcf\u4e2a\u50cf\u7d20\u5728\u79bb\u6563\u6df1\u5ea6\u4e0a\u7684\u6982\u7387\u4f30\u8ba1 - \u540e\u7eed\u7b97\u5b50 `camera_lift_2d_to_3d`\u3001`depth_weighted_feature_expand` \u5c06\u590d\u7528\u8be5\u6df1\u5ea6\u5206\u5e03\u5b8c\u6210 2D\u21923D \u5347\u7ef4\u4e0e\u7279\u5f81\u52a0\u6743 \"\"\" def __init__ ( self , in_channels : int , mid_channels : int , num_depth_bins : int , use_bn : bool = True , ): \"\"\" Args: in_channels (int): \u8f93\u5165\u7279\u5f81\u901a\u9053\u6570\u3002 # C_in mid_channels (int): \u4e2d\u95f4\u9690\u5c42\u901a\u9053\u6570\u3002 # C_mid num_depth_bins (int): \u6df1\u5ea6\u79bb\u6563 bin \u6570\u91cf D\u3002 # D use_bn (bool): \u662f\u5426\u5728\u7b2c\u4e00\u5c42\u5377\u79ef\u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN \"\"\" super () . __init__ () # \u7236\u7c7b\u6784\u9020 layers = [ # \u7b2c\u4e00\u5c42 conv \u53ca\u53ef\u9009 BN+ReLU nn . Conv2d ( in_channels , mid_channels , kernel_size = 3 , padding = 1 , bias = not use_bn ) # 3\u00d73 Conv ] if use_bn : # \u662f\u5426\u52a0\u5165 BN layers . append ( nn . BatchNorm2d ( mid_channels )) # BN \u5c42 layers . append ( nn . ReLU ( inplace = True )) # ReLU \u6fc0\u6d3b self . stem = nn . Sequential ( * layers ) # \u5c01\u88c5\u4e3a\u987a\u5e8f\u6a21\u5757 self . depth_logits = nn . Conv2d ( mid_channels , num_depth_bins , kernel_size = 1 ) # 1\u00d71 Conv \u9884\u6d4b D \u901a\u9053 self . num_depth_bins = num_depth_bins # \u8bb0\u5f55\u6df1\u5ea6 bins \u6570 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x (torch.Tensor): \u8f93\u5165\u76f8\u673a\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: torch.Tensor: \u6df1\u5ea6\u6982\u7387\u5206\u5e03 (B, D, H, W)\u3002 # \u8f93\u51fa\u5206\u5e03 \"\"\" feat = self . stem ( x ) # \u63d0\u53d6\u4e2d\u95f4\u7279\u5f81 logits = self . depth_logits ( feat ) # \u751f\u6210\u6df1\u5ea6 logits # \u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u505a softmax\uff0c\u5f97\u5230\u6bcf\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03 prob = F . softmax ( logits , dim = 1 ) # \u6cbf\u901a\u9053\u7ef4\u5f52\u4e00\u5316 return prob # \u8fd4\u56de\u6df1\u5ea6\u5206\u5e03 __init__ ( in_channels , mid_channels , num_depth_bins , use_bn = True ) \u00b6 Parameters: in_channels ( int ) \u2013 \u8f93\u5165\u7279\u5f81\u901a\u9053\u6570\u3002 # C_in mid_channels ( int ) \u2013 \u4e2d\u95f4\u9690\u5c42\u901a\u9053\u6570\u3002 # C_mid num_depth_bins ( int ) \u2013 \u6df1\u5ea6\u79bb\u6563 bin \u6570\u91cf D\u3002 # D use_bn ( bool , default: True ) \u2013 \u662f\u5426\u5728\u7b2c\u4e00\u5c42\u5377\u79ef\u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN Source code in camera_side\\network\\DepthDistributionHead.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , in_channels : int , mid_channels : int , num_depth_bins : int , use_bn : bool = True , ): \"\"\" Args: in_channels (int): \u8f93\u5165\u7279\u5f81\u901a\u9053\u6570\u3002 # C_in mid_channels (int): \u4e2d\u95f4\u9690\u5c42\u901a\u9053\u6570\u3002 # C_mid num_depth_bins (int): \u6df1\u5ea6\u79bb\u6563 bin \u6570\u91cf D\u3002 # D use_bn (bool): \u662f\u5426\u5728\u7b2c\u4e00\u5c42\u5377\u79ef\u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN \"\"\" super () . __init__ () # \u7236\u7c7b\u6784\u9020 layers = [ # \u7b2c\u4e00\u5c42 conv \u53ca\u53ef\u9009 BN+ReLU nn . Conv2d ( in_channels , mid_channels , kernel_size = 3 , padding = 1 , bias = not use_bn ) # 3\u00d73 Conv ] if use_bn : # \u662f\u5426\u52a0\u5165 BN layers . append ( nn . BatchNorm2d ( mid_channels )) # BN \u5c42 layers . append ( nn . ReLU ( inplace = True )) # ReLU \u6fc0\u6d3b self . stem = nn . Sequential ( * layers ) # \u5c01\u88c5\u4e3a\u987a\u5e8f\u6a21\u5757 self . depth_logits = nn . Conv2d ( mid_channels , num_depth_bins , kernel_size = 1 ) # 1\u00d71 Conv \u9884\u6d4b D \u901a\u9053 self . num_depth_bins = num_depth_bins # \u8bb0\u5f55\u6df1\u5ea6 bins \u6570 forward ( x ) \u00b6 Parameters: x ( Tensor ) \u2013 \u8f93\u5165\u76f8\u673a\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: Tensor \u2013 torch.Tensor: \u6df1\u5ea6\u6982\u7387\u5206\u5e03 (B, D, H, W)\u3002 # \u8f93\u51fa\u5206\u5e03 Source code in camera_side\\network\\DepthDistributionHead.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x (torch.Tensor): \u8f93\u5165\u76f8\u673a\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: torch.Tensor: \u6df1\u5ea6\u6982\u7387\u5206\u5e03 (B, D, H, W)\u3002 # \u8f93\u51fa\u5206\u5e03 \"\"\" feat = self . stem ( x ) # \u63d0\u53d6\u4e2d\u95f4\u7279\u5f81 logits = self . depth_logits ( feat ) # \u751f\u6210\u6df1\u5ea6 logits # \u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u505a softmax\uff0c\u5f97\u5230\u6bcf\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03 prob = F . softmax ( logits , dim = 1 ) # \u6cbf\u901a\u9053\u7ef4\u5f52\u4e00\u5316 return prob # \u8fd4\u56de\u6df1\u5ea6\u5206\u5e03 camera_bev_pooling \u00b6 \u5c06\u591a\u89c6\u89d2 3D \u7279\u5f81\u6c60\u5316\u5230 BEV \u7f51\u683c\u4e2d\u3002 camera_bev_pooling ( feat_3d , bev_indices , bev_h , bev_w ) \u00b6 \u5c06\u76f8\u673a\u5206\u652f\u5728\u4f53\u7d20 / frustum \u7a7a\u95f4\u4e2d\u7684\u4e09\u7ef4\u7279\u5f81\u805a\u5408\u5230 BEV \u7f51\u683c\u4e0a\uff0c \u5f97\u5230\u5f62\u72b6\u4e3a (B, C, bev_h, bev_w) \u7684 Camera BEV \u7279\u5f81\u56fe\u3002 Parameters: feat_3d ( Tensor ) \u2013 (B, D, C, H, W) \u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\uff1a - B: batch size - D: \u6df1\u5ea6\u7ef4\u5ea6\u957f\u5ea6\uff08\u5982\u6df1\u5ea6 bins \u6216\u4f53\u7d20\u5c42\u6570\uff09 - C: \u901a\u9053\u6570 - H, W: \u56fe\u50cf\u5e73\u9762\u4e0a\u7684\u91c7\u6837\u7f51\u683c\u5c3a\u5bf8\u3002 bev_indices ( Tensor ) \u2013 (B, D, H, W, 2) \u6bcf\u4e2a\u4e09\u7ef4\u4f53\u7d20\u5728 BEV \u7f51\u683c\u4e0a\u7684\u6574\u6570\u7d22\u5f15 (i, j)\uff1a - i: BEV \u4e2d\u7684\u7eb5\u5411\u7d22\u5f15\uff0c\u8303\u56f4 [0, bev_h - 1] - j: BEV \u4e2d\u7684\u6a2a\u5411\u7d22\u5f15\uff0c\u8303\u56f4 [0, bev_w - 1] - \u82e5\u67d0\u4e2a\u4f4d\u7f6e\u4e3a\u65e0\u6548\u70b9\uff0c\u53ef\u5c06\u5bf9\u5e94\u7d22\u5f15\u7f6e\u4e3a\u8d1f\u6570\uff08\u5982 -1\uff09\u3002 bev_h ( int ) \u2013 int BEV \u7f51\u683c\u7684\u9ad8\u5ea6\uff08\u7eb5\u5411\u7f51\u683c\u6570\uff09\u3002 bev_w ( int ) \u2013 int BEV \u7f51\u683c\u7684\u5bbd\u5ea6\uff08\u6a2a\u5411\u7f51\u683c\u6570\uff09\u3002 Returns: bev_feat ( Tensor ) \u2013 (B, C, bev_h, bev_w) \u5728 BEV \u7f51\u683c\u4e0a\u805a\u5408\u540e\u7684\u76f8\u673a\u7279\u5f81\u56fe\uff0c\u9ed8\u8ba4\u4f7f\u7528\u6c42\u548c\u805a\u5408\uff08sum pooling\uff09\u3002 Source code in camera_side\\operator\\camera_bev_pooling.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @torch . library . custom_op ( \"bevfusion_ops::camera_bev_pooling\" , mutates_args = [] ) def camera_bev_pooling ( feat_3d : torch . Tensor , bev_indices : torch . Tensor , bev_h : int , bev_w : int , ) -> torch . Tensor : \"\"\" \u5c06\u76f8\u673a\u5206\u652f\u5728\u4f53\u7d20 / frustum \u7a7a\u95f4\u4e2d\u7684\u4e09\u7ef4\u7279\u5f81\u805a\u5408\u5230 BEV \u7f51\u683c\u4e0a\uff0c \u5f97\u5230\u5f62\u72b6\u4e3a (B, C, bev_h, bev_w) \u7684 Camera BEV \u7279\u5f81\u56fe\u3002 Args: feat_3d: (B, D, C, H, W) \u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\uff1a - B: batch size - D: \u6df1\u5ea6\u7ef4\u5ea6\u957f\u5ea6\uff08\u5982\u6df1\u5ea6 bins \u6216\u4f53\u7d20\u5c42\u6570\uff09 - C: \u901a\u9053\u6570 - H, W: \u56fe\u50cf\u5e73\u9762\u4e0a\u7684\u91c7\u6837\u7f51\u683c\u5c3a\u5bf8\u3002 bev_indices: (B, D, H, W, 2) \u6bcf\u4e2a\u4e09\u7ef4\u4f53\u7d20\u5728 BEV \u7f51\u683c\u4e0a\u7684\u6574\u6570\u7d22\u5f15 (i, j)\uff1a - i: BEV \u4e2d\u7684\u7eb5\u5411\u7d22\u5f15\uff0c\u8303\u56f4 [0, bev_h - 1] - j: BEV \u4e2d\u7684\u6a2a\u5411\u7d22\u5f15\uff0c\u8303\u56f4 [0, bev_w - 1] - \u82e5\u67d0\u4e2a\u4f4d\u7f6e\u4e3a\u65e0\u6548\u70b9\uff0c\u53ef\u5c06\u5bf9\u5e94\u7d22\u5f15\u7f6e\u4e3a\u8d1f\u6570\uff08\u5982 -1\uff09\u3002 bev_h: int BEV \u7f51\u683c\u7684\u9ad8\u5ea6\uff08\u7eb5\u5411\u7f51\u683c\u6570\uff09\u3002 bev_w: int BEV \u7f51\u683c\u7684\u5bbd\u5ea6\uff08\u6a2a\u5411\u7f51\u683c\u6570\uff09\u3002 Returns: bev_feat: (B, C, bev_h, bev_w) \u5728 BEV \u7f51\u683c\u4e0a\u805a\u5408\u540e\u7684\u76f8\u673a\u7279\u5f81\u56fe\uff0c\u9ed8\u8ba4\u4f7f\u7528\u6c42\u548c\u805a\u5408\uff08sum pooling\uff09\u3002 \"\"\" assert feat_3d . dim () == 5 , \"feat_3d \u5fc5\u987b\u662f (B, D, C, H, W) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 assert bev_indices . dim () == 5 , \"bev_indices \u5fc5\u987b\u662f (B, D, H, W, 2) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 B , D , C , H , W = feat_3d . shape # \u8bfb\u53d6\u8f93\u5165\u5c3a\u5bf8 assert bev_indices . shape [ 0 ] == B and bev_indices . shape [ 1 ] == D , \"batch / depth \u7ef4\u5ea6\u4e0d\u5339\u914d\" # \u68c0\u67e5 B,D assert bev_indices . shape [ 2 ] == H and bev_indices . shape [ 3 ] == W , \"\u7a7a\u95f4\u5c3a\u5bf8 (H,W) \u4e0d\u5339\u914d\" # \u68c0\u67e5 H,W device = feat_3d . device # \u83b7\u53d6\u8bbe\u5907 bev_feat = torch . zeros ( B , C , bev_h , bev_w , device = device ) # \u521d\u59cb\u5316\u8f93\u51fa BEV \u7279\u5f81 # \u5c06\u4e09\u7ef4\u4f53\u7d20\u5c55\u5e73\u5230\u4e00\u4e2a\u5217\u8868\uff0c\u6309 batch \u5206\u522b\u505a scatter-add # feat_3d: (B, D, C, H, W) \u2192 (B, N, C)\uff0c\u5176\u4e2d N = D*H*W N = D * H * W # \u4e09\u7ef4\u4f4d\u7f6e\u6570\u91cf feat_flat = feat_3d . permute ( 0 , 1 , 3 , 4 , 2 ) . reshape ( B , N , C ) # (B,N,C) idx_flat = bev_indices . reshape ( B , N , 2 ) # (B,N,2) for b in range ( B ): # \u904d\u5386\u6bcf\u4e2a batch feat_b = feat_flat [ b ] # (N,C) idx_b = idx_flat [ b ] # (N,2) i = idx_b [:, 0 ] # \u7eb5\u5411\u7d22\u5f15 j = idx_b [:, 1 ] # \u6a2a\u5411\u7d22\u5f15 # \u6709\u6548\u4f4d\u7f6e\u63a9\u7801\uff1a\u7d22\u5f15\u5728\u5408\u6cd5\u8303\u56f4\u5185 valid_mask = ( i >= 0 ) & ( j >= 0 ) & ( i < bev_h ) & ( j < bev_w ) # \u8fc7\u6ee4\u65e0\u6548\u70b9 if not torch . any ( valid_mask ): # \u82e5\u5168\u90e8\u65e0\u6548\u5219\u8df3\u8fc7 continue i_valid = i [ valid_mask ] # \u6709\u6548 i j_valid = j [ valid_mask ] # \u6709\u6548 j feat_valid = feat_b [ valid_mask ] # \u6709\u6548\u7279\u5f81 (M,C) linear_idx = ( i_valid * bev_w + j_valid ) . long () # \u6620\u5c04\u5230\u4e00\u7ef4\u7d22\u5f15 (M,) # \u5c06\u7279\u5f81\u7d2f\u52a0\u5230 BEV \u7f51\u683c\u4e0a\uff1a\u5148\u5c55\u5e73\u4e3a (C, bev_h*bev_w)\uff0c\u5728\u5217\u7ef4\u5ea6\u505a index_add_ bev_b_flat = bev_feat [ b ] . reshape ( C , bev_h * bev_w ) # (C, bev_h*bev_w) bev_b_flat . index_add_ ( # \u6309\u5217\u7d2f\u52a0\u7279\u5f81 dim = 1 , index = linear_idx , source = feat_valid . T , # (C,M) ) return bev_feat # \u8fd4\u56de\u805a\u5408\u540e\u7684 BEV \u7279\u5f81 camera_geometry_transform \u00b6 \u901a\u8fc7\u76f8\u673a\u5916\u53c2 + \u5185\u53c2\u5c06 2D \u7279\u5f81\u6295\u5f71\u5230 3D \u5750\u6807\u3002 camera_geometry_transform ( points_3d , R , t ) \u00b6 \u5c06\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\u901a\u8fc7\u7ed9\u5b9a\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb\uff0c \u5e76\u8fd4\u56de\u5f62\u72b6\u4fdd\u6301\u4e3a (D, H, W, 3) \u7684\u4e09\u7ef4\u70b9\u4e91\u5f20\u91cf\u3002 Parameters: points_3d ( Tensor ) \u2013 (D, H, W, 3) \u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u901a\u5e38\u7531\u50cf\u7d20\u5347\u7ef4\u7b97\u5b50\u5f97\u5230\uff0c\u683c\u5f0f\u4e3a (x, y, z)\u3002 R ( Tensor ) \u2013 (3, 3) \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u3002 t ( Tensor ) \u2013 (3,) \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u3002 Returns: transformed_points ( Tensor ) \u2013 (D, H, W, 3) \u76ee\u6807\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u5f62\u72b6\u4e0e\u8f93\u5165 points_3d \u76f8\u540c\u3002 Source code in camera_side\\operator\\camera_geometry_transform.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @torch . library . custom_op ( \"bevfusion_ops::camera_geometry_transform\" , mutates_args = []) def camera_geometry_transform ( points_3d : torch . Tensor , R : torch . Tensor , t : torch . Tensor , ) -> torch . Tensor : \"\"\" \u5c06\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\u901a\u8fc7\u7ed9\u5b9a\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb\uff0c \u5e76\u8fd4\u56de\u5f62\u72b6\u4fdd\u6301\u4e3a (D, H, W, 3) \u7684\u4e09\u7ef4\u70b9\u4e91\u5f20\u91cf\u3002 Args: points_3d: (D, H, W, 3) \u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u901a\u5e38\u7531\u50cf\u7d20\u5347\u7ef4\u7b97\u5b50\u5f97\u5230\uff0c\u683c\u5f0f\u4e3a (x, y, z)\u3002 R: (3, 3) \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u3002 t: (3,) \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u3002 Returns: transformed_points: (D, H, W, 3) \u76ee\u6807\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u5f62\u72b6\u4e0e\u8f93\u5165 points_3d \u76f8\u540c\u3002 \"\"\" D , H , W , _ = points_3d . shape # \u8bfb\u53d6\u4f53\u7d20\u7ef4\u5ea6\u4fe1\u606f pts_flat = points_3d . reshape ( - 1 , 3 ) # \u5c55\u5e73\u6210 (D*H*W, 3) pts_trans = ( pts_flat @ R . T ) + t . view ( 1 , 3 ) # \u5e94\u7528\u65cb\u8f6c\u548c\u5e73\u79fb\u53d8\u6362 transformed_points = pts_trans . reshape ( D , H , W , 3 ) # \u8fd8\u539f\u4e3a (D,H,W,3) return transformed_points # \u8fd4\u56de\u53d8\u6362\u540e\u7684\u70b9\u4e91 camera_lift_2d_to_3d \u00b6 \u6839\u636e\u6df1\u5ea6\u79bb\u6563\u5316\uff08Depth Bins\uff09\u5c06 2D \u7279\u5f81 Lift \u5230 3D Voxels\u3002 camera_lift_2d_to_3d ( pixels_2d , depths , K , R , t ) \u00b6 \u8ba1\u7b97 2D \u50cf\u7d20\u5750\u6807\u5728\u7ed9\u5b9a\u79bb\u6563\u6df1\u5ea6\u4e0b\u5bf9\u5e94\u7684 3D \u7a7a\u95f4\u5750\u6807\uff0c\u5e76\u8fd4\u56de\u5f62\u72b6\u4e3a (D, H, W, 3) \u7684\u4e09\u7ef4\u70b9\u4e91\u5f20\u91cf\u3002 Parameters: pixels_2d ( Tensor ) \u2013 (H, W, 2) \u50cf\u7d20\u5750\u6807\u7f51\u683c\uff0c\u683c\u5f0f\u4e3a (u, v)\uff0c\u8868\u793a\u56fe\u50cf\u5e73\u9762\u4e0a\u7684\u4e8c\u7ef4\u50cf\u7d20\u5750\u6807\u3002 depths ( Tensor ) \u2013 (D,) \u79bb\u6563\u6df1\u5ea6\u53d6\u6837\u503c\uff0c\u4e00\u7ef4\u6df1\u5ea6 bin \u5411\u91cf\u3002 K ( Tensor ) \u2013 (3, 3) \u76f8\u673a\u5185\u53c2\u77e9\u9635\u3002 R ( Tensor ) \u2013 (3, 3) \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u3002 t ( Tensor ) \u2013 (3,) \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u3002 Returns: points_3d ( Tensor ) \u2013 (D, H, W, 3) \u6bcf\u4e2a\u50cf\u7d20\u5728\u6bcf\u4e2a\u6df1\u5ea6 bin \u4e0b\u5bf9\u5e94\u7684\u4e09\u7ef4\u7a7a\u95f4\u5750\u6807 (x, y, z)\u3002 Source code in camera_side\\operator\\camera_lift_2d_to_3d.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @torch . library . custom_op ( \"bevfusion_ops::camera_lift_2d_to_3d\" , mutates_args = []) def camera_lift_2d_to_3d ( pixels_2d : torch . Tensor , depths : torch . Tensor , K : torch . Tensor , R : torch . Tensor , t : torch . Tensor , ) -> torch . Tensor : \"\"\" \u8ba1\u7b97 2D \u50cf\u7d20\u5750\u6807\u5728\u7ed9\u5b9a\u79bb\u6563\u6df1\u5ea6\u4e0b\u5bf9\u5e94\u7684 3D \u7a7a\u95f4\u5750\u6807\uff0c\u5e76\u8fd4\u56de\u5f62\u72b6\u4e3a (D, H, W, 3) \u7684\u4e09\u7ef4\u70b9\u4e91\u5f20\u91cf\u3002 Args: pixels_2d: (H, W, 2) \u50cf\u7d20\u5750\u6807\u7f51\u683c\uff0c\u683c\u5f0f\u4e3a (u, v)\uff0c\u8868\u793a\u56fe\u50cf\u5e73\u9762\u4e0a\u7684\u4e8c\u7ef4\u50cf\u7d20\u5750\u6807\u3002 depths: (D,) \u79bb\u6563\u6df1\u5ea6\u53d6\u6837\u503c\uff0c\u4e00\u7ef4\u6df1\u5ea6 bin \u5411\u91cf\u3002 K: (3, 3) \u76f8\u673a\u5185\u53c2\u77e9\u9635\u3002 R: (3, 3) \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u3002 t: (3,) \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u3002 Returns: points_3d: (D, H, W, 3) \u6bcf\u4e2a\u50cf\u7d20\u5728\u6bcf\u4e2a\u6df1\u5ea6 bin \u4e0b\u5bf9\u5e94\u7684\u4e09\u7ef4\u7a7a\u95f4\u5750\u6807 (x, y, z)\u3002 \"\"\" H , W , _ = pixels_2d . shape # \u56fe\u50cf\u9ad8\u5bbd D = depths . shape [ 0 ] # \u6df1\u5ea6 bin \u6570 u = pixels_2d [ ... , 0 ] . reshape ( 1 , H , W ) # \u63d0\u53d6 u \u5750\u6807 v = pixels_2d [ ... , 1 ] . reshape ( 1 , H , W ) # \u63d0\u53d6 v \u5750\u6807 z = depths . view ( D , 1 , 1 ) # \u6df1\u5ea6\u6269\u5c55\u5230 (D,1,1) # \u7531\u9488\u5b54\u6210\u50cf\u6a21\u578b\u53cd\u7b97\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684 (x, y, z) x = ( u - K [ 0 , 2 ]) * z / K [ 0 , 0 ] # \u53cd\u6295\u5f71\u5f97\u5230 x y = ( v - K [ 1 , 2 ]) * z / K [ 1 , 1 ] # \u53cd\u6295\u5f71\u5f97\u5230 y z_expanded = z . expand_as ( x ) # \u6269\u5c55 z \u4ee5\u4fbf\u4e0e x,y \u5bf9\u9f50 pts_cam = torch . stack ([ x , y , z_expanded ], dim =- 1 ) # \u76f8\u673a\u7cfb 3D \u70b9 (D,H,W,3) pts_flat = pts_cam . reshape ( - 1 , 3 ) # \u5c55\u5e73\u4e3a (D*H*W, 3) pts_world = ( pts_flat @ R . T ) + t . view ( 1 , 3 ) # \u5750\u6807\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb pts_world = pts_world . reshape ( D , H , W , 3 ) # \u6062\u590d\u5f62\u72b6\u4e3a (D,H,W,3) return pts_world # \u8fd4\u56de\u4e09\u7ef4\u70b9\u4e91 depth_weighted_feature_expand \u00b6 \u5229\u7528\u6df1\u5ea6\u5206\u5e03\u5bf9\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u52a0\u6743\u6269\u5c55\uff0c\u589e\u5f3a BEV \u6295\u5f71\u8d28\u91cf\u3002 depth_weighted_feature_expand ( img_feats , depth_prob ) \u00b6 \u6839\u636e\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u5bf9\u76f8\u673a 2D \u7279\u5f81\u8fdb\u884c\u52a0\u6743\u5e76\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u5c55\u5f00\uff0c \u5f97\u5230\u5f62\u72b6\u4e3a (B, D, C, H, W) \u7684\u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\u3002 Parameters: img_feats ( Tensor ) \u2013 (B, C, H, W) \u76f8\u673a\u5206\u652f\u8f93\u51fa\u7684 2D \u7279\u5f81\u56fe\uff1a - B: batch size - C: \u901a\u9053\u6570 - H, W: \u7279\u5f81\u56fe\u7a7a\u95f4\u5c3a\u5bf8\u3002 depth_prob ( Tensor ) \u2013 (B, D, H, W) \u5bf9\u5e94\u50cf\u7d20\u7684\u79bb\u6563\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff1a - D: \u6df1\u5ea6 bin \u6570\u91cf - \u4e0e img_feats \u5728 (B, H, W) \u7ef4\u5ea6\u4e0a\u5bf9\u9f50\u3002 Returns: feat_3d ( Tensor ) \u2013 (B, D, C, H, W) \u6309\u6df1\u5ea6\u6982\u7387\u52a0\u6743\u5e76\u5728\u6df1\u5ea6\u7ef4\u5ea6\u5c55\u5f00\u540e\u7684\u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\uff0c \u6bcf\u4e2a\u4f53\u7d20\u4f4d\u7f6e\u540c\u65f6\u7f16\u7801\u4e86\u50cf\u7d20\u8bed\u4e49\u4e0e\u5bf9\u5e94\u6df1\u5ea6\u6743\u91cd\u3002 Source code in camera_side\\operator\\depth_weighted_feature_expand.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @torch . library . custom_op ( \"bevfusion_ops::depth_weighted_feature_expand\" , mutates_args = [] ) def depth_weighted_feature_expand ( img_feats : torch . Tensor , depth_prob : torch . Tensor , ) -> torch . Tensor : \"\"\" \u6839\u636e\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u5bf9\u76f8\u673a 2D \u7279\u5f81\u8fdb\u884c\u52a0\u6743\u5e76\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u5c55\u5f00\uff0c \u5f97\u5230\u5f62\u72b6\u4e3a (B, D, C, H, W) \u7684\u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\u3002 Args: img_feats: (B, C, H, W) \u76f8\u673a\u5206\u652f\u8f93\u51fa\u7684 2D \u7279\u5f81\u56fe\uff1a - B: batch size - C: \u901a\u9053\u6570 - H, W: \u7279\u5f81\u56fe\u7a7a\u95f4\u5c3a\u5bf8\u3002 depth_prob: (B, D, H, W) \u5bf9\u5e94\u50cf\u7d20\u7684\u79bb\u6563\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff1a - D: \u6df1\u5ea6 bin \u6570\u91cf - \u4e0e img_feats \u5728 (B, H, W) \u7ef4\u5ea6\u4e0a\u5bf9\u9f50\u3002 Returns: feat_3d: (B, D, C, H, W) \u6309\u6df1\u5ea6\u6982\u7387\u52a0\u6743\u5e76\u5728\u6df1\u5ea6\u7ef4\u5ea6\u5c55\u5f00\u540e\u7684\u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\uff0c \u6bcf\u4e2a\u4f53\u7d20\u4f4d\u7f6e\u540c\u65f6\u7f16\u7801\u4e86\u50cf\u7d20\u8bed\u4e49\u4e0e\u5bf9\u5e94\u6df1\u5ea6\u6743\u91cd\u3002 \"\"\" assert img_feats . dim () == 4 , \"img_feats \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u7b80\u5355\u5f62\u72b6\u68c0\u67e5 assert depth_prob . dim () == 4 , \"depth_prob \u5fc5\u987b\u662f (B, D, H, W) \u5f20\u91cf\" # \u7b80\u5355\u5f62\u72b6\u68c0\u67e5 assert img_feats . shape [ 0 ] == depth_prob . shape [ 0 ], \"batch \u7ef4\u5ea6\u4e0d\u5339\u914d\" # \u68c0\u67e5 B assert img_feats . shape [ 2 :] == depth_prob . shape [ 2 :], \"\u7a7a\u95f4\u5c3a\u5bf8 (H,W) \u4e0d\u5339\u914d\" # \u68c0\u67e5 H,W # \u6269\u5c55\u7ef4\u5ea6\u4ee5\u4fbf\u5e7f\u64ad\u76f8\u4e58\uff1a # img_feats: (B, C, H, W) \u2192 (B, 1, C, H, W) # depth_prob: (B, D, H, W) \u2192 (B, D, 1, H, W) img_feats_expanded = img_feats . unsqueeze ( 1 ) # \u5728\u6df1\u5ea6\u4f4d\u7f6e\u63d2\u5165\u7ef4\u5ea6 depth_prob_expanded = depth_prob . unsqueeze ( 2 ) # \u5728\u901a\u9053\u4f4d\u7f6e\u63d2\u5165\u7ef4\u5ea6 feat_3d = img_feats_expanded * depth_prob_expanded # \u6309\u6df1\u5ea6\u6982\u7387\u52a0\u6743\u5c55\u5f00 return feat_3d # (B, D, C, H, W)","title":"Camera \u5206\u652f\u6a21\u5757"},{"location":"camera_side/#camera-side","text":"Camera Side \u6a21\u5757\u7684\u4f5c\u7528\u662f\uff1a \u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u4e2d\u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u3001\u6df1\u5ea6\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u51e0\u4f55\u65b9\u6cd5\u5c06 2D \u7279\u5f81\u8f6c\u6362\u5230 3D / BEV \u7a7a\u95f4\uff0c\u4e3a\u540e\u7eed BEV \u878d\u5408\u63d0\u4f9b\u56fe\u50cf\u4fa7 BEV \u7279\u5f81\u3002 \u672c\u6a21\u5757\u5305\u542b\u56db\u7c7b\u6838\u5fc3\u529f\u80fd\uff1a \u56fe\u50cf\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff08FPN / Backbone\uff09 \u63d0\u53d6\u591a\u5c3a\u5ea6\u56fe\u50cf\u8bed\u4e49\u7279\u5f81\u3002 \u6df1\u5ea6\u9884\u6d4b\u6a21\u5757 \u8f93\u51fa\u50cf\u7d20\u7ea7\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u7528\u4e8e Lift \u7279\u5f81\u5230 3D\u3002 \u51e0\u4f55\u6295\u5f71\u7b97\u5b50\uff08Lift / Transform / Pooling\uff09 \u5c06\u56fe\u50cf\u7279\u5f81\u6620\u5c04\u81f3 3D \u7a7a\u95f4\uff0c\u518d\u6295\u5f71\u5230 BEV \u7f51\u683c\u3002 \u7279\u5f81\u589e\u5f3a\u7b97\u5b50 \u5bf9\u56fe\u50cf BEV \u7279\u5f81\u8fdb\u884c\u52a0\u6743\u6216\u6269\u5c55\u3002","title":"Camera Side\uff08\u56fe\u50cf\u4fa7\u6a21\u5757\uff09"},{"location":"camera_side/#api","text":"\u4ee5\u4e0b\u5217\u51fa Camera Side \u4e2d\u5168\u90e8\u7b97\u5b50\u4e0e\u7f51\u7edc\u6a21\u5757\uff0c\u5e76\u7ed9\u51fa\u5176\u5bf9\u5e94\u6587\u4ef6\u8def\u5f84\u3002","title":"API \u6587\u6863"},{"location":"camera_side/#camerafpnfusion","text":"\u591a\u5c3a\u5ea6 FPN \u7279\u5f81\u878d\u5408\u6a21\u5757\u3002","title":"CameraFPNFusion"},{"location":"camera_side/#camera_side.network.CameraFPNFusion.CameraFPNFusion","text":"Bases: Module CameraFPNFusion \u6a21\u5757\uff08\u56fe\u50cf\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u901a\u8fc7\u5bf9\u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5206\u522b\u505a 1\u00d71 Conv \u901a\u9053\u6620\u5c04\u3001\u4e0a\u91c7\u6837\u5230\u7edf\u4e00\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c \u7136\u540e\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u9010\u5143\u7d20\u76f8\u52a0\uff0c\u5f97\u5230\u4e00\u5f20\u878d\u5408\u540e\u7684 2D \u7279\u5f81\u56fe\u3002\u8be5\u7279\u5f81\u56fe\u4f5c\u4e3a\u540e\u7eed \u6df1\u5ea6\u4f30\u8ba1\u4e0e Camera-to-BEV \u89c6\u89d2\u53d8\u6362\u7684\u7edf\u4e00\u8f93\u5165\u63a5\u53e3\u3002","title":"CameraFPNFusion"},{"location":"camera_side/#camera_side.network.CameraFPNFusion.CameraFPNFusion--_1","text":"\u5bf9\u6bcf\u4e2a\u5c3a\u5ea6\u7279\u5f81\u4f7f\u7528 Conv2d C_in_i \u2192 C_out, kernel_size=1, stride=1, padding=0 \u505a\u901a\u9053\u7ebf\u6027\u53d8\u6362\uff0c\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u6570 C_out (\u53ef\u9009) BatchNorm2d \u5bf9\u6bcf\u4e2a\u5c3a\u5ea6\u6620\u5c04\u540e\u7684\u7279\u5f81\u505a\u5f52\u4e00\u5316 (\u53ef\u9009) ReLU \u6fc0\u6d3b \u5728\u878d\u5408\u524d\u589e\u5f3a\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b \u7279\u5f81\u4e0a\u91c7\u6837\u5230\u6700\u9ad8\u5206\u8fa8\u7387\u5e76\u9010\u5143\u7d20\u76f8\u52a0 \u5f97\u5230\u5355\u5c3a\u5ea6\u3001\u591a\u901a\u9053\u7684\u878d\u5408\u7279\u5f81\u56fe","title":"\u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09"},{"location":"camera_side/#camera_side.network.CameraFPNFusion.CameraFPNFusion--_2","text":"Args: feats (List[torch.Tensor]): \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 \u7b2c i \u4e2a\u7279\u5f81\u5f62\u72b6\u4e3a (B, C_in_i, H_i, W_i)\uff1a - B: batch size - C_in_i: \u7b2c i \u4e2a\u5c3a\u5ea6\u7684\u901a\u9053\u6570 - H_i, W_i: \u7b2c i \u4e2a\u5c3a\u5ea6\u7684\u7a7a\u95f4\u5c3a\u5bf8","title":"\u8f93\u5165"},{"location":"camera_side/#camera_side.network.CameraFPNFusion.CameraFPNFusion--_3","text":"Returns: torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe\uff0c\u5f62\u72b6\u4e3a (B, C_out, H_out, W_out)\uff0c \u5176\u4e2d H_out, W_out \u7b49\u4e8e\u5217\u8868\u4e2d\u6700\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002","title":"\u8f93\u51fa"},{"location":"camera_side/#camera_side.network.CameraFPNFusion.CameraFPNFusion--_4","text":"\u5c06\u6765\u81ea\u76f8\u673a backbone \u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5bf9\u9f50\u5230\u7edf\u4e00\u7a7a\u95f4\u5c3a\u5ea6\u4e0e\u901a\u9053\u6570 \u4f5c\u4e3a\u540e\u7eed depth head / LSS \u89c6\u89d2\u53d8\u6362\u7b97\u5b50\u7684\u7edf\u4e00 2D \u7279\u5f81\u8f93\u5165 \u4e5f\u53ef\u4f5c\u4e3a\u901a\u7528\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7b97\u5b50\u590d\u7528\u4e8e\u5176\u5b83\u76f8\u673a\u5206\u652f\u7f51\u7edc Source code in camera_side\\network\\CameraFPNFusion.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 class CameraFPNFusion ( nn . Module ): \"\"\" CameraFPNFusion \u6a21\u5757\uff08\u56fe\u50cf\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u901a\u8fc7\u5bf9\u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5206\u522b\u505a 1\u00d71 Conv \u901a\u9053\u6620\u5c04\u3001\u4e0a\u91c7\u6837\u5230\u7edf\u4e00\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c \u7136\u540e\u5728\u901a\u9053\u7ef4\u5ea6\u4e0a\u9010\u5143\u7d20\u76f8\u52a0\uff0c\u5f97\u5230\u4e00\u5f20\u878d\u5408\u540e\u7684 2D \u7279\u5f81\u56fe\u3002\u8be5\u7279\u5f81\u56fe\u4f5c\u4e3a\u540e\u7eed \u6df1\u5ea6\u4f30\u8ba1\u4e0e Camera-to-BEV \u89c6\u89d2\u53d8\u6362\u7684\u7edf\u4e00\u8f93\u5165\u63a5\u53e3\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. \u5bf9\u6bcf\u4e2a\u5c3a\u5ea6\u7279\u5f81\u4f7f\u7528 **Conv2d C_in_i \u2192 C_out, kernel_size=1, stride=1, padding=0** - \u505a\u901a\u9053\u7ebf\u6027\u53d8\u6362\uff0c\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u6570 C_out 2. **(\u53ef\u9009) BatchNorm2d** - \u5bf9\u6bcf\u4e2a\u5c3a\u5ea6\u6620\u5c04\u540e\u7684\u7279\u5f81\u505a\u5f52\u4e00\u5316 3. **(\u53ef\u9009) ReLU \u6fc0\u6d3b** - \u5728\u878d\u5408\u524d\u589e\u5f3a\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b 4. \u7279\u5f81\u4e0a\u91c7\u6837\u5230\u6700\u9ad8\u5206\u8fa8\u7387\u5e76\u9010\u5143\u7d20\u76f8\u52a0 - \u5f97\u5230\u5355\u5c3a\u5ea6\u3001\u591a\u901a\u9053\u7684\u878d\u5408\u7279\u5f81\u56fe --- ### \u8f93\u5165 Args: feats (List[torch.Tensor]): \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 \u7b2c i \u4e2a\u7279\u5f81\u5f62\u72b6\u4e3a (B, C_in_i, H_i, W_i)\uff1a - B: batch size - C_in_i: \u7b2c i \u4e2a\u5c3a\u5ea6\u7684\u901a\u9053\u6570 - H_i, W_i: \u7b2c i \u4e2a\u5c3a\u5ea6\u7684\u7a7a\u95f4\u5c3a\u5bf8 --- ### \u8f93\u51fa Returns: torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe\uff0c\u5f62\u72b6\u4e3a (B, C_out, H_out, W_out)\uff0c \u5176\u4e2d H_out, W_out \u7b49\u4e8e\u5217\u8868\u4e2d\u6700\u9ad8\u5206\u8fa8\u7387\u7279\u5f81\u56fe\u7684\u7a7a\u95f4\u5c3a\u5bf8\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u5c06\u6765\u81ea\u76f8\u673a backbone \u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u5bf9\u9f50\u5230\u7edf\u4e00\u7a7a\u95f4\u5c3a\u5ea6\u4e0e\u901a\u9053\u6570 - \u4f5c\u4e3a\u540e\u7eed depth head / LSS \u89c6\u89d2\u53d8\u6362\u7b97\u5b50\u7684\u7edf\u4e00 2D \u7279\u5f81\u8f93\u5165 - \u4e5f\u53ef\u4f5c\u4e3a\u901a\u7528\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u7b97\u5b50\u590d\u7528\u4e8e\u5176\u5b83\u76f8\u673a\u5206\u652f\u7f51\u7edc \"\"\" def __init__ ( self , in_channels_list , out_channels : int , use_bn : bool = True , use_relu : bool = True , ): \"\"\" Args: in_channels_list (List[int]): \u591a\u5c3a\u5ea6\u8f93\u5165\u7279\u5f81\u7684\u901a\u9053\u6570\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 # \u7b80\u77ed\u89e3\u91ca out_channels (int): \u878d\u5408\u540e\u7279\u5f81\u7684\u76ee\u6807\u901a\u9053\u6570\u3002 # \u76ee\u6807\u901a\u9053 use_bn (bool): \u662f\u5426\u5728 1\u00d71 Conv \u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN use_relu (bool): \u662f\u5426\u5728 BN \u540e\u4f7f\u7528 ReLU \u6fc0\u6d3b\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 ReLU \"\"\" super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . out_channels = out_channels # \u8bb0\u5f55\u8f93\u51fa\u901a\u9053\u6570 self . use_bn = use_bn # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528 BN self . use_relu = use_relu # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528 ReLU # \u4e3a\u6bcf\u4e2a\u5c3a\u5ea6\u6784\u5efa\u4e00\u5957 1\u00d71 Conv (+BN+ReLU) \u8fdb\u884c\u901a\u9053\u6620\u5c04 self . proj_layers = nn . ModuleList () # \u4fdd\u5b58\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u6620\u5c04\u5c42 for c_in in in_channels_list : # \u904d\u5386\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u8f93\u5165\u901a\u9053 layers = [ nn . Conv2d ( c_in , out_channels , 1 , bias = not use_bn )] # 1\u00d71 Conv if use_bn : layers . append ( nn . BatchNorm2d ( out_channels )) # \u53ef\u9009 BN if use_relu : layers . append ( nn . ReLU ( inplace = True )) # \u53ef\u9009 ReLU self . proj_layers . append ( nn . Sequential ( * layers )) # \u5c01\u88c5\u4e3a\u987a\u5e8f\u6a21\u5757 def forward ( self , feats ): \"\"\" \u524d\u5411\u8ba1\u7b97\uff1a\u5c06\u591a\u5c3a\u5ea6\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u5e76\u4e0a\u91c7\u6837\u540e\u76f8\u52a0\u3002 Args: feats (List[torch.Tensor]): \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u5f62\u72b6\u53c2\u89c1\u7c7b\u6ce8\u91ca\u3002 # \u8f93\u5165\u7279\u5f81\u5217\u8868 Returns: torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe (B, C_out, H_out, W_out)\u3002 # \u8fd4\u56de\u878d\u5408\u7279\u5f81 \"\"\" assert len ( feats ) == len ( self . proj_layers ), \"feats \u6570\u91cf\u4e0e proj_layers \u4e0d\u4e00\u81f4\" # \u7b80\u5355\u68c0\u67e5 # \u4ee5\u7b2c\u4e00\u4e2a\u7279\u5f81\u4e3a\u6700\u9ad8\u5206\u8fa8\u7387\u53c2\u8003 ref_feat = feats [ 0 ] # \u53c2\u8003\u5c3a\u5ea6\u7279\u5f81 _ , _ , H_ref , W_ref = ref_feat . shape # \u53c2\u8003\u7a7a\u95f4\u5c3a\u5bf8 fused = 0.0 # \u521d\u59cb\u5316\u878d\u5408\u7ed3\u679c for feat , proj in zip ( feats , self . proj_layers ): # \u904d\u5386\u6bcf\u4e2a\u5c3a\u5ea6 x = proj ( feat ) # \u901a\u9053\u6620\u5c04\u5230 C_out if x . shape [ - 2 :] != ( H_ref , W_ref ): # \u82e5\u7a7a\u95f4\u5c3a\u5bf8\u4e0d\u540c x = F . interpolate ( x , size = ( H_ref , W_ref ), mode = \"bilinear\" , align_corners = False ) # \u4e0a\u91c7\u6837\u5230\u53c2\u8003\u5c3a\u5ea6 fused = fused + x # \u9010\u5c3a\u5ea6\u7d2f\u52a0\u7279\u5f81 return fused # \u8fd4\u56de\u878d\u5408\u7ed3\u679c","title":"\u6a21\u5757\u7528\u9014"},{"location":"camera_side/#camera_side.network.CameraFPNFusion.CameraFPNFusion.__init__","text":"Parameters: in_channels_list ( List [ int ] ) \u2013 \u591a\u5c3a\u5ea6\u8f93\u5165\u7279\u5f81\u7684\u901a\u9053\u6570\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 # \u7b80\u77ed\u89e3\u91ca out_channels ( int ) \u2013 \u878d\u5408\u540e\u7279\u5f81\u7684\u76ee\u6807\u901a\u9053\u6570\u3002 # \u76ee\u6807\u901a\u9053 use_bn ( bool , default: True ) \u2013 \u662f\u5426\u5728 1\u00d71 Conv \u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN use_relu ( bool , default: True ) \u2013 \u662f\u5426\u5728 BN \u540e\u4f7f\u7528 ReLU \u6fc0\u6d3b\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 ReLU Source code in camera_side\\network\\CameraFPNFusion.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 def __init__ ( self , in_channels_list , out_channels : int , use_bn : bool = True , use_relu : bool = True , ): \"\"\" Args: in_channels_list (List[int]): \u591a\u5c3a\u5ea6\u8f93\u5165\u7279\u5f81\u7684\u901a\u9053\u6570\u5217\u8868\uff0c\u957f\u5ea6\u4e3a N_scales\u3002 # \u7b80\u77ed\u89e3\u91ca out_channels (int): \u878d\u5408\u540e\u7279\u5f81\u7684\u76ee\u6807\u901a\u9053\u6570\u3002 # \u76ee\u6807\u901a\u9053 use_bn (bool): \u662f\u5426\u5728 1\u00d71 Conv \u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN use_relu (bool): \u662f\u5426\u5728 BN \u540e\u4f7f\u7528 ReLU \u6fc0\u6d3b\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 ReLU \"\"\" super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . out_channels = out_channels # \u8bb0\u5f55\u8f93\u51fa\u901a\u9053\u6570 self . use_bn = use_bn # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528 BN self . use_relu = use_relu # \u8bb0\u5f55\u662f\u5426\u4f7f\u7528 ReLU # \u4e3a\u6bcf\u4e2a\u5c3a\u5ea6\u6784\u5efa\u4e00\u5957 1\u00d71 Conv (+BN+ReLU) \u8fdb\u884c\u901a\u9053\u6620\u5c04 self . proj_layers = nn . ModuleList () # \u4fdd\u5b58\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u6620\u5c04\u5c42 for c_in in in_channels_list : # \u904d\u5386\u6bcf\u4e2a\u5c3a\u5ea6\u7684\u8f93\u5165\u901a\u9053 layers = [ nn . Conv2d ( c_in , out_channels , 1 , bias = not use_bn )] # 1\u00d71 Conv if use_bn : layers . append ( nn . BatchNorm2d ( out_channels )) # \u53ef\u9009 BN if use_relu : layers . append ( nn . ReLU ( inplace = True )) # \u53ef\u9009 ReLU self . proj_layers . append ( nn . Sequential ( * layers )) # \u5c01\u88c5\u4e3a\u987a\u5e8f\u6a21\u5757","title":"__init__"},{"location":"camera_side/#camera_side.network.CameraFPNFusion.CameraFPNFusion.forward","text":"\u524d\u5411\u8ba1\u7b97\uff1a\u5c06\u591a\u5c3a\u5ea6\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u5e76\u4e0a\u91c7\u6837\u540e\u76f8\u52a0\u3002 Parameters: feats ( List [ Tensor ] ) \u2013 \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u5f62\u72b6\u53c2\u89c1\u7c7b\u6ce8\u91ca\u3002 # \u8f93\u5165\u7279\u5f81\u5217\u8868 Returns: \u2013 torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe (B, C_out, H_out, W_out)\u3002 # \u8fd4\u56de\u878d\u5408\u7279\u5f81 Source code in camera_side\\network\\CameraFPNFusion.py 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 def forward ( self , feats ): \"\"\" \u524d\u5411\u8ba1\u7b97\uff1a\u5c06\u591a\u5c3a\u5ea6\u7279\u5f81\u6620\u5c04\u5230\u7edf\u4e00\u901a\u9053\u5e76\u4e0a\u91c7\u6837\u540e\u76f8\u52a0\u3002 Args: feats (List[torch.Tensor]): \u591a\u5c3a\u5ea6\u76f8\u673a\u7279\u5f81\u5217\u8868\uff0c\u5f62\u72b6\u53c2\u89c1\u7c7b\u6ce8\u91ca\u3002 # \u8f93\u5165\u7279\u5f81\u5217\u8868 Returns: torch.Tensor: \u878d\u5408\u540e\u7684\u5355\u5c3a\u5ea6\u7279\u5f81\u56fe (B, C_out, H_out, W_out)\u3002 # \u8fd4\u56de\u878d\u5408\u7279\u5f81 \"\"\" assert len ( feats ) == len ( self . proj_layers ), \"feats \u6570\u91cf\u4e0e proj_layers \u4e0d\u4e00\u81f4\" # \u7b80\u5355\u68c0\u67e5 # \u4ee5\u7b2c\u4e00\u4e2a\u7279\u5f81\u4e3a\u6700\u9ad8\u5206\u8fa8\u7387\u53c2\u8003 ref_feat = feats [ 0 ] # \u53c2\u8003\u5c3a\u5ea6\u7279\u5f81 _ , _ , H_ref , W_ref = ref_feat . shape # \u53c2\u8003\u7a7a\u95f4\u5c3a\u5bf8 fused = 0.0 # \u521d\u59cb\u5316\u878d\u5408\u7ed3\u679c for feat , proj in zip ( feats , self . proj_layers ): # \u904d\u5386\u6bcf\u4e2a\u5c3a\u5ea6 x = proj ( feat ) # \u901a\u9053\u6620\u5c04\u5230 C_out if x . shape [ - 2 :] != ( H_ref , W_ref ): # \u82e5\u7a7a\u95f4\u5c3a\u5bf8\u4e0d\u540c x = F . interpolate ( x , size = ( H_ref , W_ref ), mode = \"bilinear\" , align_corners = False ) # \u4e0a\u91c7\u6837\u5230\u53c2\u8003\u5c3a\u5ea6 fused = fused + x # \u9010\u5c3a\u5ea6\u7d2f\u52a0\u7279\u5f81 return fused # \u8fd4\u56de\u878d\u5408\u7ed3\u679c","title":"forward"},{"location":"camera_side/#depthdistributionhead","text":"\u50cf\u7d20\u7ea7\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b\u6a21\u5757\u3002","title":"DepthDistributionHead"},{"location":"camera_side/#camera_side.network.DepthDistributionHead.DepthDistributionHead","text":"Bases: Module DepthDistributionHead \u6a21\u5757\uff08\u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u878d\u5408\u540e\u7684\u76f8\u673a 2D \u7279\u5f81\u56fe\uff0c\u901a\u8fc7\u82e5\u5e72 2D \u5377\u79ef\u5c42\u9884\u6d4b\u6bcf\u4e2a\u50cf\u7d20\u5728 \u79bb\u6563\u6df1\u5ea6 bins \u4e0a\u7684\u6982\u7387\u5206\u5e03\uff0c\u7528\u4e8e\u540e\u7eed\u7684 2D\u21923D \u5347\u7ef4\u4e0e Camera-to-BEV \u89c6\u89d2\u53d8\u6362\u3002\u8f93\u51fa\u4e3a (B, D, H, W)\uff0c\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u505a softmax \u5f52\u4e00\u5316\u3002","title":"DepthDistributionHead"},{"location":"camera_side/#camera_side.network.DepthDistributionHead.DepthDistributionHead--_1","text":"Conv2d C_in \u2192 C_mid, kernel_size=3, padding=1 \u5bf9\u8f93\u5165\u7279\u5f81\u505a\u5c40\u90e8\u611f\u53d7\u91ce\u589e\u5f3a (\u53ef\u9009) BatchNorm2d + ReLU \u7a33\u5b9a\u8bad\u7ec3\u5e76\u5f15\u5165\u975e\u7ebf\u6027 Conv2d C_mid \u2192 D, kernel_size=1 \u6620\u5c04\u5230\u6df1\u5ea6\u901a\u9053\u7ef4\u5ea6 D Softmax (\u6cbf depth \u7ef4\u5ea6) \u5f97\u5230\u6bcf\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03","title":"\u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09"},{"location":"camera_side/#camera_side.network.DepthDistributionHead.DepthDistributionHead--_2","text":"Args: x (torch.Tensor): \u76f8\u673a\u878d\u5408\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\uff1a - B: batch size - C_in: \u8f93\u5165\u901a\u9053\u6570 - H, W: \u7279\u5f81\u56fe\u7a7a\u95f4\u5c3a\u5bf8","title":"\u8f93\u5165"},{"location":"camera_side/#camera_side.network.DepthDistributionHead.DepthDistributionHead--_3","text":"Returns: torch.Tensor: \u50cf\u7d20\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u5f62\u72b6 (B, D, H, W)\uff0c\u5728\u7ef4\u5ea6 1 \u4e0a\u6c42\u548c\u4e3a 1\u3002","title":"\u8f93\u51fa"},{"location":"camera_side/#camera_side.network.DepthDistributionHead.DepthDistributionHead--_4","text":"\u4e3a LSS / BEVFusion \u63d0\u4f9b\u6bcf\u4e2a\u50cf\u7d20\u5728\u79bb\u6563\u6df1\u5ea6\u4e0a\u7684\u6982\u7387\u4f30\u8ba1 \u540e\u7eed\u7b97\u5b50 camera_lift_2d_to_3d \u3001 depth_weighted_feature_expand \u5c06\u590d\u7528\u8be5\u6df1\u5ea6\u5206\u5e03\u5b8c\u6210 2D\u21923D \u5347\u7ef4\u4e0e\u7279\u5f81\u52a0\u6743 Source code in camera_side\\network\\DepthDistributionHead.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class DepthDistributionHead ( nn . Module ): \"\"\" DepthDistributionHead \u6a21\u5757\uff08\u50cf\u7d20\u6df1\u5ea6\u5206\u5e03\u9884\u6d4b\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u878d\u5408\u540e\u7684\u76f8\u673a 2D \u7279\u5f81\u56fe\uff0c\u901a\u8fc7\u82e5\u5e72 2D \u5377\u79ef\u5c42\u9884\u6d4b\u6bcf\u4e2a\u50cf\u7d20\u5728 \u79bb\u6563\u6df1\u5ea6 bins \u4e0a\u7684\u6982\u7387\u5206\u5e03\uff0c\u7528\u4e8e\u540e\u7eed\u7684 2D\u21923D \u5347\u7ef4\u4e0e Camera-to-BEV \u89c6\u89d2\u53d8\u6362\u3002\u8f93\u51fa\u4e3a (B, D, H, W)\uff0c\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u505a softmax \u5f52\u4e00\u5316\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. **Conv2d C_in \u2192 C_mid, kernel_size=3, padding=1** - \u5bf9\u8f93\u5165\u7279\u5f81\u505a\u5c40\u90e8\u611f\u53d7\u91ce\u589e\u5f3a 2. **(\u53ef\u9009) BatchNorm2d + ReLU** - \u7a33\u5b9a\u8bad\u7ec3\u5e76\u5f15\u5165\u975e\u7ebf\u6027 3. **Conv2d C_mid \u2192 D, kernel_size=1** - \u6620\u5c04\u5230\u6df1\u5ea6\u901a\u9053\u7ef4\u5ea6 D 4. **Softmax (\u6cbf depth \u7ef4\u5ea6)** - \u5f97\u5230\u6bcf\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03 --- ### \u8f93\u5165 Args: x (torch.Tensor): \u76f8\u673a\u878d\u5408\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\uff1a - B: batch size - C_in: \u8f93\u5165\u901a\u9053\u6570 - H, W: \u7279\u5f81\u56fe\u7a7a\u95f4\u5c3a\u5bf8 --- ### \u8f93\u51fa Returns: torch.Tensor: \u50cf\u7d20\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u5f62\u72b6 (B, D, H, W)\uff0c\u5728\u7ef4\u5ea6 1 \u4e0a\u6c42\u548c\u4e3a 1\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u4e3a LSS / BEVFusion \u63d0\u4f9b\u6bcf\u4e2a\u50cf\u7d20\u5728\u79bb\u6563\u6df1\u5ea6\u4e0a\u7684\u6982\u7387\u4f30\u8ba1 - \u540e\u7eed\u7b97\u5b50 `camera_lift_2d_to_3d`\u3001`depth_weighted_feature_expand` \u5c06\u590d\u7528\u8be5\u6df1\u5ea6\u5206\u5e03\u5b8c\u6210 2D\u21923D \u5347\u7ef4\u4e0e\u7279\u5f81\u52a0\u6743 \"\"\" def __init__ ( self , in_channels : int , mid_channels : int , num_depth_bins : int , use_bn : bool = True , ): \"\"\" Args: in_channels (int): \u8f93\u5165\u7279\u5f81\u901a\u9053\u6570\u3002 # C_in mid_channels (int): \u4e2d\u95f4\u9690\u5c42\u901a\u9053\u6570\u3002 # C_mid num_depth_bins (int): \u6df1\u5ea6\u79bb\u6563 bin \u6570\u91cf D\u3002 # D use_bn (bool): \u662f\u5426\u5728\u7b2c\u4e00\u5c42\u5377\u79ef\u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN \"\"\" super () . __init__ () # \u7236\u7c7b\u6784\u9020 layers = [ # \u7b2c\u4e00\u5c42 conv \u53ca\u53ef\u9009 BN+ReLU nn . Conv2d ( in_channels , mid_channels , kernel_size = 3 , padding = 1 , bias = not use_bn ) # 3\u00d73 Conv ] if use_bn : # \u662f\u5426\u52a0\u5165 BN layers . append ( nn . BatchNorm2d ( mid_channels )) # BN \u5c42 layers . append ( nn . ReLU ( inplace = True )) # ReLU \u6fc0\u6d3b self . stem = nn . Sequential ( * layers ) # \u5c01\u88c5\u4e3a\u987a\u5e8f\u6a21\u5757 self . depth_logits = nn . Conv2d ( mid_channels , num_depth_bins , kernel_size = 1 ) # 1\u00d71 Conv \u9884\u6d4b D \u901a\u9053 self . num_depth_bins = num_depth_bins # \u8bb0\u5f55\u6df1\u5ea6 bins \u6570 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x (torch.Tensor): \u8f93\u5165\u76f8\u673a\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: torch.Tensor: \u6df1\u5ea6\u6982\u7387\u5206\u5e03 (B, D, H, W)\u3002 # \u8f93\u51fa\u5206\u5e03 \"\"\" feat = self . stem ( x ) # \u63d0\u53d6\u4e2d\u95f4\u7279\u5f81 logits = self . depth_logits ( feat ) # \u751f\u6210\u6df1\u5ea6 logits # \u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u505a softmax\uff0c\u5f97\u5230\u6bcf\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03 prob = F . softmax ( logits , dim = 1 ) # \u6cbf\u901a\u9053\u7ef4\u5f52\u4e00\u5316 return prob # \u8fd4\u56de\u6df1\u5ea6\u5206\u5e03","title":"\u6a21\u5757\u7528\u9014"},{"location":"camera_side/#camera_side.network.DepthDistributionHead.DepthDistributionHead.__init__","text":"Parameters: in_channels ( int ) \u2013 \u8f93\u5165\u7279\u5f81\u901a\u9053\u6570\u3002 # C_in mid_channels ( int ) \u2013 \u4e2d\u95f4\u9690\u5c42\u901a\u9053\u6570\u3002 # C_mid num_depth_bins ( int ) \u2013 \u6df1\u5ea6\u79bb\u6563 bin \u6570\u91cf D\u3002 # D use_bn ( bool , default: True ) \u2013 \u662f\u5426\u5728\u7b2c\u4e00\u5c42\u5377\u79ef\u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN Source code in camera_side\\network\\DepthDistributionHead.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 def __init__ ( self , in_channels : int , mid_channels : int , num_depth_bins : int , use_bn : bool = True , ): \"\"\" Args: in_channels (int): \u8f93\u5165\u7279\u5f81\u901a\u9053\u6570\u3002 # C_in mid_channels (int): \u4e2d\u95f4\u9690\u5c42\u901a\u9053\u6570\u3002 # C_mid num_depth_bins (int): \u6df1\u5ea6\u79bb\u6563 bin \u6570\u91cf D\u3002 # D use_bn (bool): \u662f\u5426\u5728\u7b2c\u4e00\u5c42\u5377\u79ef\u540e\u4f7f\u7528 BatchNorm2d\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528 BN \"\"\" super () . __init__ () # \u7236\u7c7b\u6784\u9020 layers = [ # \u7b2c\u4e00\u5c42 conv \u53ca\u53ef\u9009 BN+ReLU nn . Conv2d ( in_channels , mid_channels , kernel_size = 3 , padding = 1 , bias = not use_bn ) # 3\u00d73 Conv ] if use_bn : # \u662f\u5426\u52a0\u5165 BN layers . append ( nn . BatchNorm2d ( mid_channels )) # BN \u5c42 layers . append ( nn . ReLU ( inplace = True )) # ReLU \u6fc0\u6d3b self . stem = nn . Sequential ( * layers ) # \u5c01\u88c5\u4e3a\u987a\u5e8f\u6a21\u5757 self . depth_logits = nn . Conv2d ( mid_channels , num_depth_bins , kernel_size = 1 ) # 1\u00d71 Conv \u9884\u6d4b D \u901a\u9053 self . num_depth_bins = num_depth_bins # \u8bb0\u5f55\u6df1\u5ea6 bins \u6570","title":"__init__"},{"location":"camera_side/#camera_side.network.DepthDistributionHead.DepthDistributionHead.forward","text":"Parameters: x ( Tensor ) \u2013 \u8f93\u5165\u76f8\u673a\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: Tensor \u2013 torch.Tensor: \u6df1\u5ea6\u6982\u7387\u5206\u5e03 (B, D, H, W)\u3002 # \u8f93\u51fa\u5206\u5e03 Source code in camera_side\\network\\DepthDistributionHead.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x (torch.Tensor): \u8f93\u5165\u76f8\u673a\u7279\u5f81\uff0c\u5f62\u72b6 (B, C_in, H, W)\u3002 # \u8f93\u5165\u7279\u5f81 Returns: torch.Tensor: \u6df1\u5ea6\u6982\u7387\u5206\u5e03 (B, D, H, W)\u3002 # \u8f93\u51fa\u5206\u5e03 \"\"\" feat = self . stem ( x ) # \u63d0\u53d6\u4e2d\u95f4\u7279\u5f81 logits = self . depth_logits ( feat ) # \u751f\u6210\u6df1\u5ea6 logits # \u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u505a softmax\uff0c\u5f97\u5230\u6bcf\u4e2a\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03 prob = F . softmax ( logits , dim = 1 ) # \u6cbf\u901a\u9053\u7ef4\u5f52\u4e00\u5316 return prob # \u8fd4\u56de\u6df1\u5ea6\u5206\u5e03","title":"forward"},{"location":"camera_side/#camera_bev_pooling","text":"\u5c06\u591a\u89c6\u89d2 3D \u7279\u5f81\u6c60\u5316\u5230 BEV \u7f51\u683c\u4e2d\u3002","title":"camera_bev_pooling"},{"location":"camera_side/#camera_side.operator.camera_bev_pooling.camera_bev_pooling","text":"\u5c06\u76f8\u673a\u5206\u652f\u5728\u4f53\u7d20 / frustum \u7a7a\u95f4\u4e2d\u7684\u4e09\u7ef4\u7279\u5f81\u805a\u5408\u5230 BEV \u7f51\u683c\u4e0a\uff0c \u5f97\u5230\u5f62\u72b6\u4e3a (B, C, bev_h, bev_w) \u7684 Camera BEV \u7279\u5f81\u56fe\u3002 Parameters: feat_3d ( Tensor ) \u2013 (B, D, C, H, W) \u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\uff1a - B: batch size - D: \u6df1\u5ea6\u7ef4\u5ea6\u957f\u5ea6\uff08\u5982\u6df1\u5ea6 bins \u6216\u4f53\u7d20\u5c42\u6570\uff09 - C: \u901a\u9053\u6570 - H, W: \u56fe\u50cf\u5e73\u9762\u4e0a\u7684\u91c7\u6837\u7f51\u683c\u5c3a\u5bf8\u3002 bev_indices ( Tensor ) \u2013 (B, D, H, W, 2) \u6bcf\u4e2a\u4e09\u7ef4\u4f53\u7d20\u5728 BEV \u7f51\u683c\u4e0a\u7684\u6574\u6570\u7d22\u5f15 (i, j)\uff1a - i: BEV \u4e2d\u7684\u7eb5\u5411\u7d22\u5f15\uff0c\u8303\u56f4 [0, bev_h - 1] - j: BEV \u4e2d\u7684\u6a2a\u5411\u7d22\u5f15\uff0c\u8303\u56f4 [0, bev_w - 1] - \u82e5\u67d0\u4e2a\u4f4d\u7f6e\u4e3a\u65e0\u6548\u70b9\uff0c\u53ef\u5c06\u5bf9\u5e94\u7d22\u5f15\u7f6e\u4e3a\u8d1f\u6570\uff08\u5982 -1\uff09\u3002 bev_h ( int ) \u2013 int BEV \u7f51\u683c\u7684\u9ad8\u5ea6\uff08\u7eb5\u5411\u7f51\u683c\u6570\uff09\u3002 bev_w ( int ) \u2013 int BEV \u7f51\u683c\u7684\u5bbd\u5ea6\uff08\u6a2a\u5411\u7f51\u683c\u6570\uff09\u3002 Returns: bev_feat ( Tensor ) \u2013 (B, C, bev_h, bev_w) \u5728 BEV \u7f51\u683c\u4e0a\u805a\u5408\u540e\u7684\u76f8\u673a\u7279\u5f81\u56fe\uff0c\u9ed8\u8ba4\u4f7f\u7528\u6c42\u548c\u805a\u5408\uff08sum pooling\uff09\u3002 Source code in camera_side\\operator\\camera_bev_pooling.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 @torch . library . custom_op ( \"bevfusion_ops::camera_bev_pooling\" , mutates_args = [] ) def camera_bev_pooling ( feat_3d : torch . Tensor , bev_indices : torch . Tensor , bev_h : int , bev_w : int , ) -> torch . Tensor : \"\"\" \u5c06\u76f8\u673a\u5206\u652f\u5728\u4f53\u7d20 / frustum \u7a7a\u95f4\u4e2d\u7684\u4e09\u7ef4\u7279\u5f81\u805a\u5408\u5230 BEV \u7f51\u683c\u4e0a\uff0c \u5f97\u5230\u5f62\u72b6\u4e3a (B, C, bev_h, bev_w) \u7684 Camera BEV \u7279\u5f81\u56fe\u3002 Args: feat_3d: (B, D, C, H, W) \u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\uff1a - B: batch size - D: \u6df1\u5ea6\u7ef4\u5ea6\u957f\u5ea6\uff08\u5982\u6df1\u5ea6 bins \u6216\u4f53\u7d20\u5c42\u6570\uff09 - C: \u901a\u9053\u6570 - H, W: \u56fe\u50cf\u5e73\u9762\u4e0a\u7684\u91c7\u6837\u7f51\u683c\u5c3a\u5bf8\u3002 bev_indices: (B, D, H, W, 2) \u6bcf\u4e2a\u4e09\u7ef4\u4f53\u7d20\u5728 BEV \u7f51\u683c\u4e0a\u7684\u6574\u6570\u7d22\u5f15 (i, j)\uff1a - i: BEV \u4e2d\u7684\u7eb5\u5411\u7d22\u5f15\uff0c\u8303\u56f4 [0, bev_h - 1] - j: BEV \u4e2d\u7684\u6a2a\u5411\u7d22\u5f15\uff0c\u8303\u56f4 [0, bev_w - 1] - \u82e5\u67d0\u4e2a\u4f4d\u7f6e\u4e3a\u65e0\u6548\u70b9\uff0c\u53ef\u5c06\u5bf9\u5e94\u7d22\u5f15\u7f6e\u4e3a\u8d1f\u6570\uff08\u5982 -1\uff09\u3002 bev_h: int BEV \u7f51\u683c\u7684\u9ad8\u5ea6\uff08\u7eb5\u5411\u7f51\u683c\u6570\uff09\u3002 bev_w: int BEV \u7f51\u683c\u7684\u5bbd\u5ea6\uff08\u6a2a\u5411\u7f51\u683c\u6570\uff09\u3002 Returns: bev_feat: (B, C, bev_h, bev_w) \u5728 BEV \u7f51\u683c\u4e0a\u805a\u5408\u540e\u7684\u76f8\u673a\u7279\u5f81\u56fe\uff0c\u9ed8\u8ba4\u4f7f\u7528\u6c42\u548c\u805a\u5408\uff08sum pooling\uff09\u3002 \"\"\" assert feat_3d . dim () == 5 , \"feat_3d \u5fc5\u987b\u662f (B, D, C, H, W) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 assert bev_indices . dim () == 5 , \"bev_indices \u5fc5\u987b\u662f (B, D, H, W, 2) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 B , D , C , H , W = feat_3d . shape # \u8bfb\u53d6\u8f93\u5165\u5c3a\u5bf8 assert bev_indices . shape [ 0 ] == B and bev_indices . shape [ 1 ] == D , \"batch / depth \u7ef4\u5ea6\u4e0d\u5339\u914d\" # \u68c0\u67e5 B,D assert bev_indices . shape [ 2 ] == H and bev_indices . shape [ 3 ] == W , \"\u7a7a\u95f4\u5c3a\u5bf8 (H,W) \u4e0d\u5339\u914d\" # \u68c0\u67e5 H,W device = feat_3d . device # \u83b7\u53d6\u8bbe\u5907 bev_feat = torch . zeros ( B , C , bev_h , bev_w , device = device ) # \u521d\u59cb\u5316\u8f93\u51fa BEV \u7279\u5f81 # \u5c06\u4e09\u7ef4\u4f53\u7d20\u5c55\u5e73\u5230\u4e00\u4e2a\u5217\u8868\uff0c\u6309 batch \u5206\u522b\u505a scatter-add # feat_3d: (B, D, C, H, W) \u2192 (B, N, C)\uff0c\u5176\u4e2d N = D*H*W N = D * H * W # \u4e09\u7ef4\u4f4d\u7f6e\u6570\u91cf feat_flat = feat_3d . permute ( 0 , 1 , 3 , 4 , 2 ) . reshape ( B , N , C ) # (B,N,C) idx_flat = bev_indices . reshape ( B , N , 2 ) # (B,N,2) for b in range ( B ): # \u904d\u5386\u6bcf\u4e2a batch feat_b = feat_flat [ b ] # (N,C) idx_b = idx_flat [ b ] # (N,2) i = idx_b [:, 0 ] # \u7eb5\u5411\u7d22\u5f15 j = idx_b [:, 1 ] # \u6a2a\u5411\u7d22\u5f15 # \u6709\u6548\u4f4d\u7f6e\u63a9\u7801\uff1a\u7d22\u5f15\u5728\u5408\u6cd5\u8303\u56f4\u5185 valid_mask = ( i >= 0 ) & ( j >= 0 ) & ( i < bev_h ) & ( j < bev_w ) # \u8fc7\u6ee4\u65e0\u6548\u70b9 if not torch . any ( valid_mask ): # \u82e5\u5168\u90e8\u65e0\u6548\u5219\u8df3\u8fc7 continue i_valid = i [ valid_mask ] # \u6709\u6548 i j_valid = j [ valid_mask ] # \u6709\u6548 j feat_valid = feat_b [ valid_mask ] # \u6709\u6548\u7279\u5f81 (M,C) linear_idx = ( i_valid * bev_w + j_valid ) . long () # \u6620\u5c04\u5230\u4e00\u7ef4\u7d22\u5f15 (M,) # \u5c06\u7279\u5f81\u7d2f\u52a0\u5230 BEV \u7f51\u683c\u4e0a\uff1a\u5148\u5c55\u5e73\u4e3a (C, bev_h*bev_w)\uff0c\u5728\u5217\u7ef4\u5ea6\u505a index_add_ bev_b_flat = bev_feat [ b ] . reshape ( C , bev_h * bev_w ) # (C, bev_h*bev_w) bev_b_flat . index_add_ ( # \u6309\u5217\u7d2f\u52a0\u7279\u5f81 dim = 1 , index = linear_idx , source = feat_valid . T , # (C,M) ) return bev_feat # \u8fd4\u56de\u805a\u5408\u540e\u7684 BEV \u7279\u5f81","title":"camera_bev_pooling"},{"location":"camera_side/#camera_geometry_transform","text":"\u901a\u8fc7\u76f8\u673a\u5916\u53c2 + \u5185\u53c2\u5c06 2D \u7279\u5f81\u6295\u5f71\u5230 3D \u5750\u6807\u3002","title":"camera_geometry_transform"},{"location":"camera_side/#camera_side.operator.camera_geometry_transform.camera_geometry_transform","text":"\u5c06\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\u901a\u8fc7\u7ed9\u5b9a\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb\uff0c \u5e76\u8fd4\u56de\u5f62\u72b6\u4fdd\u6301\u4e3a (D, H, W, 3) \u7684\u4e09\u7ef4\u70b9\u4e91\u5f20\u91cf\u3002 Parameters: points_3d ( Tensor ) \u2013 (D, H, W, 3) \u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u901a\u5e38\u7531\u50cf\u7d20\u5347\u7ef4\u7b97\u5b50\u5f97\u5230\uff0c\u683c\u5f0f\u4e3a (x, y, z)\u3002 R ( Tensor ) \u2013 (3, 3) \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u3002 t ( Tensor ) \u2013 (3,) \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u3002 Returns: transformed_points ( Tensor ) \u2013 (D, H, W, 3) \u76ee\u6807\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u5f62\u72b6\u4e0e\u8f93\u5165 points_3d \u76f8\u540c\u3002 Source code in camera_side\\operator\\camera_geometry_transform.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @torch . library . custom_op ( \"bevfusion_ops::camera_geometry_transform\" , mutates_args = []) def camera_geometry_transform ( points_3d : torch . Tensor , R : torch . Tensor , t : torch . Tensor , ) -> torch . Tensor : \"\"\" \u5c06\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\u901a\u8fc7\u7ed9\u5b9a\u7684\u65cb\u8f6c\u548c\u5e73\u79fb\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb\uff0c \u5e76\u8fd4\u56de\u5f62\u72b6\u4fdd\u6301\u4e3a (D, H, W, 3) \u7684\u4e09\u7ef4\u70b9\u4e91\u5f20\u91cf\u3002 Args: points_3d: (D, H, W, 3) \u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u901a\u5e38\u7531\u50cf\u7d20\u5347\u7ef4\u7b97\u5b50\u5f97\u5230\uff0c\u683c\u5f0f\u4e3a (x, y, z)\u3002 R: (3, 3) \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u3002 t: (3,) \u4ece\u76f8\u673a\u5750\u6807\u7cfb\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u3002 Returns: transformed_points: (D, H, W, 3) \u76ee\u6807\u5750\u6807\u7cfb\u4e0b\u7684\u4e09\u7ef4\u70b9\u4e91\uff0c\u5f62\u72b6\u4e0e\u8f93\u5165 points_3d \u76f8\u540c\u3002 \"\"\" D , H , W , _ = points_3d . shape # \u8bfb\u53d6\u4f53\u7d20\u7ef4\u5ea6\u4fe1\u606f pts_flat = points_3d . reshape ( - 1 , 3 ) # \u5c55\u5e73\u6210 (D*H*W, 3) pts_trans = ( pts_flat @ R . T ) + t . view ( 1 , 3 ) # \u5e94\u7528\u65cb\u8f6c\u548c\u5e73\u79fb\u53d8\u6362 transformed_points = pts_trans . reshape ( D , H , W , 3 ) # \u8fd8\u539f\u4e3a (D,H,W,3) return transformed_points # \u8fd4\u56de\u53d8\u6362\u540e\u7684\u70b9\u4e91","title":"camera_geometry_transform"},{"location":"camera_side/#camera_lift_2d_to_3d","text":"\u6839\u636e\u6df1\u5ea6\u79bb\u6563\u5316\uff08Depth Bins\uff09\u5c06 2D \u7279\u5f81 Lift \u5230 3D Voxels\u3002","title":"camera_lift_2d_to_3d"},{"location":"camera_side/#camera_side.operator.camera_lift_2d_to_3d.camera_lift_2d_to_3d","text":"\u8ba1\u7b97 2D \u50cf\u7d20\u5750\u6807\u5728\u7ed9\u5b9a\u79bb\u6563\u6df1\u5ea6\u4e0b\u5bf9\u5e94\u7684 3D \u7a7a\u95f4\u5750\u6807\uff0c\u5e76\u8fd4\u56de\u5f62\u72b6\u4e3a (D, H, W, 3) \u7684\u4e09\u7ef4\u70b9\u4e91\u5f20\u91cf\u3002 Parameters: pixels_2d ( Tensor ) \u2013 (H, W, 2) \u50cf\u7d20\u5750\u6807\u7f51\u683c\uff0c\u683c\u5f0f\u4e3a (u, v)\uff0c\u8868\u793a\u56fe\u50cf\u5e73\u9762\u4e0a\u7684\u4e8c\u7ef4\u50cf\u7d20\u5750\u6807\u3002 depths ( Tensor ) \u2013 (D,) \u79bb\u6563\u6df1\u5ea6\u53d6\u6837\u503c\uff0c\u4e00\u7ef4\u6df1\u5ea6 bin \u5411\u91cf\u3002 K ( Tensor ) \u2013 (3, 3) \u76f8\u673a\u5185\u53c2\u77e9\u9635\u3002 R ( Tensor ) \u2013 (3, 3) \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u3002 t ( Tensor ) \u2013 (3,) \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u3002 Returns: points_3d ( Tensor ) \u2013 (D, H, W, 3) \u6bcf\u4e2a\u50cf\u7d20\u5728\u6bcf\u4e2a\u6df1\u5ea6 bin \u4e0b\u5bf9\u5e94\u7684\u4e09\u7ef4\u7a7a\u95f4\u5750\u6807 (x, y, z)\u3002 Source code in camera_side\\operator\\camera_lift_2d_to_3d.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 @torch . library . custom_op ( \"bevfusion_ops::camera_lift_2d_to_3d\" , mutates_args = []) def camera_lift_2d_to_3d ( pixels_2d : torch . Tensor , depths : torch . Tensor , K : torch . Tensor , R : torch . Tensor , t : torch . Tensor , ) -> torch . Tensor : \"\"\" \u8ba1\u7b97 2D \u50cf\u7d20\u5750\u6807\u5728\u7ed9\u5b9a\u79bb\u6563\u6df1\u5ea6\u4e0b\u5bf9\u5e94\u7684 3D \u7a7a\u95f4\u5750\u6807\uff0c\u5e76\u8fd4\u56de\u5f62\u72b6\u4e3a (D, H, W, 3) \u7684\u4e09\u7ef4\u70b9\u4e91\u5f20\u91cf\u3002 Args: pixels_2d: (H, W, 2) \u50cf\u7d20\u5750\u6807\u7f51\u683c\uff0c\u683c\u5f0f\u4e3a (u, v)\uff0c\u8868\u793a\u56fe\u50cf\u5e73\u9762\u4e0a\u7684\u4e8c\u7ef4\u50cf\u7d20\u5750\u6807\u3002 depths: (D,) \u79bb\u6563\u6df1\u5ea6\u53d6\u6837\u503c\uff0c\u4e00\u7ef4\u6df1\u5ea6 bin \u5411\u91cf\u3002 K: (3, 3) \u76f8\u673a\u5185\u53c2\u77e9\u9635\u3002 R: (3, 3) \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u65cb\u8f6c\u77e9\u9635\u3002 t: (3,) \u76f8\u673a\u5230\u76ee\u6807\u5750\u6807\u7cfb\u7684\u5e73\u79fb\u5411\u91cf\u3002 Returns: points_3d: (D, H, W, 3) \u6bcf\u4e2a\u50cf\u7d20\u5728\u6bcf\u4e2a\u6df1\u5ea6 bin \u4e0b\u5bf9\u5e94\u7684\u4e09\u7ef4\u7a7a\u95f4\u5750\u6807 (x, y, z)\u3002 \"\"\" H , W , _ = pixels_2d . shape # \u56fe\u50cf\u9ad8\u5bbd D = depths . shape [ 0 ] # \u6df1\u5ea6 bin \u6570 u = pixels_2d [ ... , 0 ] . reshape ( 1 , H , W ) # \u63d0\u53d6 u \u5750\u6807 v = pixels_2d [ ... , 1 ] . reshape ( 1 , H , W ) # \u63d0\u53d6 v \u5750\u6807 z = depths . view ( D , 1 , 1 ) # \u6df1\u5ea6\u6269\u5c55\u5230 (D,1,1) # \u7531\u9488\u5b54\u6210\u50cf\u6a21\u578b\u53cd\u7b97\u76f8\u673a\u5750\u6807\u7cfb\u4e0b\u7684 (x, y, z) x = ( u - K [ 0 , 2 ]) * z / K [ 0 , 0 ] # \u53cd\u6295\u5f71\u5f97\u5230 x y = ( v - K [ 1 , 2 ]) * z / K [ 1 , 1 ] # \u53cd\u6295\u5f71\u5f97\u5230 y z_expanded = z . expand_as ( x ) # \u6269\u5c55 z \u4ee5\u4fbf\u4e0e x,y \u5bf9\u9f50 pts_cam = torch . stack ([ x , y , z_expanded ], dim =- 1 ) # \u76f8\u673a\u7cfb 3D \u70b9 (D,H,W,3) pts_flat = pts_cam . reshape ( - 1 , 3 ) # \u5c55\u5e73\u4e3a (D*H*W, 3) pts_world = ( pts_flat @ R . T ) + t . view ( 1 , 3 ) # \u5750\u6807\u53d8\u6362\u5230\u76ee\u6807\u5750\u6807\u7cfb pts_world = pts_world . reshape ( D , H , W , 3 ) # \u6062\u590d\u5f62\u72b6\u4e3a (D,H,W,3) return pts_world # \u8fd4\u56de\u4e09\u7ef4\u70b9\u4e91","title":"camera_lift_2d_to_3d"},{"location":"camera_side/#depth_weighted_feature_expand","text":"\u5229\u7528\u6df1\u5ea6\u5206\u5e03\u5bf9\u56fe\u50cf\u7279\u5f81\u8fdb\u884c\u52a0\u6743\u6269\u5c55\uff0c\u589e\u5f3a BEV \u6295\u5f71\u8d28\u91cf\u3002","title":"depth_weighted_feature_expand"},{"location":"camera_side/#camera_side.operator.depth_weighted_feature_expand.depth_weighted_feature_expand","text":"\u6839\u636e\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u5bf9\u76f8\u673a 2D \u7279\u5f81\u8fdb\u884c\u52a0\u6743\u5e76\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u5c55\u5f00\uff0c \u5f97\u5230\u5f62\u72b6\u4e3a (B, D, C, H, W) \u7684\u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\u3002 Parameters: img_feats ( Tensor ) \u2013 (B, C, H, W) \u76f8\u673a\u5206\u652f\u8f93\u51fa\u7684 2D \u7279\u5f81\u56fe\uff1a - B: batch size - C: \u901a\u9053\u6570 - H, W: \u7279\u5f81\u56fe\u7a7a\u95f4\u5c3a\u5bf8\u3002 depth_prob ( Tensor ) \u2013 (B, D, H, W) \u5bf9\u5e94\u50cf\u7d20\u7684\u79bb\u6563\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff1a - D: \u6df1\u5ea6 bin \u6570\u91cf - \u4e0e img_feats \u5728 (B, H, W) \u7ef4\u5ea6\u4e0a\u5bf9\u9f50\u3002 Returns: feat_3d ( Tensor ) \u2013 (B, D, C, H, W) \u6309\u6df1\u5ea6\u6982\u7387\u52a0\u6743\u5e76\u5728\u6df1\u5ea6\u7ef4\u5ea6\u5c55\u5f00\u540e\u7684\u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\uff0c \u6bcf\u4e2a\u4f53\u7d20\u4f4d\u7f6e\u540c\u65f6\u7f16\u7801\u4e86\u50cf\u7d20\u8bed\u4e49\u4e0e\u5bf9\u5e94\u6df1\u5ea6\u6743\u91cd\u3002 Source code in camera_side\\operator\\depth_weighted_feature_expand.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 @torch . library . custom_op ( \"bevfusion_ops::depth_weighted_feature_expand\" , mutates_args = [] ) def depth_weighted_feature_expand ( img_feats : torch . Tensor , depth_prob : torch . Tensor , ) -> torch . Tensor : \"\"\" \u6839\u636e\u50cf\u7d20\u7684\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff0c\u5bf9\u76f8\u673a 2D \u7279\u5f81\u8fdb\u884c\u52a0\u6743\u5e76\u5728\u6df1\u5ea6\u7ef4\u5ea6\u4e0a\u5c55\u5f00\uff0c \u5f97\u5230\u5f62\u72b6\u4e3a (B, D, C, H, W) \u7684\u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\u3002 Args: img_feats: (B, C, H, W) \u76f8\u673a\u5206\u652f\u8f93\u51fa\u7684 2D \u7279\u5f81\u56fe\uff1a - B: batch size - C: \u901a\u9053\u6570 - H, W: \u7279\u5f81\u56fe\u7a7a\u95f4\u5c3a\u5bf8\u3002 depth_prob: (B, D, H, W) \u5bf9\u5e94\u50cf\u7d20\u7684\u79bb\u6563\u6df1\u5ea6\u6982\u7387\u5206\u5e03\uff1a - D: \u6df1\u5ea6 bin \u6570\u91cf - \u4e0e img_feats \u5728 (B, H, W) \u7ef4\u5ea6\u4e0a\u5bf9\u9f50\u3002 Returns: feat_3d: (B, D, C, H, W) \u6309\u6df1\u5ea6\u6982\u7387\u52a0\u6743\u5e76\u5728\u6df1\u5ea6\u7ef4\u5ea6\u5c55\u5f00\u540e\u7684\u4e09\u7ef4\u7279\u5f81\u5f20\u91cf\uff0c \u6bcf\u4e2a\u4f53\u7d20\u4f4d\u7f6e\u540c\u65f6\u7f16\u7801\u4e86\u50cf\u7d20\u8bed\u4e49\u4e0e\u5bf9\u5e94\u6df1\u5ea6\u6743\u91cd\u3002 \"\"\" assert img_feats . dim () == 4 , \"img_feats \u5fc5\u987b\u662f (B, C, H, W) \u5f20\u91cf\" # \u7b80\u5355\u5f62\u72b6\u68c0\u67e5 assert depth_prob . dim () == 4 , \"depth_prob \u5fc5\u987b\u662f (B, D, H, W) \u5f20\u91cf\" # \u7b80\u5355\u5f62\u72b6\u68c0\u67e5 assert img_feats . shape [ 0 ] == depth_prob . shape [ 0 ], \"batch \u7ef4\u5ea6\u4e0d\u5339\u914d\" # \u68c0\u67e5 B assert img_feats . shape [ 2 :] == depth_prob . shape [ 2 :], \"\u7a7a\u95f4\u5c3a\u5bf8 (H,W) \u4e0d\u5339\u914d\" # \u68c0\u67e5 H,W # \u6269\u5c55\u7ef4\u5ea6\u4ee5\u4fbf\u5e7f\u64ad\u76f8\u4e58\uff1a # img_feats: (B, C, H, W) \u2192 (B, 1, C, H, W) # depth_prob: (B, D, H, W) \u2192 (B, D, 1, H, W) img_feats_expanded = img_feats . unsqueeze ( 1 ) # \u5728\u6df1\u5ea6\u4f4d\u7f6e\u63d2\u5165\u7ef4\u5ea6 depth_prob_expanded = depth_prob . unsqueeze ( 2 ) # \u5728\u901a\u9053\u4f4d\u7f6e\u63d2\u5165\u7ef4\u5ea6 feat_3d = img_feats_expanded * depth_prob_expanded # \u6309\u6df1\u5ea6\u6982\u7387\u52a0\u6743\u5c55\u5f00 return feat_3d # (B, D, C, H, W)","title":"depth_weighted_feature_expand"},{"location":"lidar_side/","text":"Lidar Side\uff08\u70b9\u4e91\u4fa7\u6a21\u5757\uff09 \u00b6 Lidar Side \u6a21\u5757\u7684\u4f5c\u7528\u662f\uff1a \u5c06\u539f\u59cb LiDAR \u70b9\u4e91\u8fdb\u884c\u4f53\u7d20\u5316\u3001\u7279\u5f81\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u7f51\u7edc\u63d0\u53d6 3D \u8bed\u4e49\u8868\u793a\uff1b \u968f\u540e\u5c06 3D \u7279\u5f81\u538b\u7f29\u81f3 BEV \u7a7a\u95f4\uff0c\u4e3a\u540e\u7eed BEV \u878d\u5408\u63d0\u4f9b\u70b9\u4e91 BEV \u8868\u8fbe\u3002 \u672c\u6a21\u5757\u5305\u542b\u56db\u7c7b\u6838\u5fc3\u529f\u80fd\uff1a \u70b9\u4e91\u4f53\u7d20\u5316\uff08Voxelization\uff09 \u5c06\u7a00\u758f\u70b9\u4e91\u8f6c\u6362\u4e3a\u89c4\u5219\u4f53\u7d20\u7ed3\u6784\uff0c\u4fbf\u4e8e\u540e\u7eed\u7f51\u7edc\u5904\u7406\u3002 \u4f53\u7d20\u7279\u5f81\u7f16\u7801\uff08Voxel Feature Encoding\uff09 \u5bf9\u6bcf\u4e2a\u4f53\u7d20\u5185\u90e8\u7684\u539f\u59cb\u70b9\u7279\u5f81\u8fdb\u884c\u7f16\u7801\uff0c\u5f97\u5230\u56fa\u5b9a\u7ef4\u5ea6\u8868\u793a\u3002 \u7a00\u758f\u5377\u79ef 3D Backbone\uff08Sparse 3D CNN\uff09 \u4f7f\u7528\u7a00\u758f\u5377\u79ef\u63d0\u53d6 3D \u8bed\u4e49\u7279\u5f81\u3002 \u70b9\u4e91 BEV \u538b\u7f29\u7b97\u5b50 \u5c06 3D \u7279\u5f81\u6cbf Z \u7ef4\u538b\u7f29\u6210 BEV \u7279\u5f81\u5e73\u9762\u3002 API \u6587\u6863 \u00b6 \u4ee5\u4e0b\u5217\u51fa Lidar Side \u4e2d\u5168\u90e8\u7b97\u5b50\u4e0e\u7f51\u7edc\u6a21\u5757\uff0c\u5e76\u7ed9\u51fa\u5bf9\u5e94\u6587\u4ef6\u8def\u5f84\u3002 'LidarVoxelization' \u00b6 \u5c06\u539f\u59cb\u70b9\u4e91\u8f6c\u6362\u4e3a\u7a00\u758f\u4f53\u7d20\u7ed3\u6784\u3002 LidarVoxelization \u00b6 Bases: Module LidarVoxelization \u6a21\u5757\uff08\u70b9\u4e91\u4f53\u7d20\u5316\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u5c06\u8fde\u7eed\u7684 3D \u70b9\u4e91\u5750\u6807\u6309\u7167\u7ed9\u5b9a\u7684 voxel_size \u4e0e point_cloud_range \u91cf\u5316\u5230\u89c4\u5219 3D \u4f53\u7d20\u7f51\u683c\u4e2d\uff0c\u5e76\u8f93\u51fa\u4f53\u7d20\u4e2d\u7684\u70b9\u96c6\u5408\u3001\u4f53\u7d20\u5750\u6807\u4ee5\u53ca\u6bcf\u4e2a\u4f53\u7d20\u7684\u70b9\u6570\u3002 \u672c\u5b9e\u73b0\u4e3a\u7b80\u5316\u7248 CPU \u4f53\u7d20\u5316\u903b\u8f91\uff0c\u4fbf\u4e8e\u9605\u8bfb\u4e0e\u7b97\u5b50\u7ea7\u91cd\u6784\uff0c\u800c\u975e\u9ad8\u6027\u80fd\u5b9e\u73b0\u3002 \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 \u00b6 \u6839\u636e point_cloud_range \u4e0e voxel_size \u8ba1\u7b97 3D \u7f51\u683c\u5927\u5c0f (nx, ny, nz) \u904d\u5386 batch \u4e2d\u6bcf\u4e2a\u70b9\uff0c\u8ba1\u7b97\u5176\u6240\u5728\u4f53\u7d20\u7d22\u5f15 (ix, iy, iz) \u4e3a\u6bcf\u4e2a\u65b0\u4f53\u7d20\u5206\u914d\u4e00\u4e2a voxel \u7f16\u53f7\uff0c\u6700\u591a\u4fdd\u7559 max_voxels \u4e2a\u4f53\u7d20 \u5728\u4f53\u7d20\u5185\u90e8\u7d2f\u79ef\u70b9\uff0c\u6700\u591a\u4fdd\u7559 max_points_per_voxel \u4e2a\u70b9 \u6700\u7ec8\u8f93\u51fa\uff1a voxels: (M, T, C) coords: (M, 4) [batch_idx, z, y, x] num_points: (M,) \u8f93\u5165 \u00b6 Args: points: (B, N, C) \u6279\u91cf\u70b9\u4e91\u6570\u636e\uff1a - B: batch size - N: \u6bcf\u5e27\u70b9\u7684\u6700\u5927\u6570\u91cf - C: \u70b9\u7279\u5f81\u7ef4\u5ea6\uff08\u81f3\u5c11\u5305\u542b x,y,z\uff0c\u901a\u5e38\u4e3a 4: [x,y,z,intensity]\uff09\u3002 voxel_size (tuple[float, float, float]): \u4f53\u7d20\u5927\u5c0f (vx, vy, vz)\uff0c\u5355\u4f4d\u4e0e\u70b9\u4e91\u5750\u6807\u4e00\u81f4\u3002 point_cloud_range (tuple[float, float, float, float, float, float]): \u70b9\u4e91\u8303\u56f4 [x_min, y_min, z_min, x_max, y_max, z_max]\u3002 max_points_per_voxel (int): \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u5141\u8bb8\u4fdd\u5b58\u7684\u6700\u5927\u70b9\u6570 T\u3002 max_voxels (int): \u6bcf\u5e27\u6700\u591a\u4fdd\u7559\u7684\u4f53\u7d20\u6570\u91cf M_max\u3002 \u8f93\u51fa \u00b6 Returns: voxels: (M, T, C) \u4f53\u7d20\u5185\u70b9\u7279\u5f81\u5f20\u91cf\uff1a - M: \u5b9e\u9645\u751f\u6210\u7684\u4f53\u7d20\u6570\u91cf\uff08M \u2264 B * max_voxels\uff09 - T: \u6bcf\u4e2a\u4f53\u7d20\u7684\u6700\u5927\u70b9\u6570\uff0c\u591a\u4f59\u7684\u70b9\u4f1a\u88ab\u4e22\u5f03\uff0c\u672a\u586b\u6ee1\u4f4d\u7f6e\u4e3a 0\u3002 coords: (M, 4) \u6bcf\u4e2a\u4f53\u7d20\u5728 3D \u7f51\u683c\u4e2d\u7684\u6574\u6570\u5750\u6807 [batch_idx, z, y, x]\u3002 num_points_per_voxel: (M,) \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u771f\u5b9e\u70b9\u6570\uff0c\u7528\u4e8e\u533a\u5206\u6709\u6548\u70b9\u4e0e padding\u3002 \u6a21\u5757\u7528\u9014 \u00b6 \u5c06\u7a20\u5bc6\u70b9\u4e91\u8f6c\u6362\u4e3a\u89c4\u5219\u4f53\u7d20\u8868\u793a\uff0c\u4f5c\u4e3a\u540e\u7eed VFE \u4e0e Sparse 3D Backbone \u7684\u8f93\u5165 \u4f5c\u4e3a\u94fe\u8def \u201cLiDAR \u539f\u59cb\u70b9\u4e91 \u2192 LiDAR BEV \u7279\u5f81\u201d \u4e2d\u7684\u7b2c\u4e00\u4e2a\u79bb\u6563\u5316\u7b97\u5b50 \u5728\u7b97\u5b50\u6846\u67b6\u4e2d\u7edf\u4e00\u5c01\u88c5\u4f53\u7d20\u5316\u903b\u8f91\uff0c\u4fbf\u4e8e\u66ff\u6362\u4e3a\u66f4\u9ad8\u6027\u80fd\u7684 C++/CUDA \u5b9e\u73b0 Source code in lidar_side\\network\\LidarVoxelization.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class LidarVoxelization ( nn . Module ): \"\"\" LidarVoxelization \u6a21\u5757\uff08\u70b9\u4e91\u4f53\u7d20\u5316\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u5c06\u8fde\u7eed\u7684 3D \u70b9\u4e91\u5750\u6807\u6309\u7167\u7ed9\u5b9a\u7684 voxel_size \u4e0e point_cloud_range \u91cf\u5316\u5230\u89c4\u5219 3D \u4f53\u7d20\u7f51\u683c\u4e2d\uff0c\u5e76\u8f93\u51fa\u4f53\u7d20\u4e2d\u7684\u70b9\u96c6\u5408\u3001\u4f53\u7d20\u5750\u6807\u4ee5\u53ca\u6bcf\u4e2a\u4f53\u7d20\u7684\u70b9\u6570\u3002 \u672c\u5b9e\u73b0\u4e3a\u7b80\u5316\u7248 CPU \u4f53\u7d20\u5316\u903b\u8f91\uff0c\u4fbf\u4e8e\u9605\u8bfb\u4e0e\u7b97\u5b50\u7ea7\u91cd\u6784\uff0c\u800c\u975e\u9ad8\u6027\u80fd\u5b9e\u73b0\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. \u6839\u636e point_cloud_range \u4e0e voxel_size \u8ba1\u7b97 3D \u7f51\u683c\u5927\u5c0f (nx, ny, nz) 2. \u904d\u5386 batch \u4e2d\u6bcf\u4e2a\u70b9\uff0c\u8ba1\u7b97\u5176\u6240\u5728\u4f53\u7d20\u7d22\u5f15 (ix, iy, iz) 3. \u4e3a\u6bcf\u4e2a\u65b0\u4f53\u7d20\u5206\u914d\u4e00\u4e2a voxel \u7f16\u53f7\uff0c\u6700\u591a\u4fdd\u7559 max_voxels \u4e2a\u4f53\u7d20 4. \u5728\u4f53\u7d20\u5185\u90e8\u7d2f\u79ef\u70b9\uff0c\u6700\u591a\u4fdd\u7559 max_points_per_voxel \u4e2a\u70b9 5. \u6700\u7ec8\u8f93\u51fa\uff1a - voxels: (M, T, C) - coords: (M, 4) [batch_idx, z, y, x] - num_points: (M,) --- ### \u8f93\u5165 Args: points: (B, N, C) \u6279\u91cf\u70b9\u4e91\u6570\u636e\uff1a - B: batch size - N: \u6bcf\u5e27\u70b9\u7684\u6700\u5927\u6570\u91cf - C: \u70b9\u7279\u5f81\u7ef4\u5ea6\uff08\u81f3\u5c11\u5305\u542b x,y,z\uff0c\u901a\u5e38\u4e3a 4: [x,y,z,intensity]\uff09\u3002 voxel_size (tuple[float, float, float]): \u4f53\u7d20\u5927\u5c0f (vx, vy, vz)\uff0c\u5355\u4f4d\u4e0e\u70b9\u4e91\u5750\u6807\u4e00\u81f4\u3002 point_cloud_range (tuple[float, float, float, float, float, float]): \u70b9\u4e91\u8303\u56f4 [x_min, y_min, z_min, x_max, y_max, z_max]\u3002 max_points_per_voxel (int): \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u5141\u8bb8\u4fdd\u5b58\u7684\u6700\u5927\u70b9\u6570 T\u3002 max_voxels (int): \u6bcf\u5e27\u6700\u591a\u4fdd\u7559\u7684\u4f53\u7d20\u6570\u91cf M_max\u3002 --- ### \u8f93\u51fa Returns: voxels: (M, T, C) \u4f53\u7d20\u5185\u70b9\u7279\u5f81\u5f20\u91cf\uff1a - M: \u5b9e\u9645\u751f\u6210\u7684\u4f53\u7d20\u6570\u91cf\uff08M \u2264 B * max_voxels\uff09 - T: \u6bcf\u4e2a\u4f53\u7d20\u7684\u6700\u5927\u70b9\u6570\uff0c\u591a\u4f59\u7684\u70b9\u4f1a\u88ab\u4e22\u5f03\uff0c\u672a\u586b\u6ee1\u4f4d\u7f6e\u4e3a 0\u3002 coords: (M, 4) \u6bcf\u4e2a\u4f53\u7d20\u5728 3D \u7f51\u683c\u4e2d\u7684\u6574\u6570\u5750\u6807 [batch_idx, z, y, x]\u3002 num_points_per_voxel: (M,) \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u771f\u5b9e\u70b9\u6570\uff0c\u7528\u4e8e\u533a\u5206\u6709\u6548\u70b9\u4e0e padding\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u5c06\u7a20\u5bc6\u70b9\u4e91\u8f6c\u6362\u4e3a\u89c4\u5219\u4f53\u7d20\u8868\u793a\uff0c\u4f5c\u4e3a\u540e\u7eed VFE \u4e0e Sparse 3D Backbone \u7684\u8f93\u5165 - \u4f5c\u4e3a\u94fe\u8def \u201cLiDAR \u539f\u59cb\u70b9\u4e91 \u2192 LiDAR BEV \u7279\u5f81\u201d \u4e2d\u7684\u7b2c\u4e00\u4e2a\u79bb\u6563\u5316\u7b97\u5b50 - \u5728\u7b97\u5b50\u6846\u67b6\u4e2d\u7edf\u4e00\u5c01\u88c5\u4f53\u7d20\u5316\u903b\u8f91\uff0c\u4fbf\u4e8e\u66ff\u6362\u4e3a\u66f4\u9ad8\u6027\u80fd\u7684 C++/CUDA \u5b9e\u73b0 \"\"\" def __init__ ( self , voxel_size : tuple , point_cloud_range : tuple , max_points_per_voxel : int = 32 , max_voxels : int = 20000 , ): super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . voxel_size = voxel_size # \u8bb0\u5f55\u4f53\u7d20\u5927\u5c0f self . point_cloud_range = point_cloud_range # \u8bb0\u5f55\u70b9\u4e91\u8303\u56f4 self . max_points_per_voxel = max_points_per_voxel # \u6bcf\u4e2a\u4f53\u7d20\u6700\u5927\u70b9\u6570 self . max_voxels = max_voxels # \u6bcf\u5e27\u6700\u5927\u4f53\u7d20\u6570 # \u9884\u8ba1\u7b97\u7f51\u683c\u5927\u5c0f (nx, ny, nz) x_min , y_min , z_min , x_max , y_max , z_max = point_cloud_range # \u89e3\u5305\u8303\u56f4 vx , vy , vz = voxel_size # \u89e3\u5305\u4f53\u7d20\u5c3a\u5bf8 nx = int (( x_max - x_min ) / vx + 1e-6 ) # x \u65b9\u5411\u4f53\u7d20\u6570 ny = int (( y_max - y_min ) / vy + 1e-6 ) # y \u65b9\u5411\u4f53\u7d20\u6570 nz = int (( z_max - z_min ) / vz + 1e-6 ) # z \u65b9\u5411\u4f53\u7d20\u6570 self . grid_size = ( nx , ny , nz ) # \u4fdd\u5b58\u7f51\u683c\u5c3a\u5bf8 def forward ( self , points : torch . Tensor ): \"\"\" Args: points (torch.Tensor): \u6279\u91cf\u70b9\u4e91\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a (B, N, C)\u3002 # \u8f93\u5165\u70b9\u4e91 Returns: voxels (torch.Tensor): \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C)\u3002 # \u8f93\u51fa\u4f53\u7d20 coords (torch.Tensor): \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\u3002 # [batch, z, y, x] num_points_per_voxel (torch.Tensor): \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u70b9\u6570\u7edf\u8ba1 \"\"\" assert points . dim () == 3 , \"points \u5fc5\u987b\u662f (B, N, C) \u5f20\u91cf\" # \u57fa\u672c\u68c0\u67e5 B , N , C = points . shape # \u8bfb\u53d6\u7ef4\u5ea6 device = points . device # \u83b7\u53d6\u8bbe\u5907 vx , vy , vz = self . voxel_size # \u4f53\u7d20\u5927\u5c0f x_min , y_min , z_min , x_max , y_max , z_max = self . point_cloud_range # \u70b9\u4e91\u8303\u56f4 nx , ny , nz = self . grid_size # \u7f51\u683c\u5927\u5c0f # \u6700\u5927\u4f53\u7d20\u603b\u6570\uff1a\u6bcf\u5e27 max_voxels\uff0c\u5171 B \u5e27 max_total_voxels = B * self . max_voxels # \u4f53\u7d20\u603b\u4e0a\u9650 # \u9884\u5206\u914d\u8f93\u51fa\u5f20\u91cf voxels = torch . zeros ( ( max_total_voxels , self . max_points_per_voxel , C ), dtype = points . dtype , device = device , ) # \u4f53\u7d20\u70b9\u7279\u5f81 coords = torch . zeros ( ( max_total_voxels , 4 ), dtype = torch . int32 , device = device ) # \u4f53\u7d20\u5750\u6807 num_points_per_voxel = torch . zeros ( ( max_total_voxels ,), dtype = torch . int32 , device = device ) # \u6bcf\u4e2a\u4f53\u7d20\u70b9\u6570 voxel_count = 0 # \u5f53\u524d\u4f53\u7d20\u8ba1\u6570 voxel_map : dict = {} # (b,z,y,x) \u2192 voxel_id \u6620\u5c04 # \u904d\u5386 batch \u4e2d\u6bcf\u4e00\u5e27\u70b9\u4e91 for b in range ( B ): # \u904d\u5386\u6bcf\u4e2a batch pts = points [ b ] # (N,C) for n in range ( N ): # \u904d\u5386\u6bcf\u4e2a\u70b9 x , y , z = pts [ n , 0 ] . item (), pts [ n , 1 ] . item (), pts [ n , 2 ] . item () # \u8bfb\u53d6\u5750\u6807 # \u8fc7\u6ee4\u8d85\u51fa\u8303\u56f4\u7684\u70b9 if not ( x_min <= x < x_max and y_min <= y < y_max and z_min <= z < z_max ): continue # \u8df3\u8fc7\u65e0\u6548\u70b9 # \u8ba1\u7b97\u4f53\u7d20\u7d22\u5f15 ix = int (( x - x_min ) / vx ) iy = int (( y - y_min ) / vy ) iz = int (( z - z_min ) / vz ) if not ( 0 <= ix < nx and 0 <= iy < ny and 0 <= iz < nz ): continue # \u518d\u6b21\u5b89\u5168\u68c0\u67e5 key = ( b , iz , iy , ix ) # \u4f53\u7d20\u952e\u503c # \u4e3a\u65b0\u4f53\u7d20\u5206\u914d\u7d22\u5f15 if key not in voxel_map : if voxel_count >= max_total_voxels : # \u8d85\u51fa\u4f53\u7d20\u603b\u4e0a\u9650 continue voxel_map [ key ] = voxel_count # \u8bb0\u5f55\u6620\u5c04 coords [ voxel_count ] = torch . tensor ( [ b , iz , iy , ix ], dtype = torch . int32 , device = device ) # \u5199\u5165\u4f53\u7d20\u5750\u6807 voxel_count += 1 # \u4f53\u7d20\u8ba1\u6570\u52a0\u4e00 voxel_id = voxel_map [ key ] # \u83b7\u53d6\u4f53\u7d20\u7d22\u5f15 # \u82e5\u8be5\u4f53\u7d20\u5df2\u6ee1\uff0c\u5219\u4e22\u5f03\u591a\u4f59\u70b9 cur_num = num_points_per_voxel [ voxel_id ] . item () # \u5f53\u524d\u70b9\u6570 if cur_num >= self . max_points_per_voxel : continue voxels [ voxel_id , cur_num ] = pts [ n ] # \u5199\u5165\u70b9\u7279\u5f81 num_points_per_voxel [ voxel_id ] = cur_num + 1 # \u70b9\u6570\u52a0\u4e00 # \u622a\u53d6\u5230\u5b9e\u9645\u4f7f\u7528\u7684\u4f53\u7d20\u6570\u91cf voxels = voxels [: voxel_count ] # \u88c1\u526a\u591a\u4f59\u4f53\u7d20 coords = coords [: voxel_count ] # \u88c1\u526a\u5750\u6807 num_points_per_voxel = num_points_per_voxel [: voxel_count ] # \u88c1\u526a\u70b9\u6570 return voxels , coords , num_points_per_voxel # \u8fd4\u56de\u4f53\u7d20\u5316\u7ed3\u679c forward ( points ) \u00b6 Parameters: points ( Tensor ) \u2013 \u6279\u91cf\u70b9\u4e91\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a (B, N, C)\u3002 # \u8f93\u5165\u70b9\u4e91 Returns: voxels ( Tensor ) \u2013 \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C)\u3002 # \u8f93\u51fa\u4f53\u7d20 coords ( Tensor ) \u2013 \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\u3002 # [batch, z, y, x] num_points_per_voxel ( Tensor ) \u2013 \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u70b9\u6570\u7edf\u8ba1 Source code in lidar_side\\network\\LidarVoxelization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def forward ( self , points : torch . Tensor ): \"\"\" Args: points (torch.Tensor): \u6279\u91cf\u70b9\u4e91\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a (B, N, C)\u3002 # \u8f93\u5165\u70b9\u4e91 Returns: voxels (torch.Tensor): \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C)\u3002 # \u8f93\u51fa\u4f53\u7d20 coords (torch.Tensor): \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\u3002 # [batch, z, y, x] num_points_per_voxel (torch.Tensor): \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u70b9\u6570\u7edf\u8ba1 \"\"\" assert points . dim () == 3 , \"points \u5fc5\u987b\u662f (B, N, C) \u5f20\u91cf\" # \u57fa\u672c\u68c0\u67e5 B , N , C = points . shape # \u8bfb\u53d6\u7ef4\u5ea6 device = points . device # \u83b7\u53d6\u8bbe\u5907 vx , vy , vz = self . voxel_size # \u4f53\u7d20\u5927\u5c0f x_min , y_min , z_min , x_max , y_max , z_max = self . point_cloud_range # \u70b9\u4e91\u8303\u56f4 nx , ny , nz = self . grid_size # \u7f51\u683c\u5927\u5c0f # \u6700\u5927\u4f53\u7d20\u603b\u6570\uff1a\u6bcf\u5e27 max_voxels\uff0c\u5171 B \u5e27 max_total_voxels = B * self . max_voxels # \u4f53\u7d20\u603b\u4e0a\u9650 # \u9884\u5206\u914d\u8f93\u51fa\u5f20\u91cf voxels = torch . zeros ( ( max_total_voxels , self . max_points_per_voxel , C ), dtype = points . dtype , device = device , ) # \u4f53\u7d20\u70b9\u7279\u5f81 coords = torch . zeros ( ( max_total_voxels , 4 ), dtype = torch . int32 , device = device ) # \u4f53\u7d20\u5750\u6807 num_points_per_voxel = torch . zeros ( ( max_total_voxels ,), dtype = torch . int32 , device = device ) # \u6bcf\u4e2a\u4f53\u7d20\u70b9\u6570 voxel_count = 0 # \u5f53\u524d\u4f53\u7d20\u8ba1\u6570 voxel_map : dict = {} # (b,z,y,x) \u2192 voxel_id \u6620\u5c04 # \u904d\u5386 batch \u4e2d\u6bcf\u4e00\u5e27\u70b9\u4e91 for b in range ( B ): # \u904d\u5386\u6bcf\u4e2a batch pts = points [ b ] # (N,C) for n in range ( N ): # \u904d\u5386\u6bcf\u4e2a\u70b9 x , y , z = pts [ n , 0 ] . item (), pts [ n , 1 ] . item (), pts [ n , 2 ] . item () # \u8bfb\u53d6\u5750\u6807 # \u8fc7\u6ee4\u8d85\u51fa\u8303\u56f4\u7684\u70b9 if not ( x_min <= x < x_max and y_min <= y < y_max and z_min <= z < z_max ): continue # \u8df3\u8fc7\u65e0\u6548\u70b9 # \u8ba1\u7b97\u4f53\u7d20\u7d22\u5f15 ix = int (( x - x_min ) / vx ) iy = int (( y - y_min ) / vy ) iz = int (( z - z_min ) / vz ) if not ( 0 <= ix < nx and 0 <= iy < ny and 0 <= iz < nz ): continue # \u518d\u6b21\u5b89\u5168\u68c0\u67e5 key = ( b , iz , iy , ix ) # \u4f53\u7d20\u952e\u503c # \u4e3a\u65b0\u4f53\u7d20\u5206\u914d\u7d22\u5f15 if key not in voxel_map : if voxel_count >= max_total_voxels : # \u8d85\u51fa\u4f53\u7d20\u603b\u4e0a\u9650 continue voxel_map [ key ] = voxel_count # \u8bb0\u5f55\u6620\u5c04 coords [ voxel_count ] = torch . tensor ( [ b , iz , iy , ix ], dtype = torch . int32 , device = device ) # \u5199\u5165\u4f53\u7d20\u5750\u6807 voxel_count += 1 # \u4f53\u7d20\u8ba1\u6570\u52a0\u4e00 voxel_id = voxel_map [ key ] # \u83b7\u53d6\u4f53\u7d20\u7d22\u5f15 # \u82e5\u8be5\u4f53\u7d20\u5df2\u6ee1\uff0c\u5219\u4e22\u5f03\u591a\u4f59\u70b9 cur_num = num_points_per_voxel [ voxel_id ] . item () # \u5f53\u524d\u70b9\u6570 if cur_num >= self . max_points_per_voxel : continue voxels [ voxel_id , cur_num ] = pts [ n ] # \u5199\u5165\u70b9\u7279\u5f81 num_points_per_voxel [ voxel_id ] = cur_num + 1 # \u70b9\u6570\u52a0\u4e00 # \u622a\u53d6\u5230\u5b9e\u9645\u4f7f\u7528\u7684\u4f53\u7d20\u6570\u91cf voxels = voxels [: voxel_count ] # \u88c1\u526a\u591a\u4f59\u4f53\u7d20 coords = coords [: voxel_count ] # \u88c1\u526a\u5750\u6807 num_points_per_voxel = num_points_per_voxel [: voxel_count ] # \u88c1\u526a\u70b9\u6570 return voxels , coords , num_points_per_voxel # \u8fd4\u56de\u4f53\u7d20\u5316\u7ed3\u679c 'VoxelFeatureEncoder' \u00b6 \u5bf9\u4f53\u7d20\u4e2d\u7684\u70b9\u4e91\u8fdb\u884c\u7279\u5f81\u7f16\u7801\u3002 VoxelFeatureEncoder \u00b6 Bases: Module VoxelFeatureEncoder \u6a21\u5757\uff08\u4f53\u7d20\u7279\u5f81\u7f16\u7801\u7b97\u5b50\uff0cVFE\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u4f53\u7d20\u5316\u540e\u7684\u70b9\u4e91\u6570\u636e\uff0c\u901a\u8fc7\u9010\u70b9 MLP \u7f16\u7801\u5e76\u5728\u4f53\u7d20\u5185\u90e8\u505a \u805a\u5408\uff08\u5982 max pooling\uff09\uff0c\u5c06\u6bcf\u4e2a\u4f53\u7d20\u5185\u4e0d\u5b9a\u6570\u91cf\u7684\u70b9\u7279\u5f81\u538b\u7f29\u4e3a\u4e00\u4e2a \u5b9a\u957f\u5411\u91cf\uff0c\u7528\u4f5c\u540e\u7eed\u7a00\u758f 3D \u4e3b\u5e72\u7f51\u7edc\u7684\u8f93\u5165\u3002 \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 \u00b6 \u5bf9\u4f53\u7d20\u5185\u6bcf\u4e2a\u70b9\u505a\u9010\u70b9\u7279\u5f81\u7f16\u7801\uff1a Linear C_in \u2192 C_mid + BatchNorm1d + ReLU \u5728\u4f53\u7d20\u5185\u90e8\u6cbf\u70b9\u7ef4\u5ea6\u505a max pooling \uff1a \u5c06 (M, T, C_mid) \u805a\u5408\u4e3a (M, C_mid) \u53ef\u9009\u518d\u63a5\u4e00\u5c42\u7ebf\u6027\u53d8\u6362\uff1a Linear C_mid \u2192 C_out + BatchNorm1d + ReLU \u8f93\u5165 \u00b6 Args: voxels: (M, T, C_in) \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff1a - M: \u4f53\u7d20\u6570\u91cf - T: \u6bcf\u4e2a\u4f53\u7d20\u7684\u6700\u5927\u70b9\u6570 - C_in: \u70b9\u7279\u5f81\u7ef4\u5ea6\uff08\u5982 [x,y,z,intensity,...]\uff09\u3002 num_points_per_voxel: (M,) \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u771f\u5b9e\u70b9\u6570\uff0c\u7528\u4e8e\u533a\u5206\u6709\u6548\u70b9\u4e0e padding\u3002 \u8f93\u51fa \u00b6 Returns: voxel_feats: (M, C_out) \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u6bcf\u4e2a\u4f53\u7d20\u5bf9\u5e94\u4e00\u4e2a\u5b9a\u957f\u5411\u91cf\u3002 \u6a21\u5757\u7528\u9014 \u00b6 \u5c06\u4e0d\u5b9a\u70b9\u6570\u7684\u4f53\u7d20\u5185\u70b9\u4e91\u538b\u7f29\u4e3a\u56fa\u5b9a\u7ef4\u5ea6\u7279\u5f81 \u4f5c\u4e3a\u94fe\u8def \u201cLiDAR \u4f53\u7d20\u5316 \u2192 \u7a00\u758f 3D \u4e3b\u5e72\u7f51\u7edc\u201d \u7684\u63a5\u53e3\u7b97\u5b50 \u53ef\u4ee5\u66ff\u6362\u4e3a\u66f4\u590d\u6742\u7684\u591a\u5c42 VFE \u6216\u5e26\u6b8b\u5dee\u7684\u4f53\u7d20\u7f16\u7801\u5668 Source code in lidar_side\\network\\VoxelFeatureEncoder.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class VoxelFeatureEncoder ( nn . Module ): \"\"\" VoxelFeatureEncoder \u6a21\u5757\uff08\u4f53\u7d20\u7279\u5f81\u7f16\u7801\u7b97\u5b50\uff0cVFE\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u4f53\u7d20\u5316\u540e\u7684\u70b9\u4e91\u6570\u636e\uff0c\u901a\u8fc7\u9010\u70b9 MLP \u7f16\u7801\u5e76\u5728\u4f53\u7d20\u5185\u90e8\u505a \u805a\u5408\uff08\u5982 max pooling\uff09\uff0c\u5c06\u6bcf\u4e2a\u4f53\u7d20\u5185\u4e0d\u5b9a\u6570\u91cf\u7684\u70b9\u7279\u5f81\u538b\u7f29\u4e3a\u4e00\u4e2a \u5b9a\u957f\u5411\u91cf\uff0c\u7528\u4f5c\u540e\u7eed\u7a00\u758f 3D \u4e3b\u5e72\u7f51\u7edc\u7684\u8f93\u5165\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. \u5bf9\u4f53\u7d20\u5185\u6bcf\u4e2a\u70b9\u505a\u9010\u70b9\u7279\u5f81\u7f16\u7801\uff1a **Linear C_in \u2192 C_mid + BatchNorm1d + ReLU** 2. \u5728\u4f53\u7d20\u5185\u90e8\u6cbf\u70b9\u7ef4\u5ea6\u505a **max pooling**\uff1a \u5c06 (M, T, C_mid) \u805a\u5408\u4e3a (M, C_mid) 3. \u53ef\u9009\u518d\u63a5\u4e00\u5c42\u7ebf\u6027\u53d8\u6362\uff1a **Linear C_mid \u2192 C_out + BatchNorm1d + ReLU** --- ### \u8f93\u5165 Args: voxels: (M, T, C_in) \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff1a - M: \u4f53\u7d20\u6570\u91cf - T: \u6bcf\u4e2a\u4f53\u7d20\u7684\u6700\u5927\u70b9\u6570 - C_in: \u70b9\u7279\u5f81\u7ef4\u5ea6\uff08\u5982 [x,y,z,intensity,...]\uff09\u3002 num_points_per_voxel: (M,) \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u771f\u5b9e\u70b9\u6570\uff0c\u7528\u4e8e\u533a\u5206\u6709\u6548\u70b9\u4e0e padding\u3002 --- ### \u8f93\u51fa Returns: voxel_feats: (M, C_out) \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u6bcf\u4e2a\u4f53\u7d20\u5bf9\u5e94\u4e00\u4e2a\u5b9a\u957f\u5411\u91cf\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u5c06\u4e0d\u5b9a\u70b9\u6570\u7684\u4f53\u7d20\u5185\u70b9\u4e91\u538b\u7f29\u4e3a\u56fa\u5b9a\u7ef4\u5ea6\u7279\u5f81 - \u4f5c\u4e3a\u94fe\u8def \u201cLiDAR \u4f53\u7d20\u5316 \u2192 \u7a00\u758f 3D \u4e3b\u5e72\u7f51\u7edc\u201d \u7684\u63a5\u53e3\u7b97\u5b50 - \u53ef\u4ee5\u66ff\u6362\u4e3a\u66f4\u590d\u6742\u7684\u591a\u5c42 VFE \u6216\u5e26\u6b8b\u5dee\u7684\u4f53\u7d20\u7f16\u7801\u5668 \"\"\" def __init__ ( self , in_channels : int , mid_channels : int , out_channels : int , use_second_linear : bool = True , ): \"\"\" Args: in_channels (int): \u8f93\u5165\u70b9\u7279\u5f81\u7ef4\u5ea6 C_in\u3002 # \u70b9\u7279\u5f81\u901a\u9053 mid_channels (int): \u4e2d\u95f4\u7f16\u7801\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels (int): \u8f93\u51fa\u4f53\u7d20\u7279\u5f81\u7ef4\u5ea6 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_second_linear (bool): \u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 Linear+CBN+ReLU\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 MLP \"\"\" super () . __init__ () # \u7236\u7c7b\u6784\u9020 # \u7b2c\u4e00\u5c42\u9010\u70b9\u7f16\u7801\uff1aLinear + BN + ReLU self . linear1 = nn . Linear ( in_channels , mid_channels , bias = False ) # \u7ebf\u6027\u6620\u5c04 self . bn1 = nn . BatchNorm1d ( mid_channels ) # BN self . use_second_linear = use_second_linear # \u8bb0\u5f55\u914d\u7f6e if use_second_linear : # \u7b2c\u4e8c\u5c42\u9010\u70b9 / \u9010\u4f53\u7d20\u7f16\u7801 self . linear2 = nn . Linear ( mid_channels , out_channels , bias = False ) # \u7ebf\u6027\u6620\u5c04 self . bn2 = nn . BatchNorm1d ( out_channels ) # BN self . out_channels = out_channels # \u8f93\u51fa\u7ef4\u5ea6 else : self . out_channels = mid_channels # \u8f93\u51fa\u7ef4\u5ea6\u7b49\u4e8e\u4e2d\u95f4\u5c42 def forward ( self , voxels : torch . Tensor , num_points_per_voxel : torch . Tensor , ) -> torch . Tensor : \"\"\" Args: voxels (torch.Tensor): \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C_in)\u3002 # \u8f93\u5165\u4f53\u7d20 num_points_per_voxel (torch.Tensor): \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u6709\u6548\u70b9\u6570 Returns: torch.Tensor: \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_out)\u3002 # \u8f93\u51fa\u4f53\u7d20\u7279\u5f81 \"\"\" assert voxels . dim () == 3 , \"voxels \u5fc5\u987b\u662f (M, T, C_in) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 assert num_points_per_voxel . dim () == 1 , \"num_points_per_voxel \u5fc5\u987b\u662f\u4e00\u7ef4\u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 M , T , C_in = voxels . shape # \u8bfb\u53d6\u5c3a\u5bf8 x = voxels . reshape ( M * T , C_in ) # \u5408\u5e76\u4f53\u7d20\u548c\u70b9\u7ef4\u5ea6 # \u9010\u70b9\u7f16\u7801\uff1aLinear1 + BN1 + ReLU x = self . linear1 ( x ) # \u7ebf\u6027\u53d8\u6362 x = self . bn1 ( x ) # BN x = F . relu ( x , inplace = True ) # \u6fc0\u6d3b x = x . reshape ( M , T , - 1 ) # \u6062\u590d\u4e3a (M,T,C_mid) # \u4f53\u7d20\u5185\u90e8 max pooling\uff08\u63a9\u7801\u65e0\u6548\u70b9\uff09 mask = ( torch . arange ( T , device = voxels . device )[ None , :] . expand ( M , T ) < num_points_per_voxel . unsqueeze ( 1 ) ) # \u6709\u6548\u70b9\u63a9\u7801 (M,T) mask = mask . unsqueeze ( - 1 ) # (M,T,1) x_masked = x . masked_fill ( ~ mask , float ( \"-inf\" )) # \u65e0\u6548\u4f4d\u7f6e\u7f6e\u4e3a -inf voxel_feats , _ = x_masked . max ( dim = 1 ) # \u6cbf\u70b9\u7ef4\u5ea6\u53d6\u6700\u5927\u503c \u2192 (M,C_mid) voxel_feats [ voxel_feats == float ( \"-inf\" )] = 0.0 # \u5904\u7406\u5168\u65e0\u6548\u60c5\u51b5 if self . use_second_linear : # \u7b2c\u4e8c\u5c42\u7f16\u7801\uff1aLinear2 + BN2 + ReLU voxel_feats = self . linear2 ( voxel_feats ) # \u7ebf\u6027\u53d8\u6362 voxel_feats = self . bn2 ( voxel_feats ) # BN voxel_feats = F . relu ( voxel_feats , inplace = True ) # \u6fc0\u6d3b return voxel_feats # \u8fd4\u56de\u4f53\u7d20\u7279\u5f81 __init__ ( in_channels , mid_channels , out_channels , use_second_linear = True ) \u00b6 Parameters: in_channels ( int ) \u2013 \u8f93\u5165\u70b9\u7279\u5f81\u7ef4\u5ea6 C_in\u3002 # \u70b9\u7279\u5f81\u901a\u9053 mid_channels ( int ) \u2013 \u4e2d\u95f4\u7f16\u7801\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels ( int ) \u2013 \u8f93\u51fa\u4f53\u7d20\u7279\u5f81\u7ef4\u5ea6 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_second_linear ( bool , default: True ) \u2013 \u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 Linear+CBN+ReLU\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 MLP Source code in lidar_side\\network\\VoxelFeatureEncoder.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , in_channels : int , mid_channels : int , out_channels : int , use_second_linear : bool = True , ): \"\"\" Args: in_channels (int): \u8f93\u5165\u70b9\u7279\u5f81\u7ef4\u5ea6 C_in\u3002 # \u70b9\u7279\u5f81\u901a\u9053 mid_channels (int): \u4e2d\u95f4\u7f16\u7801\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels (int): \u8f93\u51fa\u4f53\u7d20\u7279\u5f81\u7ef4\u5ea6 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_second_linear (bool): \u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 Linear+CBN+ReLU\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 MLP \"\"\" super () . __init__ () # \u7236\u7c7b\u6784\u9020 # \u7b2c\u4e00\u5c42\u9010\u70b9\u7f16\u7801\uff1aLinear + BN + ReLU self . linear1 = nn . Linear ( in_channels , mid_channels , bias = False ) # \u7ebf\u6027\u6620\u5c04 self . bn1 = nn . BatchNorm1d ( mid_channels ) # BN self . use_second_linear = use_second_linear # \u8bb0\u5f55\u914d\u7f6e if use_second_linear : # \u7b2c\u4e8c\u5c42\u9010\u70b9 / \u9010\u4f53\u7d20\u7f16\u7801 self . linear2 = nn . Linear ( mid_channels , out_channels , bias = False ) # \u7ebf\u6027\u6620\u5c04 self . bn2 = nn . BatchNorm1d ( out_channels ) # BN self . out_channels = out_channels # \u8f93\u51fa\u7ef4\u5ea6 else : self . out_channels = mid_channels # \u8f93\u51fa\u7ef4\u5ea6\u7b49\u4e8e\u4e2d\u95f4\u5c42 forward ( voxels , num_points_per_voxel ) \u00b6 Parameters: voxels ( Tensor ) \u2013 \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C_in)\u3002 # \u8f93\u5165\u4f53\u7d20 num_points_per_voxel ( Tensor ) \u2013 \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u6709\u6548\u70b9\u6570 Returns: Tensor \u2013 torch.Tensor: \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_out)\u3002 # \u8f93\u51fa\u4f53\u7d20\u7279\u5f81 Source code in lidar_side\\network\\VoxelFeatureEncoder.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def forward ( self , voxels : torch . Tensor , num_points_per_voxel : torch . Tensor , ) -> torch . Tensor : \"\"\" Args: voxels (torch.Tensor): \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C_in)\u3002 # \u8f93\u5165\u4f53\u7d20 num_points_per_voxel (torch.Tensor): \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u6709\u6548\u70b9\u6570 Returns: torch.Tensor: \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_out)\u3002 # \u8f93\u51fa\u4f53\u7d20\u7279\u5f81 \"\"\" assert voxels . dim () == 3 , \"voxels \u5fc5\u987b\u662f (M, T, C_in) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 assert num_points_per_voxel . dim () == 1 , \"num_points_per_voxel \u5fc5\u987b\u662f\u4e00\u7ef4\u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 M , T , C_in = voxels . shape # \u8bfb\u53d6\u5c3a\u5bf8 x = voxels . reshape ( M * T , C_in ) # \u5408\u5e76\u4f53\u7d20\u548c\u70b9\u7ef4\u5ea6 # \u9010\u70b9\u7f16\u7801\uff1aLinear1 + BN1 + ReLU x = self . linear1 ( x ) # \u7ebf\u6027\u53d8\u6362 x = self . bn1 ( x ) # BN x = F . relu ( x , inplace = True ) # \u6fc0\u6d3b x = x . reshape ( M , T , - 1 ) # \u6062\u590d\u4e3a (M,T,C_mid) # \u4f53\u7d20\u5185\u90e8 max pooling\uff08\u63a9\u7801\u65e0\u6548\u70b9\uff09 mask = ( torch . arange ( T , device = voxels . device )[ None , :] . expand ( M , T ) < num_points_per_voxel . unsqueeze ( 1 ) ) # \u6709\u6548\u70b9\u63a9\u7801 (M,T) mask = mask . unsqueeze ( - 1 ) # (M,T,1) x_masked = x . masked_fill ( ~ mask , float ( \"-inf\" )) # \u65e0\u6548\u4f4d\u7f6e\u7f6e\u4e3a -inf voxel_feats , _ = x_masked . max ( dim = 1 ) # \u6cbf\u70b9\u7ef4\u5ea6\u53d6\u6700\u5927\u503c \u2192 (M,C_mid) voxel_feats [ voxel_feats == float ( \"-inf\" )] = 0.0 # \u5904\u7406\u5168\u65e0\u6548\u60c5\u51b5 if self . use_second_linear : # \u7b2c\u4e8c\u5c42\u7f16\u7801\uff1aLinear2 + BN2 + ReLU voxel_feats = self . linear2 ( voxel_feats ) # \u7ebf\u6027\u53d8\u6362 voxel_feats = self . bn2 ( voxel_feats ) # BN voxel_feats = F . relu ( voxel_feats , inplace = True ) # \u6fc0\u6d3b return voxel_feats # \u8fd4\u56de\u4f53\u7d20\u7279\u5f81 'Sparse3DBackbone' \u00b6 \u57fa\u4e8e\u7a00\u758f\u5377\u79ef\u7684 3D \u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u3002 Sparse3DBackbone \u00b6 Bases: Module Sparse3DBackbone \u6a21\u5757\uff08\u7a00\u758f\u4e09\u7ef4\u4e3b\u5e72\u7f51\u7edc\u7b97\u5b50 / SECOND \u98ce\u683c\u7b80\u5316\u7248\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u4f53\u7d20\u7279\u5f81\u53ca\u5176\u4e09\u7ef4\u5750\u6807\uff0c\u5c06\u7a00\u758f\u4f53\u7d20\u7279\u5f81\u5d4c\u5165\u5230\u89c4\u5219 3D \u4f53\u7d20\u7f51\u683c\u4e2d\uff0c \u518d\u901a\u8fc7 3D \u5377\u79ef\u5bf9\u6574\u4e2a\u4f53\u7d20\u7f51\u683c\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (B, C_out, Z, Y, X) \u7684\u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u4f9b\u540e\u7eed\u9ad8\u5ea6\u7ef4\u538b\u7f29\u7b97\u5b50\u4f7f\u7528\u3002 \u6ce8\u610f\uff1a\u672c\u5b9e\u73b0\u4e3a\u201c\u7a20\u5bc6 3D \u5377\u79ef\u7248\u672c\u201d\u7684\u7b80\u5316\u9aa8\u67b6\uff0c\u7528\u4e8e\u7b97\u5b50\u7ea7\u91cd\u6784\u4e0e\u5b9e\u9a8c Demo\uff0c \u5e76\u975e\u771f\u6b63\u7684\u9ad8\u6027\u80fd SparseConv \u5b9e\u73b0\u3002 Parameters: in_channels ( int ) \u2013 int \u8f93\u5165\u4f53\u7d20\u7279\u5f81\u901a\u9053\u6570 C_in\u3002 hidden_channels ( int ) \u2013 int \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\uff0c\u7528\u4e8e\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002 out_channels ( int ) \u2013 int \u8f93\u51fa\u4e09\u7ef4\u7279\u5f81\u901a\u9053\u6570 C_out\u3002 grid_size ( Tuple [ int , int , int ] ) \u2013 tuple[int, int, int] \u4e09\u7ef4\u4f53\u7d20\u7f51\u683c\u5c3a\u5bf8 (Z, Y, X)\uff0c\u4e0e\u4f53\u7d20\u5316\u9636\u6bb5\u4f7f\u7528\u7684\u7f51\u683c\u5927\u5c0f\u4e00\u81f4\u3002 Returns: feat_3d \u2013 (B, C_out, Z, Y, X) \u7f16\u7801\u540e\u7684\u4e09\u7ef4\u4f53\u7d20\u7279\u5f81\u4f53\uff0c\u7528\u4e8e\u540e\u7eed 3D\u2192BEV \u538b\u7f29\u7b97\u5b50\u3002 Source code in lidar_side\\network\\Sparse3DBackbone.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class Sparse3DBackbone ( nn . Module ): \"\"\" Sparse3DBackbone \u6a21\u5757\uff08\u7a00\u758f\u4e09\u7ef4\u4e3b\u5e72\u7f51\u7edc\u7b97\u5b50 / SECOND \u98ce\u683c\u7b80\u5316\u7248\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u4f53\u7d20\u7279\u5f81\u53ca\u5176\u4e09\u7ef4\u5750\u6807\uff0c\u5c06\u7a00\u758f\u4f53\u7d20\u7279\u5f81\u5d4c\u5165\u5230\u89c4\u5219 3D \u4f53\u7d20\u7f51\u683c\u4e2d\uff0c \u518d\u901a\u8fc7 3D \u5377\u79ef\u5bf9\u6574\u4e2a\u4f53\u7d20\u7f51\u683c\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (B, C_out, Z, Y, X) \u7684\u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u4f9b\u540e\u7eed\u9ad8\u5ea6\u7ef4\u538b\u7f29\u7b97\u5b50\u4f7f\u7528\u3002 \u6ce8\u610f\uff1a\u672c\u5b9e\u73b0\u4e3a\u201c\u7a20\u5bc6 3D \u5377\u79ef\u7248\u672c\u201d\u7684\u7b80\u5316\u9aa8\u67b6\uff0c\u7528\u4e8e\u7b97\u5b50\u7ea7\u91cd\u6784\u4e0e\u5b9e\u9a8c Demo\uff0c \u5e76\u975e\u771f\u6b63\u7684\u9ad8\u6027\u80fd SparseConv \u5b9e\u73b0\u3002 Args: in_channels: int \u8f93\u5165\u4f53\u7d20\u7279\u5f81\u901a\u9053\u6570 C_in\u3002 hidden_channels: int \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\uff0c\u7528\u4e8e\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002 out_channels: int \u8f93\u51fa\u4e09\u7ef4\u7279\u5f81\u901a\u9053\u6570 C_out\u3002 grid_size: tuple[int, int, int] \u4e09\u7ef4\u4f53\u7d20\u7f51\u683c\u5c3a\u5bf8 (Z, Y, X)\uff0c\u4e0e\u4f53\u7d20\u5316\u9636\u6bb5\u4f7f\u7528\u7684\u7f51\u683c\u5927\u5c0f\u4e00\u81f4\u3002 Returns: feat_3d: (B, C_out, Z, Y, X) \u7f16\u7801\u540e\u7684\u4e09\u7ef4\u4f53\u7d20\u7279\u5f81\u4f53\uff0c\u7528\u4e8e\u540e\u7eed 3D\u2192BEV \u538b\u7f29\u7b97\u5b50\u3002 \"\"\" def __init__ ( self , in_channels : int , hidden_channels : int , out_channels : int , grid_size : Tuple [ int , int , int ], ): super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . in_channels = in_channels # \u8f93\u5165\u901a\u9053\u6570 self . hidden_channels = hidden_channels # \u4e2d\u95f4\u901a\u9053\u6570 self . out_channels = out_channels # \u8f93\u51fa\u901a\u9053\u6570 self . grid_size = grid_size # (Z, Y, X) \u7f51\u683c\u5927\u5c0f # \u7b80\u5316\u7248 3D \u4e3b\u5e72\uff1a\u4e24\u5c42 3D \u5377\u79ef + BN + ReLU self . conv_block1 = nn . Sequential ( nn . Conv3d ( in_channels , hidden_channels , kernel_size = 3 , padding = 1 , bias = False ), # 3D \u5377\u79ef nn . BatchNorm3d ( hidden_channels ), # BN nn . ReLU ( inplace = True ), # \u6fc0\u6d3b ) self . conv_block2 = nn . Sequential ( nn . Conv3d ( hidden_channels , out_channels , kernel_size = 3 , padding = 1 , bias = False ), # 3D \u5377\u79ef nn . BatchNorm3d ( out_channels ), # BN nn . ReLU ( inplace = True ), # \u6fc0\u6d3b ) def forward ( self , voxel_feats : torch . Tensor , voxel_coords : torch . Tensor , batch_size : int , ) -> torch . Tensor : \"\"\" Args: voxel_feats (torch.Tensor): \u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_in)\u3002 # \u6bcf\u4e2a\u4f53\u7d20\u4e00\u4e2a\u7279\u5f81\u5411\u91cf voxel_coords (torch.Tensor): \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\uff0c\u683c\u5f0f\u4e3a [batch_idx, z, y, x]\u3002 # \u4f53\u7d20\u5728\u7f51\u683c\u4e2d\u7684\u7d22\u5f15 batch_size (int): \u5f53\u524d\u6279\u6b21\u5927\u5c0f B\u3002 # batch \u6570\u91cf Returns: torch.Tensor: \u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u5f62\u72b6 (B, C_out, Z, Y, X)\u3002 # \u7a20\u5bc6 3D \u7279\u5f81 \"\"\" assert voxel_feats . dim () == 2 , \"voxel_feats \u5fc5\u987b\u662f (M, C_in) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 assert voxel_coords . dim () == 2 and voxel_coords . shape [ 1 ] == 4 , \"voxel_coords \u5fc5\u987b\u662f (M, 4) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 device = voxel_feats . device # \u83b7\u53d6\u8bbe\u5907 M , C_in = voxel_feats . shape # \u8bfb\u53d6\u4f53\u7d20\u6570\u91cf\u4e0e\u901a\u9053\u6570 Z , Y , X = self . grid_size # \u89e3\u5305\u7f51\u683c\u5c3a\u5bf8 # \u521d\u59cb\u5316\u7a20\u5bc6\u4f53\u7d20\u7f51\u683c\uff1a (B, C_in, Z, Y, X) dense_grid = torch . zeros ( batch_size , C_in , Z , Y , X , dtype = voxel_feats . dtype , device = device , ) # \u521d\u59cb\u5316 3D \u4f53\u7d20\u7f51\u683c # \u5c06\u7a00\u758f\u4f53\u7d20\u7279\u5f81 scatter \u5230\u7a20\u5bc6\u4f53\u7d20\u7f51\u683c\u4e2d b = voxel_coords [:, 0 ] . long () # batch \u7d22\u5f15 z = voxel_coords [:, 1 ] . long () # z \u8f74\u7d22\u5f15 y = voxel_coords [:, 2 ] . long () # y \u8f74\u7d22\u5f15 x = voxel_coords [:, 3 ] . long () # x \u8f74\u7d22\u5f15 # \u8fc7\u6ee4\u8d8a\u754c\u4f53\u7d20\uff08\u7406\u8bba\u4e0a voxelization \u5df2\u7ecf\u4fdd\u8bc1\u5408\u6cd5\uff0c\u8fd9\u91cc\u518d\u505a\u4e00\u6b21\u5b89\u5168\u68c0\u67e5\uff09 valid_mask = ( ( b >= 0 ) & ( b < batch_size ) & ( z >= 0 ) & ( z < Z ) & ( y >= 0 ) & ( y < Y ) & ( x >= 0 ) & ( x < X ) ) # \u5408\u6cd5\u4f53\u7d20\u63a9\u7801 if valid_mask . any (): # \u82e5\u5b58\u5728\u5408\u6cd5\u4f53\u7d20 b_valid = b [ valid_mask ] # \u6709\u6548 batch \u7d22\u5f15 z_valid = z [ valid_mask ] # \u6709\u6548 z \u7d22\u5f15 y_valid = y [ valid_mask ] # \u6709\u6548 y \u7d22\u5f15 x_valid = x [ valid_mask ] # \u6709\u6548 x \u7d22\u5f15 feats_valid = voxel_feats [ valid_mask ] # \u6709\u6548\u4f53\u7d20\u7279\u5f81 (M_valid,C_in) dense_grid [ b_valid , :, z_valid , y_valid , x_valid ] = feats_valid # \u586b\u5145\u5230\u7a20\u5bc6\u7f51\u683c\u4e2d # \u901a\u8fc7 3D \u5377\u79ef\u4e3b\u5e72\u63d0\u53d6\u4e09\u7ef4\u7279\u5f81 x3d = self . conv_block1 ( dense_grid ) # \u7b2c\u4e00\u5c42 3D \u5377\u79ef\u5757 x3d = self . conv_block2 ( x3d ) # \u7b2c\u4e8c\u5c42 3D \u5377\u79ef\u5757 return x3d # \u8fd4\u56de\u4e09\u7ef4\u7279\u5f81\u4f53 forward ( voxel_feats , voxel_coords , batch_size ) \u00b6 Parameters: voxel_feats ( Tensor ) \u2013 \u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_in)\u3002 # \u6bcf\u4e2a\u4f53\u7d20\u4e00\u4e2a\u7279\u5f81\u5411\u91cf voxel_coords ( Tensor ) \u2013 \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\uff0c\u683c\u5f0f\u4e3a [batch_idx, z, y, x]\u3002 # \u4f53\u7d20\u5728\u7f51\u683c\u4e2d\u7684\u7d22\u5f15 batch_size ( int ) \u2013 \u5f53\u524d\u6279\u6b21\u5927\u5c0f B\u3002 # batch \u6570\u91cf Returns: Tensor \u2013 torch.Tensor: \u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u5f62\u72b6 (B, C_out, Z, Y, X)\u3002 # \u7a20\u5bc6 3D \u7279\u5f81 Source code in lidar_side\\network\\Sparse3DBackbone.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , voxel_feats : torch . Tensor , voxel_coords : torch . Tensor , batch_size : int , ) -> torch . Tensor : \"\"\" Args: voxel_feats (torch.Tensor): \u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_in)\u3002 # \u6bcf\u4e2a\u4f53\u7d20\u4e00\u4e2a\u7279\u5f81\u5411\u91cf voxel_coords (torch.Tensor): \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\uff0c\u683c\u5f0f\u4e3a [batch_idx, z, y, x]\u3002 # \u4f53\u7d20\u5728\u7f51\u683c\u4e2d\u7684\u7d22\u5f15 batch_size (int): \u5f53\u524d\u6279\u6b21\u5927\u5c0f B\u3002 # batch \u6570\u91cf Returns: torch.Tensor: \u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u5f62\u72b6 (B, C_out, Z, Y, X)\u3002 # \u7a20\u5bc6 3D \u7279\u5f81 \"\"\" assert voxel_feats . dim () == 2 , \"voxel_feats \u5fc5\u987b\u662f (M, C_in) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 assert voxel_coords . dim () == 2 and voxel_coords . shape [ 1 ] == 4 , \"voxel_coords \u5fc5\u987b\u662f (M, 4) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 device = voxel_feats . device # \u83b7\u53d6\u8bbe\u5907 M , C_in = voxel_feats . shape # \u8bfb\u53d6\u4f53\u7d20\u6570\u91cf\u4e0e\u901a\u9053\u6570 Z , Y , X = self . grid_size # \u89e3\u5305\u7f51\u683c\u5c3a\u5bf8 # \u521d\u59cb\u5316\u7a20\u5bc6\u4f53\u7d20\u7f51\u683c\uff1a (B, C_in, Z, Y, X) dense_grid = torch . zeros ( batch_size , C_in , Z , Y , X , dtype = voxel_feats . dtype , device = device , ) # \u521d\u59cb\u5316 3D \u4f53\u7d20\u7f51\u683c # \u5c06\u7a00\u758f\u4f53\u7d20\u7279\u5f81 scatter \u5230\u7a20\u5bc6\u4f53\u7d20\u7f51\u683c\u4e2d b = voxel_coords [:, 0 ] . long () # batch \u7d22\u5f15 z = voxel_coords [:, 1 ] . long () # z \u8f74\u7d22\u5f15 y = voxel_coords [:, 2 ] . long () # y \u8f74\u7d22\u5f15 x = voxel_coords [:, 3 ] . long () # x \u8f74\u7d22\u5f15 # \u8fc7\u6ee4\u8d8a\u754c\u4f53\u7d20\uff08\u7406\u8bba\u4e0a voxelization \u5df2\u7ecf\u4fdd\u8bc1\u5408\u6cd5\uff0c\u8fd9\u91cc\u518d\u505a\u4e00\u6b21\u5b89\u5168\u68c0\u67e5\uff09 valid_mask = ( ( b >= 0 ) & ( b < batch_size ) & ( z >= 0 ) & ( z < Z ) & ( y >= 0 ) & ( y < Y ) & ( x >= 0 ) & ( x < X ) ) # \u5408\u6cd5\u4f53\u7d20\u63a9\u7801 if valid_mask . any (): # \u82e5\u5b58\u5728\u5408\u6cd5\u4f53\u7d20 b_valid = b [ valid_mask ] # \u6709\u6548 batch \u7d22\u5f15 z_valid = z [ valid_mask ] # \u6709\u6548 z \u7d22\u5f15 y_valid = y [ valid_mask ] # \u6709\u6548 y \u7d22\u5f15 x_valid = x [ valid_mask ] # \u6709\u6548 x \u7d22\u5f15 feats_valid = voxel_feats [ valid_mask ] # \u6709\u6548\u4f53\u7d20\u7279\u5f81 (M_valid,C_in) dense_grid [ b_valid , :, z_valid , y_valid , x_valid ] = feats_valid # \u586b\u5145\u5230\u7a20\u5bc6\u7f51\u683c\u4e2d # \u901a\u8fc7 3D \u5377\u79ef\u4e3b\u5e72\u63d0\u53d6\u4e09\u7ef4\u7279\u5f81 x3d = self . conv_block1 ( dense_grid ) # \u7b2c\u4e00\u5c42 3D \u5377\u79ef\u5757 x3d = self . conv_block2 ( x3d ) # \u7b2c\u4e8c\u5c42 3D \u5377\u79ef\u5757 return x3d # \u8fd4\u56de\u4e09\u7ef4\u7279\u5f81\u4f53 'lidar_flatten_z_to_bev' \u00b6 \u5c06 3D \u7279\u5f81\u6cbf\u9ad8\u5ea6\u7ef4\uff08Z\uff09\u538b\u7f29\u4e3a BEV \u5e73\u9762\u7279\u5f81\u3002 lidar_flatten_z_to_bev ( feat_3d ) \u00b6 \u5c06 LiDAR \u4e09\u7ef4\u4f53\u7d20\u7279\u5f81\u6cbf\u9ad8\u5ea6\u7ef4\u5ea6 z \u8fdb\u884c\u805a\u5408\uff0c\u538b\u7f29\u4e3a BEV \u5e73\u9762\u4e0a\u7684 \u4e8c\u7ef4\u7279\u5f81\u56fe\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (B, C, H, W)\u3002 Parameters: feat_3d ( Tensor ) \u2013 (B, C, Z, H, W) LiDAR \u5206\u652f\u8f93\u51fa\u7684\u4e09\u7ef4\u7279\u5f81\u4f53\uff1a - B: batch size - C: \u901a\u9053\u6570 - Z: \u9ad8\u5ea6\u65b9\u5411\u4f53\u7d20\u5c42\u6570 - H, W: \u6c34\u5e73\u5e73\u9762\u4e0a\u7684\u4f53\u7d20\u7f51\u683c\u5c3a\u5bf8\u3002 Returns: bev_feat ( Tensor ) \u2013 (B, C, H, W) \u6cbf\u9ad8\u5ea6\u7ef4\u5ea6\u805a\u5408\u540e\u7684 BEV \u7279\u5f81\u56fe\uff0c\u8fd9\u91cc\u91c7\u7528 sum pooling \u8fdb\u884c\u805a\u5408\u3002 Source code in lidar_side\\operator\\lidar_flatten_z_to_bev.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @torch . library . custom_op ( \"bevfusion_ops::lidar_flatten_z_to_bev\" , mutates_args = [] ) def lidar_flatten_z_to_bev ( feat_3d : torch . Tensor , ) -> torch . Tensor : \"\"\" \u5c06 LiDAR \u4e09\u7ef4\u4f53\u7d20\u7279\u5f81\u6cbf\u9ad8\u5ea6\u7ef4\u5ea6 z \u8fdb\u884c\u805a\u5408\uff0c\u538b\u7f29\u4e3a BEV \u5e73\u9762\u4e0a\u7684 \u4e8c\u7ef4\u7279\u5f81\u56fe\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (B, C, H, W)\u3002 Args: feat_3d: (B, C, Z, H, W) LiDAR \u5206\u652f\u8f93\u51fa\u7684\u4e09\u7ef4\u7279\u5f81\u4f53\uff1a - B: batch size - C: \u901a\u9053\u6570 - Z: \u9ad8\u5ea6\u65b9\u5411\u4f53\u7d20\u5c42\u6570 - H, W: \u6c34\u5e73\u5e73\u9762\u4e0a\u7684\u4f53\u7d20\u7f51\u683c\u5c3a\u5bf8\u3002 Returns: bev_feat: (B, C, H, W) \u6cbf\u9ad8\u5ea6\u7ef4\u5ea6\u805a\u5408\u540e\u7684 BEV \u7279\u5f81\u56fe\uff0c\u8fd9\u91cc\u91c7\u7528 sum pooling \u8fdb\u884c\u805a\u5408\u3002 \"\"\" assert feat_3d . dim () == 5 , \"feat_3d \u5fc5\u987b\u662f (B, C, Z, H, W) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 # \u4f7f\u7528 sum pooling \u6cbf\u9ad8\u5ea6\u7ef4\u5ea6\u805a\u5408\uff1a # (B, C, Z, H, W) \u2192 (B, C, H, W) bev_feat = feat_3d . sum ( dim = 2 ) # \u6cbf Z \u7ef4\u6c42\u548c return bev_feat # \u8fd4\u56de BEV \u7279\u5f81","title":"LiDAR \u5206\u652f\u6a21\u5757"},{"location":"lidar_side/#lidar-side","text":"Lidar Side \u6a21\u5757\u7684\u4f5c\u7528\u662f\uff1a \u5c06\u539f\u59cb LiDAR \u70b9\u4e91\u8fdb\u884c\u4f53\u7d20\u5316\u3001\u7279\u5f81\u7f16\u7801\uff0c\u5e76\u901a\u8fc7\u7f51\u7edc\u63d0\u53d6 3D \u8bed\u4e49\u8868\u793a\uff1b \u968f\u540e\u5c06 3D \u7279\u5f81\u538b\u7f29\u81f3 BEV \u7a7a\u95f4\uff0c\u4e3a\u540e\u7eed BEV \u878d\u5408\u63d0\u4f9b\u70b9\u4e91 BEV \u8868\u8fbe\u3002 \u672c\u6a21\u5757\u5305\u542b\u56db\u7c7b\u6838\u5fc3\u529f\u80fd\uff1a \u70b9\u4e91\u4f53\u7d20\u5316\uff08Voxelization\uff09 \u5c06\u7a00\u758f\u70b9\u4e91\u8f6c\u6362\u4e3a\u89c4\u5219\u4f53\u7d20\u7ed3\u6784\uff0c\u4fbf\u4e8e\u540e\u7eed\u7f51\u7edc\u5904\u7406\u3002 \u4f53\u7d20\u7279\u5f81\u7f16\u7801\uff08Voxel Feature Encoding\uff09 \u5bf9\u6bcf\u4e2a\u4f53\u7d20\u5185\u90e8\u7684\u539f\u59cb\u70b9\u7279\u5f81\u8fdb\u884c\u7f16\u7801\uff0c\u5f97\u5230\u56fa\u5b9a\u7ef4\u5ea6\u8868\u793a\u3002 \u7a00\u758f\u5377\u79ef 3D Backbone\uff08Sparse 3D CNN\uff09 \u4f7f\u7528\u7a00\u758f\u5377\u79ef\u63d0\u53d6 3D \u8bed\u4e49\u7279\u5f81\u3002 \u70b9\u4e91 BEV \u538b\u7f29\u7b97\u5b50 \u5c06 3D \u7279\u5f81\u6cbf Z \u7ef4\u538b\u7f29\u6210 BEV \u7279\u5f81\u5e73\u9762\u3002","title":"Lidar Side\uff08\u70b9\u4e91\u4fa7\u6a21\u5757\uff09"},{"location":"lidar_side/#api","text":"\u4ee5\u4e0b\u5217\u51fa Lidar Side \u4e2d\u5168\u90e8\u7b97\u5b50\u4e0e\u7f51\u7edc\u6a21\u5757\uff0c\u5e76\u7ed9\u51fa\u5bf9\u5e94\u6587\u4ef6\u8def\u5f84\u3002","title":"API \u6587\u6863"},{"location":"lidar_side/#lidarvoxelization","text":"\u5c06\u539f\u59cb\u70b9\u4e91\u8f6c\u6362\u4e3a\u7a00\u758f\u4f53\u7d20\u7ed3\u6784\u3002","title":"'LidarVoxelization'"},{"location":"lidar_side/#lidar_side.network.LidarVoxelization.LidarVoxelization","text":"Bases: Module LidarVoxelization \u6a21\u5757\uff08\u70b9\u4e91\u4f53\u7d20\u5316\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u5c06\u8fde\u7eed\u7684 3D \u70b9\u4e91\u5750\u6807\u6309\u7167\u7ed9\u5b9a\u7684 voxel_size \u4e0e point_cloud_range \u91cf\u5316\u5230\u89c4\u5219 3D \u4f53\u7d20\u7f51\u683c\u4e2d\uff0c\u5e76\u8f93\u51fa\u4f53\u7d20\u4e2d\u7684\u70b9\u96c6\u5408\u3001\u4f53\u7d20\u5750\u6807\u4ee5\u53ca\u6bcf\u4e2a\u4f53\u7d20\u7684\u70b9\u6570\u3002 \u672c\u5b9e\u73b0\u4e3a\u7b80\u5316\u7248 CPU \u4f53\u7d20\u5316\u903b\u8f91\uff0c\u4fbf\u4e8e\u9605\u8bfb\u4e0e\u7b97\u5b50\u7ea7\u91cd\u6784\uff0c\u800c\u975e\u9ad8\u6027\u80fd\u5b9e\u73b0\u3002","title":"LidarVoxelization"},{"location":"lidar_side/#lidar_side.network.LidarVoxelization.LidarVoxelization--_1","text":"\u6839\u636e point_cloud_range \u4e0e voxel_size \u8ba1\u7b97 3D \u7f51\u683c\u5927\u5c0f (nx, ny, nz) \u904d\u5386 batch \u4e2d\u6bcf\u4e2a\u70b9\uff0c\u8ba1\u7b97\u5176\u6240\u5728\u4f53\u7d20\u7d22\u5f15 (ix, iy, iz) \u4e3a\u6bcf\u4e2a\u65b0\u4f53\u7d20\u5206\u914d\u4e00\u4e2a voxel \u7f16\u53f7\uff0c\u6700\u591a\u4fdd\u7559 max_voxels \u4e2a\u4f53\u7d20 \u5728\u4f53\u7d20\u5185\u90e8\u7d2f\u79ef\u70b9\uff0c\u6700\u591a\u4fdd\u7559 max_points_per_voxel \u4e2a\u70b9 \u6700\u7ec8\u8f93\u51fa\uff1a voxels: (M, T, C) coords: (M, 4) [batch_idx, z, y, x] num_points: (M,)","title":"\u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09"},{"location":"lidar_side/#lidar_side.network.LidarVoxelization.LidarVoxelization--_2","text":"Args: points: (B, N, C) \u6279\u91cf\u70b9\u4e91\u6570\u636e\uff1a - B: batch size - N: \u6bcf\u5e27\u70b9\u7684\u6700\u5927\u6570\u91cf - C: \u70b9\u7279\u5f81\u7ef4\u5ea6\uff08\u81f3\u5c11\u5305\u542b x,y,z\uff0c\u901a\u5e38\u4e3a 4: [x,y,z,intensity]\uff09\u3002 voxel_size (tuple[float, float, float]): \u4f53\u7d20\u5927\u5c0f (vx, vy, vz)\uff0c\u5355\u4f4d\u4e0e\u70b9\u4e91\u5750\u6807\u4e00\u81f4\u3002 point_cloud_range (tuple[float, float, float, float, float, float]): \u70b9\u4e91\u8303\u56f4 [x_min, y_min, z_min, x_max, y_max, z_max]\u3002 max_points_per_voxel (int): \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u5141\u8bb8\u4fdd\u5b58\u7684\u6700\u5927\u70b9\u6570 T\u3002 max_voxels (int): \u6bcf\u5e27\u6700\u591a\u4fdd\u7559\u7684\u4f53\u7d20\u6570\u91cf M_max\u3002","title":"\u8f93\u5165"},{"location":"lidar_side/#lidar_side.network.LidarVoxelization.LidarVoxelization--_3","text":"Returns: voxels: (M, T, C) \u4f53\u7d20\u5185\u70b9\u7279\u5f81\u5f20\u91cf\uff1a - M: \u5b9e\u9645\u751f\u6210\u7684\u4f53\u7d20\u6570\u91cf\uff08M \u2264 B * max_voxels\uff09 - T: \u6bcf\u4e2a\u4f53\u7d20\u7684\u6700\u5927\u70b9\u6570\uff0c\u591a\u4f59\u7684\u70b9\u4f1a\u88ab\u4e22\u5f03\uff0c\u672a\u586b\u6ee1\u4f4d\u7f6e\u4e3a 0\u3002 coords: (M, 4) \u6bcf\u4e2a\u4f53\u7d20\u5728 3D \u7f51\u683c\u4e2d\u7684\u6574\u6570\u5750\u6807 [batch_idx, z, y, x]\u3002 num_points_per_voxel: (M,) \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u771f\u5b9e\u70b9\u6570\uff0c\u7528\u4e8e\u533a\u5206\u6709\u6548\u70b9\u4e0e padding\u3002","title":"\u8f93\u51fa"},{"location":"lidar_side/#lidar_side.network.LidarVoxelization.LidarVoxelization--_4","text":"\u5c06\u7a20\u5bc6\u70b9\u4e91\u8f6c\u6362\u4e3a\u89c4\u5219\u4f53\u7d20\u8868\u793a\uff0c\u4f5c\u4e3a\u540e\u7eed VFE \u4e0e Sparse 3D Backbone \u7684\u8f93\u5165 \u4f5c\u4e3a\u94fe\u8def \u201cLiDAR \u539f\u59cb\u70b9\u4e91 \u2192 LiDAR BEV \u7279\u5f81\u201d \u4e2d\u7684\u7b2c\u4e00\u4e2a\u79bb\u6563\u5316\u7b97\u5b50 \u5728\u7b97\u5b50\u6846\u67b6\u4e2d\u7edf\u4e00\u5c01\u88c5\u4f53\u7d20\u5316\u903b\u8f91\uff0c\u4fbf\u4e8e\u66ff\u6362\u4e3a\u66f4\u9ad8\u6027\u80fd\u7684 C++/CUDA \u5b9e\u73b0 Source code in lidar_side\\network\\LidarVoxelization.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class LidarVoxelization ( nn . Module ): \"\"\" LidarVoxelization \u6a21\u5757\uff08\u70b9\u4e91\u4f53\u7d20\u5316\u7b97\u5b50\uff09\u3002 \u8be5\u6a21\u5757\u5c06\u8fde\u7eed\u7684 3D \u70b9\u4e91\u5750\u6807\u6309\u7167\u7ed9\u5b9a\u7684 voxel_size \u4e0e point_cloud_range \u91cf\u5316\u5230\u89c4\u5219 3D \u4f53\u7d20\u7f51\u683c\u4e2d\uff0c\u5e76\u8f93\u51fa\u4f53\u7d20\u4e2d\u7684\u70b9\u96c6\u5408\u3001\u4f53\u7d20\u5750\u6807\u4ee5\u53ca\u6bcf\u4e2a\u4f53\u7d20\u7684\u70b9\u6570\u3002 \u672c\u5b9e\u73b0\u4e3a\u7b80\u5316\u7248 CPU \u4f53\u7d20\u5316\u903b\u8f91\uff0c\u4fbf\u4e8e\u9605\u8bfb\u4e0e\u7b97\u5b50\u7ea7\u91cd\u6784\uff0c\u800c\u975e\u9ad8\u6027\u80fd\u5b9e\u73b0\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. \u6839\u636e point_cloud_range \u4e0e voxel_size \u8ba1\u7b97 3D \u7f51\u683c\u5927\u5c0f (nx, ny, nz) 2. \u904d\u5386 batch \u4e2d\u6bcf\u4e2a\u70b9\uff0c\u8ba1\u7b97\u5176\u6240\u5728\u4f53\u7d20\u7d22\u5f15 (ix, iy, iz) 3. \u4e3a\u6bcf\u4e2a\u65b0\u4f53\u7d20\u5206\u914d\u4e00\u4e2a voxel \u7f16\u53f7\uff0c\u6700\u591a\u4fdd\u7559 max_voxels \u4e2a\u4f53\u7d20 4. \u5728\u4f53\u7d20\u5185\u90e8\u7d2f\u79ef\u70b9\uff0c\u6700\u591a\u4fdd\u7559 max_points_per_voxel \u4e2a\u70b9 5. \u6700\u7ec8\u8f93\u51fa\uff1a - voxels: (M, T, C) - coords: (M, 4) [batch_idx, z, y, x] - num_points: (M,) --- ### \u8f93\u5165 Args: points: (B, N, C) \u6279\u91cf\u70b9\u4e91\u6570\u636e\uff1a - B: batch size - N: \u6bcf\u5e27\u70b9\u7684\u6700\u5927\u6570\u91cf - C: \u70b9\u7279\u5f81\u7ef4\u5ea6\uff08\u81f3\u5c11\u5305\u542b x,y,z\uff0c\u901a\u5e38\u4e3a 4: [x,y,z,intensity]\uff09\u3002 voxel_size (tuple[float, float, float]): \u4f53\u7d20\u5927\u5c0f (vx, vy, vz)\uff0c\u5355\u4f4d\u4e0e\u70b9\u4e91\u5750\u6807\u4e00\u81f4\u3002 point_cloud_range (tuple[float, float, float, float, float, float]): \u70b9\u4e91\u8303\u56f4 [x_min, y_min, z_min, x_max, y_max, z_max]\u3002 max_points_per_voxel (int): \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u5141\u8bb8\u4fdd\u5b58\u7684\u6700\u5927\u70b9\u6570 T\u3002 max_voxels (int): \u6bcf\u5e27\u6700\u591a\u4fdd\u7559\u7684\u4f53\u7d20\u6570\u91cf M_max\u3002 --- ### \u8f93\u51fa Returns: voxels: (M, T, C) \u4f53\u7d20\u5185\u70b9\u7279\u5f81\u5f20\u91cf\uff1a - M: \u5b9e\u9645\u751f\u6210\u7684\u4f53\u7d20\u6570\u91cf\uff08M \u2264 B * max_voxels\uff09 - T: \u6bcf\u4e2a\u4f53\u7d20\u7684\u6700\u5927\u70b9\u6570\uff0c\u591a\u4f59\u7684\u70b9\u4f1a\u88ab\u4e22\u5f03\uff0c\u672a\u586b\u6ee1\u4f4d\u7f6e\u4e3a 0\u3002 coords: (M, 4) \u6bcf\u4e2a\u4f53\u7d20\u5728 3D \u7f51\u683c\u4e2d\u7684\u6574\u6570\u5750\u6807 [batch_idx, z, y, x]\u3002 num_points_per_voxel: (M,) \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u771f\u5b9e\u70b9\u6570\uff0c\u7528\u4e8e\u533a\u5206\u6709\u6548\u70b9\u4e0e padding\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u5c06\u7a20\u5bc6\u70b9\u4e91\u8f6c\u6362\u4e3a\u89c4\u5219\u4f53\u7d20\u8868\u793a\uff0c\u4f5c\u4e3a\u540e\u7eed VFE \u4e0e Sparse 3D Backbone \u7684\u8f93\u5165 - \u4f5c\u4e3a\u94fe\u8def \u201cLiDAR \u539f\u59cb\u70b9\u4e91 \u2192 LiDAR BEV \u7279\u5f81\u201d \u4e2d\u7684\u7b2c\u4e00\u4e2a\u79bb\u6563\u5316\u7b97\u5b50 - \u5728\u7b97\u5b50\u6846\u67b6\u4e2d\u7edf\u4e00\u5c01\u88c5\u4f53\u7d20\u5316\u903b\u8f91\uff0c\u4fbf\u4e8e\u66ff\u6362\u4e3a\u66f4\u9ad8\u6027\u80fd\u7684 C++/CUDA \u5b9e\u73b0 \"\"\" def __init__ ( self , voxel_size : tuple , point_cloud_range : tuple , max_points_per_voxel : int = 32 , max_voxels : int = 20000 , ): super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . voxel_size = voxel_size # \u8bb0\u5f55\u4f53\u7d20\u5927\u5c0f self . point_cloud_range = point_cloud_range # \u8bb0\u5f55\u70b9\u4e91\u8303\u56f4 self . max_points_per_voxel = max_points_per_voxel # \u6bcf\u4e2a\u4f53\u7d20\u6700\u5927\u70b9\u6570 self . max_voxels = max_voxels # \u6bcf\u5e27\u6700\u5927\u4f53\u7d20\u6570 # \u9884\u8ba1\u7b97\u7f51\u683c\u5927\u5c0f (nx, ny, nz) x_min , y_min , z_min , x_max , y_max , z_max = point_cloud_range # \u89e3\u5305\u8303\u56f4 vx , vy , vz = voxel_size # \u89e3\u5305\u4f53\u7d20\u5c3a\u5bf8 nx = int (( x_max - x_min ) / vx + 1e-6 ) # x \u65b9\u5411\u4f53\u7d20\u6570 ny = int (( y_max - y_min ) / vy + 1e-6 ) # y \u65b9\u5411\u4f53\u7d20\u6570 nz = int (( z_max - z_min ) / vz + 1e-6 ) # z \u65b9\u5411\u4f53\u7d20\u6570 self . grid_size = ( nx , ny , nz ) # \u4fdd\u5b58\u7f51\u683c\u5c3a\u5bf8 def forward ( self , points : torch . Tensor ): \"\"\" Args: points (torch.Tensor): \u6279\u91cf\u70b9\u4e91\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a (B, N, C)\u3002 # \u8f93\u5165\u70b9\u4e91 Returns: voxels (torch.Tensor): \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C)\u3002 # \u8f93\u51fa\u4f53\u7d20 coords (torch.Tensor): \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\u3002 # [batch, z, y, x] num_points_per_voxel (torch.Tensor): \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u70b9\u6570\u7edf\u8ba1 \"\"\" assert points . dim () == 3 , \"points \u5fc5\u987b\u662f (B, N, C) \u5f20\u91cf\" # \u57fa\u672c\u68c0\u67e5 B , N , C = points . shape # \u8bfb\u53d6\u7ef4\u5ea6 device = points . device # \u83b7\u53d6\u8bbe\u5907 vx , vy , vz = self . voxel_size # \u4f53\u7d20\u5927\u5c0f x_min , y_min , z_min , x_max , y_max , z_max = self . point_cloud_range # \u70b9\u4e91\u8303\u56f4 nx , ny , nz = self . grid_size # \u7f51\u683c\u5927\u5c0f # \u6700\u5927\u4f53\u7d20\u603b\u6570\uff1a\u6bcf\u5e27 max_voxels\uff0c\u5171 B \u5e27 max_total_voxels = B * self . max_voxels # \u4f53\u7d20\u603b\u4e0a\u9650 # \u9884\u5206\u914d\u8f93\u51fa\u5f20\u91cf voxels = torch . zeros ( ( max_total_voxels , self . max_points_per_voxel , C ), dtype = points . dtype , device = device , ) # \u4f53\u7d20\u70b9\u7279\u5f81 coords = torch . zeros ( ( max_total_voxels , 4 ), dtype = torch . int32 , device = device ) # \u4f53\u7d20\u5750\u6807 num_points_per_voxel = torch . zeros ( ( max_total_voxels ,), dtype = torch . int32 , device = device ) # \u6bcf\u4e2a\u4f53\u7d20\u70b9\u6570 voxel_count = 0 # \u5f53\u524d\u4f53\u7d20\u8ba1\u6570 voxel_map : dict = {} # (b,z,y,x) \u2192 voxel_id \u6620\u5c04 # \u904d\u5386 batch \u4e2d\u6bcf\u4e00\u5e27\u70b9\u4e91 for b in range ( B ): # \u904d\u5386\u6bcf\u4e2a batch pts = points [ b ] # (N,C) for n in range ( N ): # \u904d\u5386\u6bcf\u4e2a\u70b9 x , y , z = pts [ n , 0 ] . item (), pts [ n , 1 ] . item (), pts [ n , 2 ] . item () # \u8bfb\u53d6\u5750\u6807 # \u8fc7\u6ee4\u8d85\u51fa\u8303\u56f4\u7684\u70b9 if not ( x_min <= x < x_max and y_min <= y < y_max and z_min <= z < z_max ): continue # \u8df3\u8fc7\u65e0\u6548\u70b9 # \u8ba1\u7b97\u4f53\u7d20\u7d22\u5f15 ix = int (( x - x_min ) / vx ) iy = int (( y - y_min ) / vy ) iz = int (( z - z_min ) / vz ) if not ( 0 <= ix < nx and 0 <= iy < ny and 0 <= iz < nz ): continue # \u518d\u6b21\u5b89\u5168\u68c0\u67e5 key = ( b , iz , iy , ix ) # \u4f53\u7d20\u952e\u503c # \u4e3a\u65b0\u4f53\u7d20\u5206\u914d\u7d22\u5f15 if key not in voxel_map : if voxel_count >= max_total_voxels : # \u8d85\u51fa\u4f53\u7d20\u603b\u4e0a\u9650 continue voxel_map [ key ] = voxel_count # \u8bb0\u5f55\u6620\u5c04 coords [ voxel_count ] = torch . tensor ( [ b , iz , iy , ix ], dtype = torch . int32 , device = device ) # \u5199\u5165\u4f53\u7d20\u5750\u6807 voxel_count += 1 # \u4f53\u7d20\u8ba1\u6570\u52a0\u4e00 voxel_id = voxel_map [ key ] # \u83b7\u53d6\u4f53\u7d20\u7d22\u5f15 # \u82e5\u8be5\u4f53\u7d20\u5df2\u6ee1\uff0c\u5219\u4e22\u5f03\u591a\u4f59\u70b9 cur_num = num_points_per_voxel [ voxel_id ] . item () # \u5f53\u524d\u70b9\u6570 if cur_num >= self . max_points_per_voxel : continue voxels [ voxel_id , cur_num ] = pts [ n ] # \u5199\u5165\u70b9\u7279\u5f81 num_points_per_voxel [ voxel_id ] = cur_num + 1 # \u70b9\u6570\u52a0\u4e00 # \u622a\u53d6\u5230\u5b9e\u9645\u4f7f\u7528\u7684\u4f53\u7d20\u6570\u91cf voxels = voxels [: voxel_count ] # \u88c1\u526a\u591a\u4f59\u4f53\u7d20 coords = coords [: voxel_count ] # \u88c1\u526a\u5750\u6807 num_points_per_voxel = num_points_per_voxel [: voxel_count ] # \u88c1\u526a\u70b9\u6570 return voxels , coords , num_points_per_voxel # \u8fd4\u56de\u4f53\u7d20\u5316\u7ed3\u679c","title":"\u6a21\u5757\u7528\u9014"},{"location":"lidar_side/#lidar_side.network.LidarVoxelization.LidarVoxelization.forward","text":"Parameters: points ( Tensor ) \u2013 \u6279\u91cf\u70b9\u4e91\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a (B, N, C)\u3002 # \u8f93\u5165\u70b9\u4e91 Returns: voxels ( Tensor ) \u2013 \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C)\u3002 # \u8f93\u51fa\u4f53\u7d20 coords ( Tensor ) \u2013 \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\u3002 # [batch, z, y, x] num_points_per_voxel ( Tensor ) \u2013 \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u70b9\u6570\u7edf\u8ba1 Source code in lidar_side\\network\\LidarVoxelization.py 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def forward ( self , points : torch . Tensor ): \"\"\" Args: points (torch.Tensor): \u6279\u91cf\u70b9\u4e91\u5f20\u91cf\uff0c\u5f62\u72b6\u4e3a (B, N, C)\u3002 # \u8f93\u5165\u70b9\u4e91 Returns: voxels (torch.Tensor): \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C)\u3002 # \u8f93\u51fa\u4f53\u7d20 coords (torch.Tensor): \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\u3002 # [batch, z, y, x] num_points_per_voxel (torch.Tensor): \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u70b9\u6570\u7edf\u8ba1 \"\"\" assert points . dim () == 3 , \"points \u5fc5\u987b\u662f (B, N, C) \u5f20\u91cf\" # \u57fa\u672c\u68c0\u67e5 B , N , C = points . shape # \u8bfb\u53d6\u7ef4\u5ea6 device = points . device # \u83b7\u53d6\u8bbe\u5907 vx , vy , vz = self . voxel_size # \u4f53\u7d20\u5927\u5c0f x_min , y_min , z_min , x_max , y_max , z_max = self . point_cloud_range # \u70b9\u4e91\u8303\u56f4 nx , ny , nz = self . grid_size # \u7f51\u683c\u5927\u5c0f # \u6700\u5927\u4f53\u7d20\u603b\u6570\uff1a\u6bcf\u5e27 max_voxels\uff0c\u5171 B \u5e27 max_total_voxels = B * self . max_voxels # \u4f53\u7d20\u603b\u4e0a\u9650 # \u9884\u5206\u914d\u8f93\u51fa\u5f20\u91cf voxels = torch . zeros ( ( max_total_voxels , self . max_points_per_voxel , C ), dtype = points . dtype , device = device , ) # \u4f53\u7d20\u70b9\u7279\u5f81 coords = torch . zeros ( ( max_total_voxels , 4 ), dtype = torch . int32 , device = device ) # \u4f53\u7d20\u5750\u6807 num_points_per_voxel = torch . zeros ( ( max_total_voxels ,), dtype = torch . int32 , device = device ) # \u6bcf\u4e2a\u4f53\u7d20\u70b9\u6570 voxel_count = 0 # \u5f53\u524d\u4f53\u7d20\u8ba1\u6570 voxel_map : dict = {} # (b,z,y,x) \u2192 voxel_id \u6620\u5c04 # \u904d\u5386 batch \u4e2d\u6bcf\u4e00\u5e27\u70b9\u4e91 for b in range ( B ): # \u904d\u5386\u6bcf\u4e2a batch pts = points [ b ] # (N,C) for n in range ( N ): # \u904d\u5386\u6bcf\u4e2a\u70b9 x , y , z = pts [ n , 0 ] . item (), pts [ n , 1 ] . item (), pts [ n , 2 ] . item () # \u8bfb\u53d6\u5750\u6807 # \u8fc7\u6ee4\u8d85\u51fa\u8303\u56f4\u7684\u70b9 if not ( x_min <= x < x_max and y_min <= y < y_max and z_min <= z < z_max ): continue # \u8df3\u8fc7\u65e0\u6548\u70b9 # \u8ba1\u7b97\u4f53\u7d20\u7d22\u5f15 ix = int (( x - x_min ) / vx ) iy = int (( y - y_min ) / vy ) iz = int (( z - z_min ) / vz ) if not ( 0 <= ix < nx and 0 <= iy < ny and 0 <= iz < nz ): continue # \u518d\u6b21\u5b89\u5168\u68c0\u67e5 key = ( b , iz , iy , ix ) # \u4f53\u7d20\u952e\u503c # \u4e3a\u65b0\u4f53\u7d20\u5206\u914d\u7d22\u5f15 if key not in voxel_map : if voxel_count >= max_total_voxels : # \u8d85\u51fa\u4f53\u7d20\u603b\u4e0a\u9650 continue voxel_map [ key ] = voxel_count # \u8bb0\u5f55\u6620\u5c04 coords [ voxel_count ] = torch . tensor ( [ b , iz , iy , ix ], dtype = torch . int32 , device = device ) # \u5199\u5165\u4f53\u7d20\u5750\u6807 voxel_count += 1 # \u4f53\u7d20\u8ba1\u6570\u52a0\u4e00 voxel_id = voxel_map [ key ] # \u83b7\u53d6\u4f53\u7d20\u7d22\u5f15 # \u82e5\u8be5\u4f53\u7d20\u5df2\u6ee1\uff0c\u5219\u4e22\u5f03\u591a\u4f59\u70b9 cur_num = num_points_per_voxel [ voxel_id ] . item () # \u5f53\u524d\u70b9\u6570 if cur_num >= self . max_points_per_voxel : continue voxels [ voxel_id , cur_num ] = pts [ n ] # \u5199\u5165\u70b9\u7279\u5f81 num_points_per_voxel [ voxel_id ] = cur_num + 1 # \u70b9\u6570\u52a0\u4e00 # \u622a\u53d6\u5230\u5b9e\u9645\u4f7f\u7528\u7684\u4f53\u7d20\u6570\u91cf voxels = voxels [: voxel_count ] # \u88c1\u526a\u591a\u4f59\u4f53\u7d20 coords = coords [: voxel_count ] # \u88c1\u526a\u5750\u6807 num_points_per_voxel = num_points_per_voxel [: voxel_count ] # \u88c1\u526a\u70b9\u6570 return voxels , coords , num_points_per_voxel # \u8fd4\u56de\u4f53\u7d20\u5316\u7ed3\u679c","title":"forward"},{"location":"lidar_side/#voxelfeatureencoder","text":"\u5bf9\u4f53\u7d20\u4e2d\u7684\u70b9\u4e91\u8fdb\u884c\u7279\u5f81\u7f16\u7801\u3002","title":"'VoxelFeatureEncoder'"},{"location":"lidar_side/#lidar_side.network.VoxelFeatureEncoder.VoxelFeatureEncoder","text":"Bases: Module VoxelFeatureEncoder \u6a21\u5757\uff08\u4f53\u7d20\u7279\u5f81\u7f16\u7801\u7b97\u5b50\uff0cVFE\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u4f53\u7d20\u5316\u540e\u7684\u70b9\u4e91\u6570\u636e\uff0c\u901a\u8fc7\u9010\u70b9 MLP \u7f16\u7801\u5e76\u5728\u4f53\u7d20\u5185\u90e8\u505a \u805a\u5408\uff08\u5982 max pooling\uff09\uff0c\u5c06\u6bcf\u4e2a\u4f53\u7d20\u5185\u4e0d\u5b9a\u6570\u91cf\u7684\u70b9\u7279\u5f81\u538b\u7f29\u4e3a\u4e00\u4e2a \u5b9a\u957f\u5411\u91cf\uff0c\u7528\u4f5c\u540e\u7eed\u7a00\u758f 3D \u4e3b\u5e72\u7f51\u7edc\u7684\u8f93\u5165\u3002","title":"VoxelFeatureEncoder"},{"location":"lidar_side/#lidar_side.network.VoxelFeatureEncoder.VoxelFeatureEncoder--_1","text":"\u5bf9\u4f53\u7d20\u5185\u6bcf\u4e2a\u70b9\u505a\u9010\u70b9\u7279\u5f81\u7f16\u7801\uff1a Linear C_in \u2192 C_mid + BatchNorm1d + ReLU \u5728\u4f53\u7d20\u5185\u90e8\u6cbf\u70b9\u7ef4\u5ea6\u505a max pooling \uff1a \u5c06 (M, T, C_mid) \u805a\u5408\u4e3a (M, C_mid) \u53ef\u9009\u518d\u63a5\u4e00\u5c42\u7ebf\u6027\u53d8\u6362\uff1a Linear C_mid \u2192 C_out + BatchNorm1d + ReLU","title":"\u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09"},{"location":"lidar_side/#lidar_side.network.VoxelFeatureEncoder.VoxelFeatureEncoder--_2","text":"Args: voxels: (M, T, C_in) \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff1a - M: \u4f53\u7d20\u6570\u91cf - T: \u6bcf\u4e2a\u4f53\u7d20\u7684\u6700\u5927\u70b9\u6570 - C_in: \u70b9\u7279\u5f81\u7ef4\u5ea6\uff08\u5982 [x,y,z,intensity,...]\uff09\u3002 num_points_per_voxel: (M,) \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u771f\u5b9e\u70b9\u6570\uff0c\u7528\u4e8e\u533a\u5206\u6709\u6548\u70b9\u4e0e padding\u3002","title":"\u8f93\u5165"},{"location":"lidar_side/#lidar_side.network.VoxelFeatureEncoder.VoxelFeatureEncoder--_3","text":"Returns: voxel_feats: (M, C_out) \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u6bcf\u4e2a\u4f53\u7d20\u5bf9\u5e94\u4e00\u4e2a\u5b9a\u957f\u5411\u91cf\u3002","title":"\u8f93\u51fa"},{"location":"lidar_side/#lidar_side.network.VoxelFeatureEncoder.VoxelFeatureEncoder--_4","text":"\u5c06\u4e0d\u5b9a\u70b9\u6570\u7684\u4f53\u7d20\u5185\u70b9\u4e91\u538b\u7f29\u4e3a\u56fa\u5b9a\u7ef4\u5ea6\u7279\u5f81 \u4f5c\u4e3a\u94fe\u8def \u201cLiDAR \u4f53\u7d20\u5316 \u2192 \u7a00\u758f 3D \u4e3b\u5e72\u7f51\u7edc\u201d \u7684\u63a5\u53e3\u7b97\u5b50 \u53ef\u4ee5\u66ff\u6362\u4e3a\u66f4\u590d\u6742\u7684\u591a\u5c42 VFE \u6216\u5e26\u6b8b\u5dee\u7684\u4f53\u7d20\u7f16\u7801\u5668 Source code in lidar_side\\network\\VoxelFeatureEncoder.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class VoxelFeatureEncoder ( nn . Module ): \"\"\" VoxelFeatureEncoder \u6a21\u5757\uff08\u4f53\u7d20\u7279\u5f81\u7f16\u7801\u7b97\u5b50\uff0cVFE\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u4f53\u7d20\u5316\u540e\u7684\u70b9\u4e91\u6570\u636e\uff0c\u901a\u8fc7\u9010\u70b9 MLP \u7f16\u7801\u5e76\u5728\u4f53\u7d20\u5185\u90e8\u505a \u805a\u5408\uff08\u5982 max pooling\uff09\uff0c\u5c06\u6bcf\u4e2a\u4f53\u7d20\u5185\u4e0d\u5b9a\u6570\u91cf\u7684\u70b9\u7279\u5f81\u538b\u7f29\u4e3a\u4e00\u4e2a \u5b9a\u957f\u5411\u91cf\uff0c\u7528\u4f5c\u540e\u7eed\u7a00\u758f 3D \u4e3b\u5e72\u7f51\u7edc\u7684\u8f93\u5165\u3002 --- ### \u7ed3\u6784\u8bf4\u660e\uff08\u6309\u987a\u5e8f\uff09 1. \u5bf9\u4f53\u7d20\u5185\u6bcf\u4e2a\u70b9\u505a\u9010\u70b9\u7279\u5f81\u7f16\u7801\uff1a **Linear C_in \u2192 C_mid + BatchNorm1d + ReLU** 2. \u5728\u4f53\u7d20\u5185\u90e8\u6cbf\u70b9\u7ef4\u5ea6\u505a **max pooling**\uff1a \u5c06 (M, T, C_mid) \u805a\u5408\u4e3a (M, C_mid) 3. \u53ef\u9009\u518d\u63a5\u4e00\u5c42\u7ebf\u6027\u53d8\u6362\uff1a **Linear C_mid \u2192 C_out + BatchNorm1d + ReLU** --- ### \u8f93\u5165 Args: voxels: (M, T, C_in) \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff1a - M: \u4f53\u7d20\u6570\u91cf - T: \u6bcf\u4e2a\u4f53\u7d20\u7684\u6700\u5927\u70b9\u6570 - C_in: \u70b9\u7279\u5f81\u7ef4\u5ea6\uff08\u5982 [x,y,z,intensity,...]\uff09\u3002 num_points_per_voxel: (M,) \u6bcf\u4e2a\u4f53\u7d20\u4e2d\u771f\u5b9e\u70b9\u6570\uff0c\u7528\u4e8e\u533a\u5206\u6709\u6548\u70b9\u4e0e padding\u3002 --- ### \u8f93\u51fa Returns: voxel_feats: (M, C_out) \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u6bcf\u4e2a\u4f53\u7d20\u5bf9\u5e94\u4e00\u4e2a\u5b9a\u957f\u5411\u91cf\u3002 --- ### \u6a21\u5757\u7528\u9014 - \u5c06\u4e0d\u5b9a\u70b9\u6570\u7684\u4f53\u7d20\u5185\u70b9\u4e91\u538b\u7f29\u4e3a\u56fa\u5b9a\u7ef4\u5ea6\u7279\u5f81 - \u4f5c\u4e3a\u94fe\u8def \u201cLiDAR \u4f53\u7d20\u5316 \u2192 \u7a00\u758f 3D \u4e3b\u5e72\u7f51\u7edc\u201d \u7684\u63a5\u53e3\u7b97\u5b50 - \u53ef\u4ee5\u66ff\u6362\u4e3a\u66f4\u590d\u6742\u7684\u591a\u5c42 VFE \u6216\u5e26\u6b8b\u5dee\u7684\u4f53\u7d20\u7f16\u7801\u5668 \"\"\" def __init__ ( self , in_channels : int , mid_channels : int , out_channels : int , use_second_linear : bool = True , ): \"\"\" Args: in_channels (int): \u8f93\u5165\u70b9\u7279\u5f81\u7ef4\u5ea6 C_in\u3002 # \u70b9\u7279\u5f81\u901a\u9053 mid_channels (int): \u4e2d\u95f4\u7f16\u7801\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels (int): \u8f93\u51fa\u4f53\u7d20\u7279\u5f81\u7ef4\u5ea6 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_second_linear (bool): \u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 Linear+CBN+ReLU\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 MLP \"\"\" super () . __init__ () # \u7236\u7c7b\u6784\u9020 # \u7b2c\u4e00\u5c42\u9010\u70b9\u7f16\u7801\uff1aLinear + BN + ReLU self . linear1 = nn . Linear ( in_channels , mid_channels , bias = False ) # \u7ebf\u6027\u6620\u5c04 self . bn1 = nn . BatchNorm1d ( mid_channels ) # BN self . use_second_linear = use_second_linear # \u8bb0\u5f55\u914d\u7f6e if use_second_linear : # \u7b2c\u4e8c\u5c42\u9010\u70b9 / \u9010\u4f53\u7d20\u7f16\u7801 self . linear2 = nn . Linear ( mid_channels , out_channels , bias = False ) # \u7ebf\u6027\u6620\u5c04 self . bn2 = nn . BatchNorm1d ( out_channels ) # BN self . out_channels = out_channels # \u8f93\u51fa\u7ef4\u5ea6 else : self . out_channels = mid_channels # \u8f93\u51fa\u7ef4\u5ea6\u7b49\u4e8e\u4e2d\u95f4\u5c42 def forward ( self , voxels : torch . Tensor , num_points_per_voxel : torch . Tensor , ) -> torch . Tensor : \"\"\" Args: voxels (torch.Tensor): \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C_in)\u3002 # \u8f93\u5165\u4f53\u7d20 num_points_per_voxel (torch.Tensor): \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u6709\u6548\u70b9\u6570 Returns: torch.Tensor: \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_out)\u3002 # \u8f93\u51fa\u4f53\u7d20\u7279\u5f81 \"\"\" assert voxels . dim () == 3 , \"voxels \u5fc5\u987b\u662f (M, T, C_in) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 assert num_points_per_voxel . dim () == 1 , \"num_points_per_voxel \u5fc5\u987b\u662f\u4e00\u7ef4\u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 M , T , C_in = voxels . shape # \u8bfb\u53d6\u5c3a\u5bf8 x = voxels . reshape ( M * T , C_in ) # \u5408\u5e76\u4f53\u7d20\u548c\u70b9\u7ef4\u5ea6 # \u9010\u70b9\u7f16\u7801\uff1aLinear1 + BN1 + ReLU x = self . linear1 ( x ) # \u7ebf\u6027\u53d8\u6362 x = self . bn1 ( x ) # BN x = F . relu ( x , inplace = True ) # \u6fc0\u6d3b x = x . reshape ( M , T , - 1 ) # \u6062\u590d\u4e3a (M,T,C_mid) # \u4f53\u7d20\u5185\u90e8 max pooling\uff08\u63a9\u7801\u65e0\u6548\u70b9\uff09 mask = ( torch . arange ( T , device = voxels . device )[ None , :] . expand ( M , T ) < num_points_per_voxel . unsqueeze ( 1 ) ) # \u6709\u6548\u70b9\u63a9\u7801 (M,T) mask = mask . unsqueeze ( - 1 ) # (M,T,1) x_masked = x . masked_fill ( ~ mask , float ( \"-inf\" )) # \u65e0\u6548\u4f4d\u7f6e\u7f6e\u4e3a -inf voxel_feats , _ = x_masked . max ( dim = 1 ) # \u6cbf\u70b9\u7ef4\u5ea6\u53d6\u6700\u5927\u503c \u2192 (M,C_mid) voxel_feats [ voxel_feats == float ( \"-inf\" )] = 0.0 # \u5904\u7406\u5168\u65e0\u6548\u60c5\u51b5 if self . use_second_linear : # \u7b2c\u4e8c\u5c42\u7f16\u7801\uff1aLinear2 + BN2 + ReLU voxel_feats = self . linear2 ( voxel_feats ) # \u7ebf\u6027\u53d8\u6362 voxel_feats = self . bn2 ( voxel_feats ) # BN voxel_feats = F . relu ( voxel_feats , inplace = True ) # \u6fc0\u6d3b return voxel_feats # \u8fd4\u56de\u4f53\u7d20\u7279\u5f81","title":"\u6a21\u5757\u7528\u9014"},{"location":"lidar_side/#lidar_side.network.VoxelFeatureEncoder.VoxelFeatureEncoder.__init__","text":"Parameters: in_channels ( int ) \u2013 \u8f93\u5165\u70b9\u7279\u5f81\u7ef4\u5ea6 C_in\u3002 # \u70b9\u7279\u5f81\u901a\u9053 mid_channels ( int ) \u2013 \u4e2d\u95f4\u7f16\u7801\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels ( int ) \u2013 \u8f93\u51fa\u4f53\u7d20\u7279\u5f81\u7ef4\u5ea6 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_second_linear ( bool , default: True ) \u2013 \u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 Linear+CBN+ReLU\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 MLP Source code in lidar_side\\network\\VoxelFeatureEncoder.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 def __init__ ( self , in_channels : int , mid_channels : int , out_channels : int , use_second_linear : bool = True , ): \"\"\" Args: in_channels (int): \u8f93\u5165\u70b9\u7279\u5f81\u7ef4\u5ea6 C_in\u3002 # \u70b9\u7279\u5f81\u901a\u9053 mid_channels (int): \u4e2d\u95f4\u7f16\u7801\u901a\u9053\u6570 C_mid\u3002 # \u4e2d\u95f4\u901a\u9053 out_channels (int): \u8f93\u51fa\u4f53\u7d20\u7279\u5f81\u7ef4\u5ea6 C_out\u3002 # \u8f93\u51fa\u901a\u9053 use_second_linear (bool): \u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 Linear+CBN+ReLU\u3002 # \u63a7\u5236\u662f\u5426\u4f7f\u7528\u7b2c\u4e8c\u5c42 MLP \"\"\" super () . __init__ () # \u7236\u7c7b\u6784\u9020 # \u7b2c\u4e00\u5c42\u9010\u70b9\u7f16\u7801\uff1aLinear + BN + ReLU self . linear1 = nn . Linear ( in_channels , mid_channels , bias = False ) # \u7ebf\u6027\u6620\u5c04 self . bn1 = nn . BatchNorm1d ( mid_channels ) # BN self . use_second_linear = use_second_linear # \u8bb0\u5f55\u914d\u7f6e if use_second_linear : # \u7b2c\u4e8c\u5c42\u9010\u70b9 / \u9010\u4f53\u7d20\u7f16\u7801 self . linear2 = nn . Linear ( mid_channels , out_channels , bias = False ) # \u7ebf\u6027\u6620\u5c04 self . bn2 = nn . BatchNorm1d ( out_channels ) # BN self . out_channels = out_channels # \u8f93\u51fa\u7ef4\u5ea6 else : self . out_channels = mid_channels # \u8f93\u51fa\u7ef4\u5ea6\u7b49\u4e8e\u4e2d\u95f4\u5c42","title":"__init__"},{"location":"lidar_side/#lidar_side.network.VoxelFeatureEncoder.VoxelFeatureEncoder.forward","text":"Parameters: voxels ( Tensor ) \u2013 \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C_in)\u3002 # \u8f93\u5165\u4f53\u7d20 num_points_per_voxel ( Tensor ) \u2013 \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u6709\u6548\u70b9\u6570 Returns: Tensor \u2013 torch.Tensor: \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_out)\u3002 # \u8f93\u51fa\u4f53\u7d20\u7279\u5f81 Source code in lidar_side\\network\\VoxelFeatureEncoder.py 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 def forward ( self , voxels : torch . Tensor , num_points_per_voxel : torch . Tensor , ) -> torch . Tensor : \"\"\" Args: voxels (torch.Tensor): \u4f53\u7d20\u5185\u70b9\u7279\u5f81\uff0c\u5f62\u72b6 (M, T, C_in)\u3002 # \u8f93\u5165\u4f53\u7d20 num_points_per_voxel (torch.Tensor): \u6bcf\u4e2a\u4f53\u7d20\u771f\u5b9e\u70b9\u6570\uff0c\u5f62\u72b6 (M,)\u3002 # \u6709\u6548\u70b9\u6570 Returns: torch.Tensor: \u7f16\u7801\u540e\u7684\u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_out)\u3002 # \u8f93\u51fa\u4f53\u7d20\u7279\u5f81 \"\"\" assert voxels . dim () == 3 , \"voxels \u5fc5\u987b\u662f (M, T, C_in) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 assert num_points_per_voxel . dim () == 1 , \"num_points_per_voxel \u5fc5\u987b\u662f\u4e00\u7ef4\u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 M , T , C_in = voxels . shape # \u8bfb\u53d6\u5c3a\u5bf8 x = voxels . reshape ( M * T , C_in ) # \u5408\u5e76\u4f53\u7d20\u548c\u70b9\u7ef4\u5ea6 # \u9010\u70b9\u7f16\u7801\uff1aLinear1 + BN1 + ReLU x = self . linear1 ( x ) # \u7ebf\u6027\u53d8\u6362 x = self . bn1 ( x ) # BN x = F . relu ( x , inplace = True ) # \u6fc0\u6d3b x = x . reshape ( M , T , - 1 ) # \u6062\u590d\u4e3a (M,T,C_mid) # \u4f53\u7d20\u5185\u90e8 max pooling\uff08\u63a9\u7801\u65e0\u6548\u70b9\uff09 mask = ( torch . arange ( T , device = voxels . device )[ None , :] . expand ( M , T ) < num_points_per_voxel . unsqueeze ( 1 ) ) # \u6709\u6548\u70b9\u63a9\u7801 (M,T) mask = mask . unsqueeze ( - 1 ) # (M,T,1) x_masked = x . masked_fill ( ~ mask , float ( \"-inf\" )) # \u65e0\u6548\u4f4d\u7f6e\u7f6e\u4e3a -inf voxel_feats , _ = x_masked . max ( dim = 1 ) # \u6cbf\u70b9\u7ef4\u5ea6\u53d6\u6700\u5927\u503c \u2192 (M,C_mid) voxel_feats [ voxel_feats == float ( \"-inf\" )] = 0.0 # \u5904\u7406\u5168\u65e0\u6548\u60c5\u51b5 if self . use_second_linear : # \u7b2c\u4e8c\u5c42\u7f16\u7801\uff1aLinear2 + BN2 + ReLU voxel_feats = self . linear2 ( voxel_feats ) # \u7ebf\u6027\u53d8\u6362 voxel_feats = self . bn2 ( voxel_feats ) # BN voxel_feats = F . relu ( voxel_feats , inplace = True ) # \u6fc0\u6d3b return voxel_feats # \u8fd4\u56de\u4f53\u7d20\u7279\u5f81","title":"forward"},{"location":"lidar_side/#sparse3dbackbone","text":"\u57fa\u4e8e\u7a00\u758f\u5377\u79ef\u7684 3D \u7279\u5f81\u63d0\u53d6\u7f51\u7edc\u3002","title":"'Sparse3DBackbone'"},{"location":"lidar_side/#lidar_side.network.Sparse3DBackbone.Sparse3DBackbone","text":"Bases: Module Sparse3DBackbone \u6a21\u5757\uff08\u7a00\u758f\u4e09\u7ef4\u4e3b\u5e72\u7f51\u7edc\u7b97\u5b50 / SECOND \u98ce\u683c\u7b80\u5316\u7248\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u4f53\u7d20\u7279\u5f81\u53ca\u5176\u4e09\u7ef4\u5750\u6807\uff0c\u5c06\u7a00\u758f\u4f53\u7d20\u7279\u5f81\u5d4c\u5165\u5230\u89c4\u5219 3D \u4f53\u7d20\u7f51\u683c\u4e2d\uff0c \u518d\u901a\u8fc7 3D \u5377\u79ef\u5bf9\u6574\u4e2a\u4f53\u7d20\u7f51\u683c\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (B, C_out, Z, Y, X) \u7684\u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u4f9b\u540e\u7eed\u9ad8\u5ea6\u7ef4\u538b\u7f29\u7b97\u5b50\u4f7f\u7528\u3002 \u6ce8\u610f\uff1a\u672c\u5b9e\u73b0\u4e3a\u201c\u7a20\u5bc6 3D \u5377\u79ef\u7248\u672c\u201d\u7684\u7b80\u5316\u9aa8\u67b6\uff0c\u7528\u4e8e\u7b97\u5b50\u7ea7\u91cd\u6784\u4e0e\u5b9e\u9a8c Demo\uff0c \u5e76\u975e\u771f\u6b63\u7684\u9ad8\u6027\u80fd SparseConv \u5b9e\u73b0\u3002 Parameters: in_channels ( int ) \u2013 int \u8f93\u5165\u4f53\u7d20\u7279\u5f81\u901a\u9053\u6570 C_in\u3002 hidden_channels ( int ) \u2013 int \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\uff0c\u7528\u4e8e\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002 out_channels ( int ) \u2013 int \u8f93\u51fa\u4e09\u7ef4\u7279\u5f81\u901a\u9053\u6570 C_out\u3002 grid_size ( Tuple [ int , int , int ] ) \u2013 tuple[int, int, int] \u4e09\u7ef4\u4f53\u7d20\u7f51\u683c\u5c3a\u5bf8 (Z, Y, X)\uff0c\u4e0e\u4f53\u7d20\u5316\u9636\u6bb5\u4f7f\u7528\u7684\u7f51\u683c\u5927\u5c0f\u4e00\u81f4\u3002 Returns: feat_3d \u2013 (B, C_out, Z, Y, X) \u7f16\u7801\u540e\u7684\u4e09\u7ef4\u4f53\u7d20\u7279\u5f81\u4f53\uff0c\u7528\u4e8e\u540e\u7eed 3D\u2192BEV \u538b\u7f29\u7b97\u5b50\u3002 Source code in lidar_side\\network\\Sparse3DBackbone.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 class Sparse3DBackbone ( nn . Module ): \"\"\" Sparse3DBackbone \u6a21\u5757\uff08\u7a00\u758f\u4e09\u7ef4\u4e3b\u5e72\u7f51\u7edc\u7b97\u5b50 / SECOND \u98ce\u683c\u7b80\u5316\u7248\uff09\u3002 \u8be5\u6a21\u5757\u63a5\u6536\u4f53\u7d20\u7279\u5f81\u53ca\u5176\u4e09\u7ef4\u5750\u6807\uff0c\u5c06\u7a00\u758f\u4f53\u7d20\u7279\u5f81\u5d4c\u5165\u5230\u89c4\u5219 3D \u4f53\u7d20\u7f51\u683c\u4e2d\uff0c \u518d\u901a\u8fc7 3D \u5377\u79ef\u5bf9\u6574\u4e2a\u4f53\u7d20\u7f51\u683c\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (B, C_out, Z, Y, X) \u7684\u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u4f9b\u540e\u7eed\u9ad8\u5ea6\u7ef4\u538b\u7f29\u7b97\u5b50\u4f7f\u7528\u3002 \u6ce8\u610f\uff1a\u672c\u5b9e\u73b0\u4e3a\u201c\u7a20\u5bc6 3D \u5377\u79ef\u7248\u672c\u201d\u7684\u7b80\u5316\u9aa8\u67b6\uff0c\u7528\u4e8e\u7b97\u5b50\u7ea7\u91cd\u6784\u4e0e\u5b9e\u9a8c Demo\uff0c \u5e76\u975e\u771f\u6b63\u7684\u9ad8\u6027\u80fd SparseConv \u5b9e\u73b0\u3002 Args: in_channels: int \u8f93\u5165\u4f53\u7d20\u7279\u5f81\u901a\u9053\u6570 C_in\u3002 hidden_channels: int \u4e2d\u95f4\u5c42\u901a\u9053\u6570 C_mid\uff0c\u7528\u4e8e\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\u3002 out_channels: int \u8f93\u51fa\u4e09\u7ef4\u7279\u5f81\u901a\u9053\u6570 C_out\u3002 grid_size: tuple[int, int, int] \u4e09\u7ef4\u4f53\u7d20\u7f51\u683c\u5c3a\u5bf8 (Z, Y, X)\uff0c\u4e0e\u4f53\u7d20\u5316\u9636\u6bb5\u4f7f\u7528\u7684\u7f51\u683c\u5927\u5c0f\u4e00\u81f4\u3002 Returns: feat_3d: (B, C_out, Z, Y, X) \u7f16\u7801\u540e\u7684\u4e09\u7ef4\u4f53\u7d20\u7279\u5f81\u4f53\uff0c\u7528\u4e8e\u540e\u7eed 3D\u2192BEV \u538b\u7f29\u7b97\u5b50\u3002 \"\"\" def __init__ ( self , in_channels : int , hidden_channels : int , out_channels : int , grid_size : Tuple [ int , int , int ], ): super () . __init__ () # \u8c03\u7528\u7236\u7c7b\u6784\u9020 self . in_channels = in_channels # \u8f93\u5165\u901a\u9053\u6570 self . hidden_channels = hidden_channels # \u4e2d\u95f4\u901a\u9053\u6570 self . out_channels = out_channels # \u8f93\u51fa\u901a\u9053\u6570 self . grid_size = grid_size # (Z, Y, X) \u7f51\u683c\u5927\u5c0f # \u7b80\u5316\u7248 3D \u4e3b\u5e72\uff1a\u4e24\u5c42 3D \u5377\u79ef + BN + ReLU self . conv_block1 = nn . Sequential ( nn . Conv3d ( in_channels , hidden_channels , kernel_size = 3 , padding = 1 , bias = False ), # 3D \u5377\u79ef nn . BatchNorm3d ( hidden_channels ), # BN nn . ReLU ( inplace = True ), # \u6fc0\u6d3b ) self . conv_block2 = nn . Sequential ( nn . Conv3d ( hidden_channels , out_channels , kernel_size = 3 , padding = 1 , bias = False ), # 3D \u5377\u79ef nn . BatchNorm3d ( out_channels ), # BN nn . ReLU ( inplace = True ), # \u6fc0\u6d3b ) def forward ( self , voxel_feats : torch . Tensor , voxel_coords : torch . Tensor , batch_size : int , ) -> torch . Tensor : \"\"\" Args: voxel_feats (torch.Tensor): \u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_in)\u3002 # \u6bcf\u4e2a\u4f53\u7d20\u4e00\u4e2a\u7279\u5f81\u5411\u91cf voxel_coords (torch.Tensor): \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\uff0c\u683c\u5f0f\u4e3a [batch_idx, z, y, x]\u3002 # \u4f53\u7d20\u5728\u7f51\u683c\u4e2d\u7684\u7d22\u5f15 batch_size (int): \u5f53\u524d\u6279\u6b21\u5927\u5c0f B\u3002 # batch \u6570\u91cf Returns: torch.Tensor: \u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u5f62\u72b6 (B, C_out, Z, Y, X)\u3002 # \u7a20\u5bc6 3D \u7279\u5f81 \"\"\" assert voxel_feats . dim () == 2 , \"voxel_feats \u5fc5\u987b\u662f (M, C_in) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 assert voxel_coords . dim () == 2 and voxel_coords . shape [ 1 ] == 4 , \"voxel_coords \u5fc5\u987b\u662f (M, 4) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 device = voxel_feats . device # \u83b7\u53d6\u8bbe\u5907 M , C_in = voxel_feats . shape # \u8bfb\u53d6\u4f53\u7d20\u6570\u91cf\u4e0e\u901a\u9053\u6570 Z , Y , X = self . grid_size # \u89e3\u5305\u7f51\u683c\u5c3a\u5bf8 # \u521d\u59cb\u5316\u7a20\u5bc6\u4f53\u7d20\u7f51\u683c\uff1a (B, C_in, Z, Y, X) dense_grid = torch . zeros ( batch_size , C_in , Z , Y , X , dtype = voxel_feats . dtype , device = device , ) # \u521d\u59cb\u5316 3D \u4f53\u7d20\u7f51\u683c # \u5c06\u7a00\u758f\u4f53\u7d20\u7279\u5f81 scatter \u5230\u7a20\u5bc6\u4f53\u7d20\u7f51\u683c\u4e2d b = voxel_coords [:, 0 ] . long () # batch \u7d22\u5f15 z = voxel_coords [:, 1 ] . long () # z \u8f74\u7d22\u5f15 y = voxel_coords [:, 2 ] . long () # y \u8f74\u7d22\u5f15 x = voxel_coords [:, 3 ] . long () # x \u8f74\u7d22\u5f15 # \u8fc7\u6ee4\u8d8a\u754c\u4f53\u7d20\uff08\u7406\u8bba\u4e0a voxelization \u5df2\u7ecf\u4fdd\u8bc1\u5408\u6cd5\uff0c\u8fd9\u91cc\u518d\u505a\u4e00\u6b21\u5b89\u5168\u68c0\u67e5\uff09 valid_mask = ( ( b >= 0 ) & ( b < batch_size ) & ( z >= 0 ) & ( z < Z ) & ( y >= 0 ) & ( y < Y ) & ( x >= 0 ) & ( x < X ) ) # \u5408\u6cd5\u4f53\u7d20\u63a9\u7801 if valid_mask . any (): # \u82e5\u5b58\u5728\u5408\u6cd5\u4f53\u7d20 b_valid = b [ valid_mask ] # \u6709\u6548 batch \u7d22\u5f15 z_valid = z [ valid_mask ] # \u6709\u6548 z \u7d22\u5f15 y_valid = y [ valid_mask ] # \u6709\u6548 y \u7d22\u5f15 x_valid = x [ valid_mask ] # \u6709\u6548 x \u7d22\u5f15 feats_valid = voxel_feats [ valid_mask ] # \u6709\u6548\u4f53\u7d20\u7279\u5f81 (M_valid,C_in) dense_grid [ b_valid , :, z_valid , y_valid , x_valid ] = feats_valid # \u586b\u5145\u5230\u7a20\u5bc6\u7f51\u683c\u4e2d # \u901a\u8fc7 3D \u5377\u79ef\u4e3b\u5e72\u63d0\u53d6\u4e09\u7ef4\u7279\u5f81 x3d = self . conv_block1 ( dense_grid ) # \u7b2c\u4e00\u5c42 3D \u5377\u79ef\u5757 x3d = self . conv_block2 ( x3d ) # \u7b2c\u4e8c\u5c42 3D \u5377\u79ef\u5757 return x3d # \u8fd4\u56de\u4e09\u7ef4\u7279\u5f81\u4f53","title":"Sparse3DBackbone"},{"location":"lidar_side/#lidar_side.network.Sparse3DBackbone.Sparse3DBackbone.forward","text":"Parameters: voxel_feats ( Tensor ) \u2013 \u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_in)\u3002 # \u6bcf\u4e2a\u4f53\u7d20\u4e00\u4e2a\u7279\u5f81\u5411\u91cf voxel_coords ( Tensor ) \u2013 \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\uff0c\u683c\u5f0f\u4e3a [batch_idx, z, y, x]\u3002 # \u4f53\u7d20\u5728\u7f51\u683c\u4e2d\u7684\u7d22\u5f15 batch_size ( int ) \u2013 \u5f53\u524d\u6279\u6b21\u5927\u5c0f B\u3002 # batch \u6570\u91cf Returns: Tensor \u2013 torch.Tensor: \u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u5f62\u72b6 (B, C_out, Z, Y, X)\u3002 # \u7a20\u5bc6 3D \u7279\u5f81 Source code in lidar_side\\network\\Sparse3DBackbone.py 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 def forward ( self , voxel_feats : torch . Tensor , voxel_coords : torch . Tensor , batch_size : int , ) -> torch . Tensor : \"\"\" Args: voxel_feats (torch.Tensor): \u4f53\u7d20\u7279\u5f81\uff0c\u5f62\u72b6 (M, C_in)\u3002 # \u6bcf\u4e2a\u4f53\u7d20\u4e00\u4e2a\u7279\u5f81\u5411\u91cf voxel_coords (torch.Tensor): \u4f53\u7d20\u5750\u6807\uff0c\u5f62\u72b6 (M, 4)\uff0c\u683c\u5f0f\u4e3a [batch_idx, z, y, x]\u3002 # \u4f53\u7d20\u5728\u7f51\u683c\u4e2d\u7684\u7d22\u5f15 batch_size (int): \u5f53\u524d\u6279\u6b21\u5927\u5c0f B\u3002 # batch \u6570\u91cf Returns: torch.Tensor: \u4e09\u7ef4\u7279\u5f81\u4f53\uff0c\u5f62\u72b6 (B, C_out, Z, Y, X)\u3002 # \u7a20\u5bc6 3D \u7279\u5f81 \"\"\" assert voxel_feats . dim () == 2 , \"voxel_feats \u5fc5\u987b\u662f (M, C_in) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 assert voxel_coords . dim () == 2 and voxel_coords . shape [ 1 ] == 4 , \"voxel_coords \u5fc5\u987b\u662f (M, 4) \u5f20\u91cf\" # \u68c0\u67e5\u7ef4\u5ea6 device = voxel_feats . device # \u83b7\u53d6\u8bbe\u5907 M , C_in = voxel_feats . shape # \u8bfb\u53d6\u4f53\u7d20\u6570\u91cf\u4e0e\u901a\u9053\u6570 Z , Y , X = self . grid_size # \u89e3\u5305\u7f51\u683c\u5c3a\u5bf8 # \u521d\u59cb\u5316\u7a20\u5bc6\u4f53\u7d20\u7f51\u683c\uff1a (B, C_in, Z, Y, X) dense_grid = torch . zeros ( batch_size , C_in , Z , Y , X , dtype = voxel_feats . dtype , device = device , ) # \u521d\u59cb\u5316 3D \u4f53\u7d20\u7f51\u683c # \u5c06\u7a00\u758f\u4f53\u7d20\u7279\u5f81 scatter \u5230\u7a20\u5bc6\u4f53\u7d20\u7f51\u683c\u4e2d b = voxel_coords [:, 0 ] . long () # batch \u7d22\u5f15 z = voxel_coords [:, 1 ] . long () # z \u8f74\u7d22\u5f15 y = voxel_coords [:, 2 ] . long () # y \u8f74\u7d22\u5f15 x = voxel_coords [:, 3 ] . long () # x \u8f74\u7d22\u5f15 # \u8fc7\u6ee4\u8d8a\u754c\u4f53\u7d20\uff08\u7406\u8bba\u4e0a voxelization \u5df2\u7ecf\u4fdd\u8bc1\u5408\u6cd5\uff0c\u8fd9\u91cc\u518d\u505a\u4e00\u6b21\u5b89\u5168\u68c0\u67e5\uff09 valid_mask = ( ( b >= 0 ) & ( b < batch_size ) & ( z >= 0 ) & ( z < Z ) & ( y >= 0 ) & ( y < Y ) & ( x >= 0 ) & ( x < X ) ) # \u5408\u6cd5\u4f53\u7d20\u63a9\u7801 if valid_mask . any (): # \u82e5\u5b58\u5728\u5408\u6cd5\u4f53\u7d20 b_valid = b [ valid_mask ] # \u6709\u6548 batch \u7d22\u5f15 z_valid = z [ valid_mask ] # \u6709\u6548 z \u7d22\u5f15 y_valid = y [ valid_mask ] # \u6709\u6548 y \u7d22\u5f15 x_valid = x [ valid_mask ] # \u6709\u6548 x \u7d22\u5f15 feats_valid = voxel_feats [ valid_mask ] # \u6709\u6548\u4f53\u7d20\u7279\u5f81 (M_valid,C_in) dense_grid [ b_valid , :, z_valid , y_valid , x_valid ] = feats_valid # \u586b\u5145\u5230\u7a20\u5bc6\u7f51\u683c\u4e2d # \u901a\u8fc7 3D \u5377\u79ef\u4e3b\u5e72\u63d0\u53d6\u4e09\u7ef4\u7279\u5f81 x3d = self . conv_block1 ( dense_grid ) # \u7b2c\u4e00\u5c42 3D \u5377\u79ef\u5757 x3d = self . conv_block2 ( x3d ) # \u7b2c\u4e8c\u5c42 3D \u5377\u79ef\u5757 return x3d # \u8fd4\u56de\u4e09\u7ef4\u7279\u5f81\u4f53","title":"forward"},{"location":"lidar_side/#lidar_flatten_z_to_bev","text":"\u5c06 3D \u7279\u5f81\u6cbf\u9ad8\u5ea6\u7ef4\uff08Z\uff09\u538b\u7f29\u4e3a BEV \u5e73\u9762\u7279\u5f81\u3002","title":"'lidar_flatten_z_to_bev'"},{"location":"lidar_side/#lidar_side.operator.lidar_flatten_z_to_bev.lidar_flatten_z_to_bev","text":"\u5c06 LiDAR \u4e09\u7ef4\u4f53\u7d20\u7279\u5f81\u6cbf\u9ad8\u5ea6\u7ef4\u5ea6 z \u8fdb\u884c\u805a\u5408\uff0c\u538b\u7f29\u4e3a BEV \u5e73\u9762\u4e0a\u7684 \u4e8c\u7ef4\u7279\u5f81\u56fe\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (B, C, H, W)\u3002 Parameters: feat_3d ( Tensor ) \u2013 (B, C, Z, H, W) LiDAR \u5206\u652f\u8f93\u51fa\u7684\u4e09\u7ef4\u7279\u5f81\u4f53\uff1a - B: batch size - C: \u901a\u9053\u6570 - Z: \u9ad8\u5ea6\u65b9\u5411\u4f53\u7d20\u5c42\u6570 - H, W: \u6c34\u5e73\u5e73\u9762\u4e0a\u7684\u4f53\u7d20\u7f51\u683c\u5c3a\u5bf8\u3002 Returns: bev_feat ( Tensor ) \u2013 (B, C, H, W) \u6cbf\u9ad8\u5ea6\u7ef4\u5ea6\u805a\u5408\u540e\u7684 BEV \u7279\u5f81\u56fe\uff0c\u8fd9\u91cc\u91c7\u7528 sum pooling \u8fdb\u884c\u805a\u5408\u3002 Source code in lidar_side\\operator\\lidar_flatten_z_to_bev.py 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 @torch . library . custom_op ( \"bevfusion_ops::lidar_flatten_z_to_bev\" , mutates_args = [] ) def lidar_flatten_z_to_bev ( feat_3d : torch . Tensor , ) -> torch . Tensor : \"\"\" \u5c06 LiDAR \u4e09\u7ef4\u4f53\u7d20\u7279\u5f81\u6cbf\u9ad8\u5ea6\u7ef4\u5ea6 z \u8fdb\u884c\u805a\u5408\uff0c\u538b\u7f29\u4e3a BEV \u5e73\u9762\u4e0a\u7684 \u4e8c\u7ef4\u7279\u5f81\u56fe\uff0c\u8f93\u51fa\u5f62\u72b6\u4e3a (B, C, H, W)\u3002 Args: feat_3d: (B, C, Z, H, W) LiDAR \u5206\u652f\u8f93\u51fa\u7684\u4e09\u7ef4\u7279\u5f81\u4f53\uff1a - B: batch size - C: \u901a\u9053\u6570 - Z: \u9ad8\u5ea6\u65b9\u5411\u4f53\u7d20\u5c42\u6570 - H, W: \u6c34\u5e73\u5e73\u9762\u4e0a\u7684\u4f53\u7d20\u7f51\u683c\u5c3a\u5bf8\u3002 Returns: bev_feat: (B, C, H, W) \u6cbf\u9ad8\u5ea6\u7ef4\u5ea6\u805a\u5408\u540e\u7684 BEV \u7279\u5f81\u56fe\uff0c\u8fd9\u91cc\u91c7\u7528 sum pooling \u8fdb\u884c\u805a\u5408\u3002 \"\"\" assert feat_3d . dim () == 5 , \"feat_3d \u5fc5\u987b\u662f (B, C, Z, H, W) \u5f20\u91cf\" # \u57fa\u672c\u5f62\u72b6\u68c0\u67e5 # \u4f7f\u7528 sum pooling \u6cbf\u9ad8\u5ea6\u7ef4\u5ea6\u805a\u5408\uff1a # (B, C, Z, H, W) \u2192 (B, C, H, W) bev_feat = feat_3d . sum ( dim = 2 ) # \u6cbf Z \u7ef4\u6c42\u548c return bev_feat # \u8fd4\u56de BEV \u7279\u5f81","title":"lidar_flatten_z_to_bev"}]}